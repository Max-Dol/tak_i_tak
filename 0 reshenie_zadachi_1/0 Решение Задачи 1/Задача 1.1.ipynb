{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13d7505-038f-4718-952e-1535ea5733f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Функция для загрузки данных\n",
    "def load_data(sample_size):\n",
    "    attributes = pd.read_parquet('F:/competition/attributes.parquet')\n",
    "    resnet = pd.read_parquet('F:/competition/resnet.parquet')\n",
    "    text_and_bert = pd.read_parquet('F:/competition/text_and_bert.parquet')\n",
    "    train = pd.read_parquet('F:/competition/train.parquet')\n",
    "    test = pd.read_parquet('F:/competition/test.parquet')\n",
    "\n",
    "    if sample_size < 1:\n",
    "        attributes = attributes.sample(frac=sample_size, random_state=42)\n",
    "        resnet = resnet.sample(frac=sample_size, random_state=42)\n",
    "        text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)\n",
    "        train = train.sample(frac=sample_size, random_state=42)\n",
    "        test = test.sample(frac=sample_size, random_state=42)\n",
    "\n",
    "    return attributes, resnet, text_and_bert, train, test\n",
    "\n",
    "# Загрузка данных\n",
    "attributes, resnet, text_and_bert, train, test = load_data(sample_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e445f32-e8ba-49bc-9244-4a5994649c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid</th>\n",
       "      <th>categories</th>\n",
       "      <th>characteristic_attributes_mapping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47920382</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Детские товары\", \"3\": \"Игру...</td>\n",
       "      <td>{\"Цвет товара\": [\"бежевый\", \"светло-розовый\"],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49801845</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Хобби и творчество\", \"3\": \"...</td>\n",
       "      <td>{\"Количество в упаковке, шт\": [\"1\"], \"Бренд\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49853444</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Хобби и творчество\", \"3\": \"...</td>\n",
       "      <td>{\"Бренд\": [\"Vervaco\"], \"Тип\": [\"Набор для выши...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49893028</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Хобби и творчество\", \"3\": \"...</td>\n",
       "      <td>{\"Цвет товара\": [\"серый\"], \"Ширина, см\": [\"0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49987483</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Хобби и творчество\", \"3\": \"...</td>\n",
       "      <td>{\"Цвет товара\": [\"разноцветный\"], \"Название цв...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252564</th>\n",
       "      <td>1799186009</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Спорт и отдых\", \"3\": \"Скейт...</td>\n",
       "      <td>{\"Материал деки\": [\"ABS пластик\", \"Алюминий\"],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252565</th>\n",
       "      <td>1801305376</td>\n",
       "      <td>{\"1\": \"Книги и цифровые книги\", \"2\": \"Книги\", ...</td>\n",
       "      <td>{\"Год выпуска\": [\"2024\"], \"Количество страниц\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252566</th>\n",
       "      <td>1802944592</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Спорт и отдых\", \"3\": \"Аксес...</td>\n",
       "      <td>{\"Толщина, мм\": [\"0.06\"], \"Бренд\": [\"Daiwa\"], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252567</th>\n",
       "      <td>1803038155</td>\n",
       "      <td>{\"1\": \"Книги и цифровые книги\", \"2\": \"Книги\", ...</td>\n",
       "      <td>{\"Год выпуска\": [\"2022\"], \"Количество страниц\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252568</th>\n",
       "      <td>1809203525</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Дом и сад\", \"3\": \"Садовый д...</td>\n",
       "      <td>{\"Артикул\": [\"С237\"], \"Комплектация\": [\"1 шт\"]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2252569 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          variantid                                         categories  \\\n",
       "0          47920382  {\"1\": \"EPG\", \"2\": \"Детские товары\", \"3\": \"Игру...   \n",
       "1          49801845  {\"1\": \"EPG\", \"2\": \"Хобби и творчество\", \"3\": \"...   \n",
       "2          49853444  {\"1\": \"EPG\", \"2\": \"Хобби и творчество\", \"3\": \"...   \n",
       "3          49893028  {\"1\": \"EPG\", \"2\": \"Хобби и творчество\", \"3\": \"...   \n",
       "4          49987483  {\"1\": \"EPG\", \"2\": \"Хобби и творчество\", \"3\": \"...   \n",
       "...             ...                                                ...   \n",
       "2252564  1799186009  {\"1\": \"EPG\", \"2\": \"Спорт и отдых\", \"3\": \"Скейт...   \n",
       "2252565  1801305376  {\"1\": \"Книги и цифровые книги\", \"2\": \"Книги\", ...   \n",
       "2252566  1802944592  {\"1\": \"EPG\", \"2\": \"Спорт и отдых\", \"3\": \"Аксес...   \n",
       "2252567  1803038155  {\"1\": \"Книги и цифровые книги\", \"2\": \"Книги\", ...   \n",
       "2252568  1809203525  {\"1\": \"EPG\", \"2\": \"Дом и сад\", \"3\": \"Садовый д...   \n",
       "\n",
       "                         characteristic_attributes_mapping  \n",
       "0        {\"Цвет товара\": [\"бежевый\", \"светло-розовый\"],...  \n",
       "1        {\"Количество в упаковке, шт\": [\"1\"], \"Бренд\": ...  \n",
       "2        {\"Бренд\": [\"Vervaco\"], \"Тип\": [\"Набор для выши...  \n",
       "3        {\"Цвет товара\": [\"серый\"], \"Ширина, см\": [\"0.8...  \n",
       "4        {\"Цвет товара\": [\"разноцветный\"], \"Название цв...  \n",
       "...                                                    ...  \n",
       "2252564  {\"Материал деки\": [\"ABS пластик\", \"Алюминий\"],...  \n",
       "2252565  {\"Год выпуска\": [\"2024\"], \"Количество страниц\"...  \n",
       "2252566  {\"Толщина, мм\": [\"0.06\"], \"Бренд\": [\"Daiwa\"], ...  \n",
       "2252567  {\"Год выпуска\": [\"2022\"], \"Количество страниц\"...  \n",
       "2252568  {\"Артикул\": [\"С237\"], \"Комплектация\": [\"1 шт\"]...  \n",
       "\n",
       "[2252569 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec47efb8-1455-45fb-81c6-8fec6c5dcf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "    (position_embeddings): Embedding(2048, 312)\n",
       "    (token_type_embeddings): Embedding(2, 312)\n",
       "    (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-2): 3 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "          (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Инициализация модели и токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ab9b69-7596-4813-b6a4-c51ffb2b03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    # Токенизация текста: преобразует текст в формат, пригодный для модели BERT\n",
    "    # padding=True добавляет нули к коротким последовательностям, truncation=True обрезает длинные последовательности\n",
    "    # return_tensors=\"pt\" возвращает результат в формате тензоров PyTorch\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # torch.no_grad() отключает автоматическое вычисление градиентов для экономии памяти и ускорения инференса\n",
    "    with torch.no_grad():\n",
    "        # Прогоняем данные через модель, перемещая их на устройство (GPU или CPU)\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    \n",
    "    # Извлекаем эмбеддинги CLS-токена (он расположен в начале последовательности и предназначен для классификации)\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    # Нормализуем эмбеддинги (делаем их единичными векторами), чтобы можно было сравнивать их по косинусному сходству\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    \n",
    "    # Преобразуем тензор с устройства (GPU или CPU) в numpy-массив на CPU\n",
    "    return embeddings[0].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39f75234-7ffe-4da3-b988-e599ebaafe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def create_features(row, bert_df, resnet_df):\n",
    "    variantid1, variantid2 = row['variantid1'], row['variantid2']\n",
    "    \n",
    "    # Эмбеддинги названий из BERT\n",
    "    name_bert_1_values = bert_df.loc[bert_df['variantid'] == variantid1, 'name_bert_64'].values\n",
    "    name_bert_2_values = bert_df.loc[bert_df['variantid'] == variantid2, 'name_bert_64'].values\n",
    "\n",
    "    # Проверка наличия данных\n",
    "    if len(name_bert_1_values) == 0 or len(name_bert_2_values) == 0:\n",
    "        print(f\"Эмбеддинги для variantid1: {variantid1} или variantid2: {variantid2} не найдены.\")\n",
    "        return [0, 0]\n",
    "\n",
    "    try:\n",
    "        name_bert_1 = np.array(name_bert_1_values[0]).reshape(1, -1)  # Преобразуем в 2D массив\n",
    "        name_bert_2 = np.array(name_bert_2_values[0]).reshape(1, -1)  # Преобразуем в 2D массив\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при преобразовании эмбеддингов названий: {e}\")\n",
    "        return [0, 0]\n",
    "\n",
    "    # Косинусное сходство между эмбеддингами названий\n",
    "    try:\n",
    "        name_similarity = cosine_similarity(name_bert_1, name_bert_2)[0][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при вычислении косинусного сходства для названий: {e}\")\n",
    "        name_similarity = 0\n",
    "\n",
    "    # Эмбеддинги изображений из ResNet\n",
    "    pic_embedding_1_values = resnet_df.loc[resnet_df['variantid'] == variantid1, 'main_pic_embeddings_resnet_v1'].values\n",
    "    pic_embedding_2_values = resnet_df.loc[resnet_df['variantid'] == variantid2, 'main_pic_embeddings_resnet_v1'].values\n",
    "\n",
    "    # Проверка наличия данных\n",
    "    if len(pic_embedding_1_values) == 0 or len(pic_embedding_2_values) == 0:\n",
    "        print(f\"Эмбеддинги изображений для variantid1: {variantid1} или variantid2: {variantid2} не найдены.\")\n",
    "        return [name_similarity, 0]\n",
    "\n",
    "    try:\n",
    "        pic_embedding_1 = np.array(pic_embedding_1_values[0]).reshape(1, -1)  # Преобразуем в 2D массив\n",
    "        pic_embedding_2 = np.array(pic_embedding_2_values[0]).reshape(1, -1)  # Преобразуем в 2D массив\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при преобразовании эмбеддингов изображений: {e}\")\n",
    "        return [name_similarity, 0]\n",
    "\n",
    "    # Косинусное сходство между эмбеддингами изображений\n",
    "    try:\n",
    "        pic_similarity = cosine_similarity(pic_embedding_1, pic_embedding_2)[0][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при вычислении косинусного сходства для изображений: {e}\")\n",
    "        pic_similarity = 0\n",
    "\n",
    "    return [name_similarity, pic_similarity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79605938-7a45-41f7-9dd7-3f796ed71125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507118d-fa32-43c5-9066-a0c7c8610184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c103dc27-77f0-4046-8291-523adb0c1545",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Создание обучающего датасета\u001b[39;00m\n\u001b[0;32m      2\u001b[0m small_train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m train_features \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_and_bert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m train_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_features\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m      5\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Создание обучающего датасета\u001b[39;00m\n\u001b[0;32m      2\u001b[0m small_train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m train_features \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mcreate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_and_bert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m train_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_features\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m      5\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "Cell \u001b[1;32mIn[15], line 36\u001b[0m, in \u001b[0;36mcreate_features\u001b[1;34m(row, bert_df, resnet_df)\u001b[0m\n\u001b[0;32m     33\u001b[0m pic_embedding_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pic_embedding_2_values[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Косинусное сходство между эмбеддингами изображений\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m pic_similarity \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic_embedding_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic_embedding_2\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [name_similarity, pic_similarity]\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1679\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1636\u001b[0m \n\u001b[0;32m   1637\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1675\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1679\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1681\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:185\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    175\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    176\u001b[0m         X,\n\u001b[0;32m    177\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    195\u001b[0m         Y,\n\u001b[0;32m    196\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    201\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    202\u001b[0m     )\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\utils\\_array_api.py:751\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    749\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Создание обучающего датасета\n",
    "small_train = train.head(10)\n",
    "train_features = train.apply(lambda row: create_features(row, text_and_bert, resnet), axis=1)\n",
    "train_features = np.array(train_features.tolist())\n",
    "train_labels = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3737610b-957d-477e-8fb5-f251899173c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[0.8170074820518494, 0.9416620135307312, 0.31...\n",
      "1    [[-0.43339717388153076, -0.17318281531333923, ...\n",
      "2    [[0.11314830183982849, -0.34010639786720276, -...\n",
      "3    [[0.25037717819213867, 0.33753663301467896, 0....\n",
      "4    [[0.43453288078308105, 0.09419603645801544, -0...\n",
      "Name: main_pic_embeddings_resnet_v1, dtype: object\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Посмотрите на несколько примеров данных\n",
    "print(resnet['main_pic_embeddings_resnet_v1'].head())\n",
    "\n",
    "# Проверьте тип данных\n",
    "print(type(resnet['main_pic_embeddings_resnet_v1'].iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9fa9415-217d-43f0-b9dd-1efd05ca0162",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pic_embedding_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpic_embedding_1\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pic_embedding_1' is not defined"
     ]
    }
   ],
   "source": [
    "pic_embedding_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1dc1e1-13f5-4dd5-bbb7-82031fe005be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773b7b2-7c07-4eb0-b271-df106cacf35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Обучение модели\n",
    "model = LogisticRegression()\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "# Применение на тестовом датасете\n",
    "test_features = test.apply(lambda row: create_features(row, text_and_bert, resnet), axis=1)\n",
    "test_features = np.array(test_features.tolist())\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "# Сохранение результатов\n",
    "submission = pd.DataFrame({\n",
    "    'variantid1': test['variantid1'],\n",
    "    'variantid2': test['variantid2'],\n",
    "    'target': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e92b041-69e2-489a-a628-81391c01c751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ba411-43e3-4077-8ece-ad3f2e671b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61fadb-e0d5-4cd4-872c-c3034d5620e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09f375-b57b-4c6e-a55b-5b309e4c67e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851219c-e228-471e-af7a-d7d9f804afa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f7ca2-1a7c-4382-a1d0-19846b17d914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d6245-dd77-41c9-81cd-31c5e15d2ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386819f-3f06-424d-a47e-2b3eeb72dea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a7b35-7db1-45c6-9548-405440fedd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7409a1e-7958-450c-af54-b37a9eb996c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73ee2f40-8749-4a99-9723-bd34c425bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import joblib\n",
    "import json  # Добавлен импорт модуля json\n",
    "\n",
    "# Загрузка данных\n",
    "def load_data(sample_size=0.1):\n",
    "    attributes = pd.read_parquet('F:/competition/attributes.parquet')\n",
    "    resnet = pd.read_parquet('F:/competition/resnet.parquet')\n",
    "    text_and_bert = pd.read_parquet('F:/competition/text_and_bert.parquet')\n",
    "    train = pd.read_parquet('F:/competition/train.parquet')\n",
    "    test = pd.read_parquet('F:/competition/test.parquet')\n",
    "\n",
    "    if sample_size < 1:\n",
    "        attributes = attributes.sample(frac=sample_size, random_state=42)\n",
    "        resnet = resnet.sample(frac=sample_size, random_state=42)\n",
    "        text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)\n",
    "        train = train.sample(frac=sample_size, random_state=42)\n",
    "        test = test.sample(frac=sample_size, random_state=42)\n",
    "\n",
    "    return attributes, resnet, text_and_bert, train, test\n",
    "\n",
    "# Загрузка данных\n",
    "attributes, resnet, text_and_bert, train, test = load_data(sample_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71d20560-f8b1-4442-891d-1341b8122401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(train, attributes, resnet, text_and_bert):\n",
    "    # Присоединяем атрибуты\n",
    "    train = train.merge(attributes[['variantid', 'characteristic_attributes_mapping']], \n",
    "                        left_on='variantid1', right_on='variantid', how='left').drop('variantid', axis=1)\n",
    "    train = train.merge(attributes[['variantid', 'characteristic_attributes_mapping']], \n",
    "                        left_on='variantid2', right_on='variantid', how='left', suffixes=('_1', '_2')).drop('variantid', axis=1)\n",
    "\n",
    "    # Присоединяем текстовые эмбеддинги BERT\n",
    "    train = train.merge(text_and_bert[['variantid', 'name_bert_64']], \n",
    "                        left_on='variantid1', right_on='variantid', how='left').drop('variantid', axis=1)\n",
    "    train = train.merge(text_and_bert[['variantid', 'name_bert_64']], \n",
    "                        left_on='variantid2', right_on='variantid', how='left', suffixes=('_1', '_2')).drop('variantid', axis=1)\n",
    "\n",
    "    # Преобразование эмбеддингов в числовые массивы\n",
    "    for col in ['name_bert_64_1', 'name_bert_64_2']:\n",
    "        # Преобразуем только если значение не NaN\n",
    "        train[col] = train[col].apply(lambda x: np.array(json.loads(x)) if isinstance(x, str) else np.nan)\n",
    "\n",
    "    # Векторы ResNet для изображений\n",
    "    train = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], \n",
    "                        left_on='variantid1', right_on='variantid', how='left').drop('variantid', axis=1)\n",
    "    train = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], \n",
    "                        left_on='variantid2', right_on='variantid', how='left', suffixes=('_resnet1', '_resnet2')).drop('variantid', axis=1)\n",
    "\n",
    "    # Преобразование эмбеддингов ResNet в числовые массивы\n",
    "    for col in ['main_pic_embeddings_resnet_v1_resnet1', 'main_pic_embeddings_resnet_v1_resnet2']:\n",
    "        # Преобразуем только если значение не NaN\n",
    "        train[col] = train[col].apply(lambda x: np.array(json.loads(x)) if isinstance(x, str) else np.nan)\n",
    "\n",
    "    # Проверяем создание всех эмбеддингов в виде массива\n",
    "    if any(col not in train.columns for col in ['name_bert_64_1', 'name_bert_64_2', 'main_pic_embeddings_resnet_v1_resnet1', 'main_pic_embeddings_resnet_v1_resnet2']):\n",
    "        raise KeyError(\"Не удалось создать один из столбцов с эмбеддингами.\")\n",
    "\n",
    "    # Объединение всех эмбеддингов в единый вектор признаков\n",
    "    train['combined_features'] = train.apply(lambda row: np.concatenate([\n",
    "        row['name_bert_64_1'] if isinstance(row['name_bert_64_1'], np.ndarray) else np.zeros(64), \n",
    "        row['name_bert_64_2'] if isinstance(row['name_bert_64_2'], np.ndarray) else np.zeros(64), \n",
    "        row['main_pic_embeddings_resnet_v1_resnet1'] if isinstance(row['main_pic_embeddings_resnet_v1_resnet1'], np.ndarray) else np.zeros(2048), \n",
    "        row['main_pic_embeddings_resnet_v1_resnet2'] if isinstance(row['main_pic_embeddings_resnet_v1_resnet2'], np.ndarray) else np.zeros(2048)\n",
    "    ]), axis=1)\n",
    "\n",
    "    # Проверяем успешное создание столбца combined_features\n",
    "    if 'combined_features' not in train.columns:\n",
    "        raise KeyError(\"Не удалось создать столбец 'combined_features'.\")\n",
    "\n",
    "    return train\n",
    "# Подготовка признаков\n",
    "train = prepare_features(train, attributes, resnet, text_and_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "654e2991-f37f-4c43-9508-1251aec861cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variantid1                                    0\n",
      "variantid2                                    0\n",
      "target                                        0\n",
      "characteristic_attributes_mapping_1      105253\n",
      "characteristic_attributes_mapping_2      105228\n",
      "name_bert_64_1                           116852\n",
      "name_bert_64_2                           116852\n",
      "main_pic_embeddings_resnet_v1_resnet1    116852\n",
      "main_pic_embeddings_resnet_v1_resnet2    116852\n",
      "combined_features                             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cb6347d-c564-43c1-ac0c-2d315d4a62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9d281ce-62b1-4a79-a02c-9d7f153c1003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid1</th>\n",
       "      <th>variantid2</th>\n",
       "      <th>target</th>\n",
       "      <th>characteristic_attributes_mapping_1</th>\n",
       "      <th>characteristic_attributes_mapping_2</th>\n",
       "      <th>name_bert_64_1</th>\n",
       "      <th>name_bert_64_2</th>\n",
       "      <th>main_pic_embeddings_resnet_v1_resnet1</th>\n",
       "      <th>main_pic_embeddings_resnet_v1_resnet2</th>\n",
       "      <th>combined_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>938776155</td>\n",
       "      <td>473815138</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1349745192</td>\n",
       "      <td>1349703019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1450853415</td>\n",
       "      <td>1450853943</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>741508506</td>\n",
       "      <td>413821437</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1337038519</td>\n",
       "      <td>1313965970</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"Комплектация\": [\"Стайлер, инструкция, упаков...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116847</th>\n",
       "      <td>101957372</td>\n",
       "      <td>1269490207</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"Артикул\": [\"665334\"], \"Партномер (артикул пр...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116848</th>\n",
       "      <td>700695167</td>\n",
       "      <td>700666712</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116849</th>\n",
       "      <td>1557204320</td>\n",
       "      <td>1010812448</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116850</th>\n",
       "      <td>1480454634</td>\n",
       "      <td>1491520915</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116851</th>\n",
       "      <td>1448475557</td>\n",
       "      <td>1211403157</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116852 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        variantid1  variantid2  target  \\\n",
       "0        938776155   473815138       1   \n",
       "1       1349745192  1349703019       0   \n",
       "2       1450853415  1450853943       1   \n",
       "3        741508506   413821437       0   \n",
       "4       1337038519  1313965970       1   \n",
       "...            ...         ...     ...   \n",
       "116847   101957372  1269490207       0   \n",
       "116848   700695167   700666712       0   \n",
       "116849  1557204320  1010812448       1   \n",
       "116850  1480454634  1491520915       0   \n",
       "116851  1448475557  1211403157       1   \n",
       "\n",
       "                      characteristic_attributes_mapping_1  \\\n",
       "0                                                       0   \n",
       "1                                                       0   \n",
       "2                                                       0   \n",
       "3                                                       0   \n",
       "4       {\"Комплектация\": [\"Стайлер, инструкция, упаков...   \n",
       "...                                                   ...   \n",
       "116847  {\"Артикул\": [\"665334\"], \"Партномер (артикул пр...   \n",
       "116848                                                  0   \n",
       "116849                                                  0   \n",
       "116850                                                  0   \n",
       "116851                                                  0   \n",
       "\n",
       "       characteristic_attributes_mapping_2  name_bert_64_1  name_bert_64_2  \\\n",
       "0                                        0             0.0             0.0   \n",
       "1                                        0             0.0             0.0   \n",
       "2                                        0             0.0             0.0   \n",
       "3                                        0             0.0             0.0   \n",
       "4                                        0             0.0             0.0   \n",
       "...                                    ...             ...             ...   \n",
       "116847                                   0             0.0             0.0   \n",
       "116848                                   0             0.0             0.0   \n",
       "116849                                   0             0.0             0.0   \n",
       "116850                                   0             0.0             0.0   \n",
       "116851                                   0             0.0             0.0   \n",
       "\n",
       "        main_pic_embeddings_resnet_v1_resnet1  \\\n",
       "0                                         0.0   \n",
       "1                                         0.0   \n",
       "2                                         0.0   \n",
       "3                                         0.0   \n",
       "4                                         0.0   \n",
       "...                                       ...   \n",
       "116847                                    0.0   \n",
       "116848                                    0.0   \n",
       "116849                                    0.0   \n",
       "116850                                    0.0   \n",
       "116851                                    0.0   \n",
       "\n",
       "        main_pic_embeddings_resnet_v1_resnet2  \\\n",
       "0                                         0.0   \n",
       "1                                         0.0   \n",
       "2                                         0.0   \n",
       "3                                         0.0   \n",
       "4                                         0.0   \n",
       "...                                       ...   \n",
       "116847                                    0.0   \n",
       "116848                                    0.0   \n",
       "116849                                    0.0   \n",
       "116850                                    0.0   \n",
       "116851                                    0.0   \n",
       "\n",
       "                                        combined_features  \n",
       "0       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                   ...  \n",
       "116847  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "116848  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "116849  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "116850  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "116851  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[116852 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d469c35-e381-490c-ada2-5c44e3dd3b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a7ad576-6763-4752-b15e-f44d8160673d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR AUC: 0.7412177484917205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Разделение на тренировочную и валидационную выборки\n",
    "X = np.vstack(train['combined_features'].values)\n",
    "y = train['target'].values\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Масштабирование данных\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Обучение модели\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Прогнозирование\n",
    "y_pred_prob = logreg.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Вычисление метрики PR AUC\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f\"PR AUC: {pr_auc}\")\n",
    "\n",
    "# Сохранение модели\n",
    "joblib.dump(logreg, 'model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63de53ad-a81b-4ad7-babf-82e338ea17f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "C:/Go_Agent/pipelines/BuildMaster/catboost.git/catboost/libs/data/quantization.cpp:2420: All features are either constant or ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Обучение модели CatBoost\u001b[39;00m\n\u001b[0;32m      2\u001b[0m catboost_model \u001b[38;5;241m=\u001b[39m CatBoostClassifier(iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRAUC\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mcatboost_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Прогнозирование\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y_pred_prob \u001b[38;5;241m=\u001b[39m catboost_model\u001b[38;5;241m.\u001b[39mpredict_proba(X_val_scaled)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\catboost\\core.py:5220\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5218\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5220\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5221\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5222\u001b[0m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\catboost\\core.py:2400\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2397\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2399\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2400\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2406\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2408\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2409\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\catboost\\core.py:1780\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4833\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4882\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: C:/Go_Agent/pipelines/BuildMaster/catboost.git/catboost/libs/data/quantization.cpp:2420: All features are either constant or ignored."
     ]
    }
   ],
   "source": [
    "# Обучение модели CatBoost\n",
    "catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, eval_metric='PRAUC', verbose=200)\n",
    "catboost_model.fit(X_train_scaled, y_train, eval_set=(X_val_scaled, y_val))\n",
    "\n",
    "# Прогнозирование\n",
    "y_pred_prob = catboost_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Вычисление метрики PR AUC\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f\"PR AUC: {pr_auc}\")\n",
    "\n",
    "# Сохранение модели и скейлера\n",
    "joblib.dump(catboost_model, 'catboost_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b337a-5b74-41d3-b048-754fd289c6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfee9514-0de9-4a1d-8f93-537051cf2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Применение модели к тестовым данным\n",
    "def predict_on_test(test, text_and_bert, resnet, attributes, model, scaler):\n",
    "    test = prepare_features(test, attributes, resnet, text_and_bert)\n",
    "    X_test = np.vstack(test['combined_features'].values)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    test['target'] = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    test[['variantid1', 'variantid2', 'target']].to_csv('submission.csv', index=False)\n",
    "\n",
    "# Инференс\n",
    "predict_on_test(test, text_and_bert, resnet, attributes, logreg, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0982c-4bfa-41b0-9e25-9fe497299fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3a45a53-5272-481d-953a-e9150addfd33",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Подготовка признаков\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_and_bert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Разделение на тренировочную и валидационную выборки\u001b[39;00m\n\u001b[0;32m     64\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n",
      "Cell \u001b[1;32mIn[21], line 38\u001b[0m, in \u001b[0;36mprepare_features\u001b[1;34m(train, attributes, resnet, text_and_bert)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Преобразование эмбеддингов в числовые массивы\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_bert_64_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_bert_64_2\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 38\u001b[0m     train[col] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Векторы ResNet для изображений\u001b[39;00m\n\u001b[0;32m     41\u001b[0m train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mmerge(resnet[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariantid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain_pic_embeddings_resnet_v1\u001b[39m\u001b[38;5;124m'\u001b[39m]], left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariantid1\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariantid\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariantid\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\series.py:4917\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[21], line 38\u001b[0m, in \u001b[0;36mprepare_features.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Преобразование эмбеддингов в числовые массивы\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_bert_64_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname_bert_64_2\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 38\u001b[0m     train[col] \u001b[38;5;241m=\u001b[39m train[col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Векторы ResNet для изображений\u001b[39;00m\n\u001b[0;32m     41\u001b[0m train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mmerge(resnet[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariantid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain_pic_embeddings_resnet_v1\u001b[39m\u001b[38;5;124m'\u001b[39m]], left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariantid1\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariantid\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariantid\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\json\\__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[1;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    340\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not float"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import joblib\n",
    "from catboost import CatBoostClassifier  # Добавляем CatBoostClassifier\n",
    "\n",
    "# Загрузка данных\n",
    "def load_data(sample_size=0.1):\n",
    "    attributes = pd.read_parquet('F:/competition/attributes.parquet')\n",
    "    resnet = pd.read_parquet('F:/competition/resnet.parquet')\n",
    "    text_and_bert = pd.read_parquet('F:/competition/text_and_bert.parquet')\n",
    "    train = pd.read_parquet('F:/competition/train.parquet')\n",
    "    test = pd.read_parquet('F:/competition/test.parquet')\n",
    "\n",
    "    if sample_size < 1:\n",
    "        attributes = attributes.sample(frac=sample_size, random_state=42)\n",
    "        resnet = resnet.sample(frac=sample_size, random_state=42)\n",
    "        text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)\n",
    "        train = train.sample(frac=sample_size, random_state=42)\n",
    "        test = test.sample(frac=sample_size, random_state=42)\n",
    "\n",
    "    return attributes, resnet, text_and_bert, train, test\n",
    "\n",
    "# Загрузка данных\n",
    "attributes, resnet, text_and_bert, train, test = load_data(sample_size=0.1)\n",
    "\n",
    "# Преобразование данных\n",
    "def prepare_features(train, attributes, resnet, text_and_bert):\n",
    "    # Объединение данных по variantid\n",
    "    train = train.merge(text_and_bert[['variantid', 'name_bert_64']], left_on='variantid1', right_on='variantid', how='left').drop('variantid', axis=1)\n",
    "    train = train.merge(text_and_bert[['variantid', 'name_bert_64']], left_on='variantid2', right_on='variantid', suffixes=('_1', '_2')).drop('variantid', axis=1)\n",
    "    \n",
    "    # Преобразование эмбеддингов в числовые массивы\n",
    "    for col in ['name_bert_64_1', 'name_bert_64_2']:\n",
    "        train[col] = train[col].apply(lambda x: np.array(json.loads(x)))\n",
    "\n",
    "    # Векторы ResNet для изображений\n",
    "    train = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left').drop('variantid', axis=1)\n",
    "    train = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', suffixes=('_img1', '_img2')).drop('variantid', axis=1)\n",
    "    \n",
    "    # Преобразование эмбеддингов изображений в числовые массивы\n",
    "    for col in ['main_pic_embeddings_resnet_v1_img1', 'main_pic_embeddings_resnet_v1_img2']:\n",
    "        train[col] = train[col].apply(lambda x: np.array(json.loads(x)))\n",
    "\n",
    "    # Атрибуты\n",
    "    train = train.merge(attributes[['variantid', 'characteristic_attributes_mapping']], left_on='variantid1', right_on='variantid', how='left').drop('variantid', axis=1)\n",
    "    train = train.merge(attributes[['variantid', 'characteristic_attributes_mapping']], left_on='variantid2', right_on='variantid', suffixes=('_attr1', '_attr2')).drop('variantid', axis=1)\n",
    "\n",
    "    # Преобразование атрибутов\n",
    "    train['attribute_similarity'] = train.apply(lambda row: compute_similarity(row['characteristic_attributes_mapping_attr1'], row['characteristic_attributes_mapping_attr2']), axis=1)\n",
    "\n",
    "    # Конкатенация всех эмбеддингов и признаков\n",
    "    train['combined_features'] = train.apply(lambda row: np.concatenate([row['name_bert_64_1'], row['name_bert_64_2'], row['main_pic_embeddings_resnet_v1_img1'], row['main_pic_embeddings_resnet_v1_img2'], [row['attribute_similarity']]]), axis=1)\n",
    "\n",
    "    return train\n",
    "\n",
    "# Подготовка признаков\n",
    "train = prepare_features(train, attributes, resnet, text_and_bert)\n",
    "\n",
    "# Разделение на тренировочную и валидационную выборки\n",
    "X = np.vstack(train['combined_features'].values)\n",
    "y = train['target'].values\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Масштабирование данных\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Обучение модели CatBoost\n",
    "catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, eval_metric='PRAUC', verbose=200)\n",
    "catboost_model.fit(X_train_scaled, y_train, eval_set=(X_val_scaled, y_val))\n",
    "\n",
    "# Прогнозирование\n",
    "y_pred_prob = catboost_model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "# Вычисление метрики PR AUC\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f\"PR AUC: {pr_auc}\")\n",
    "\n",
    "# Сохранение модели и скейлера\n",
    "joblib.dump(catboost_model, 'catboost_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Применение модели к тестовым данным\n",
    "def predict_on_test(test, text_and_bert, resnet, attributes, model, scaler):\n",
    "    test = prepare_features(test, attributes, resnet, text_and_bert)\n",
    "    X_test = np.vstack(test['combined_features'].values)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    test['target'] = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    test[['variantid1', 'variantid2', 'target']].to_csv('submission.csv', index=False)\n",
    "\n",
    "# Инференс\n",
    "predict_on_test(test, text_and_bert, resnet, attributes, catboost_model, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf11efb-1566-4abd-8628-6555e70e8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример с TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Пример создания векторизатора\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "joblib.dump(tfidf_vectorizer, 'vectorizer.pkl')  # Сохранение векторизатора\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17ca81-7730-4a47-bfee-4a5c20c9152f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fea4a0-7237-4f4c-a839-d7178b2206d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06847e2-1890-4032-b592-c8bb22bf0c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129bf97-34f0-426a-8fcf-941054d1859c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d356dd3-4b24-4fc9-8e5b-8e5e4e09b1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b2006-bb10-493f-849b-5e21b88ab08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164dee74-d606-4d11-9882-f125f9d8f95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c2be886-49fe-41a8-a716-0585e80c246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRAUC: 0.5572370449850078\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Путь к данным  \n",
    "attributes_path = 'F:/competition/attributes.parquet'  \n",
    "resnet_path = 'F:/competition/resnet.parquet'  \n",
    "text_and_bert_path = 'F:/competition/text_and_bert.parquet'  \n",
    "train_path = 'F:/competition/train.parquet'  \n",
    "test_path = 'F:/competition/test.parquet'  \n",
    "\n",
    "# Функция для загрузки данных  \n",
    "def load_data(sample_size=0.1):  \n",
    "    # Загрузка данных из файлов Parquet  \n",
    "    attributes = pd.read_parquet(attributes_path)  \n",
    "    resnet = pd.read_parquet(resnet_path)  \n",
    "    text_and_bert = pd.read_parquet(text_and_bert_path)  \n",
    "    train = pd.read_parquet(train_path)  \n",
    "    test = pd.read_parquet(test_path)  \n",
    "\n",
    "    # Если sample_size меньше 1, то выборка данных  \n",
    "    if sample_size < 1:  \n",
    "        attributes = attributes.sample(frac=sample_size, random_state=42)  \n",
    "        resnet = resnet.sample(frac=sample_size, random_state=42)  \n",
    "        text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)  \n",
    "        train = train.sample(frac=sample_size, random_state=42)  \n",
    "        test = test.sample(frac=sample_size, random_state=42)  \n",
    "\n",
    "    # Возвращаем 5 DataFrame'ов\n",
    "    return attributes, resnet, text_and_bert, train, test  \n",
    "\n",
    "# Обработка текстовых данных\n",
    "def process_text_and_bert(df):\n",
    "    # Объединение столбцов name и description в единый текст\n",
    "    df['combined_text'] = df['name'] + ' ' + df['description']\n",
    "    return df\n",
    "\n",
    "# Объединение данных\n",
    "def merge_data(train, resnet, text_and_bert):\n",
    "    # Объединение с эмбеддингами ResNet\n",
    "    train_data = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_1'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    train_data = train_data.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_2'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Объединение с текстовыми данными и эмбеддингами BERT\n",
    "    train_data = train_data.merge(text_and_bert[['variantid', 'combined_text', 'name_bert_64']], left_on='variantid1', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'combined_text': 'text_1', 'name_bert_64': 'text_embedding_1'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    train_data = train_data.merge(text_and_bert[['variantid', 'combined_text', 'name_bert_64']], left_on='variantid2', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'combined_text': 'text_2', 'name_bert_64': 'text_embedding_2'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Удаление строк, в которых есть пропущенные значения\n",
    "    train_data = train_data.dropna()\n",
    "\n",
    "    return train_data\n",
    "\n",
    "# Объединение эмбеддингов\n",
    "def combine_embeddings(row):\n",
    "    # Объединение эмбеддингов изображений и текста\n",
    "    pic_embeddings = np.concatenate([row['pic_embeddings_1'][0], row['pic_embeddings_2'][0]])\n",
    "    text_embeddings = np.concatenate([row['text_embedding_1'], row['text_embedding_2']])\n",
    "    return np.concatenate([pic_embeddings, text_embeddings])\n",
    "\n",
    "# Подготовка данных для обучения модели\n",
    "def prepare_data(train_data):\n",
    "    # Объединение эмбеддингов изображений и текстов\n",
    "    train_data['combined_embeddings'] = train_data.apply(combine_embeddings, axis=1)\n",
    "\n",
    "    # Преобразование объединенных эмбеддингов в массив numpy\n",
    "    X = np.vstack(train_data['combined_embeddings'].values)\n",
    "    y = train_data['target']\n",
    "\n",
    "    # Разделение данных на обучающую и валидационную выборки\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "# Обучение модели\n",
    "def train_model(X_train, y_train):\n",
    "    model = LogisticRegression(max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Сохранение обученной модели\n",
    "    joblib.dump(model, 'baseline.pkl')\n",
    "    return model\n",
    "\n",
    "# Оценка модели\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "    prauc = auc(recall, precision)\n",
    "    print(f'PRAUC: {prauc}')\n",
    "\n",
    "# Главная функция для запуска всего процесса\n",
    "def main():\n",
    "    attributes, resnet, text_and_bert, train, test = load_data()\n",
    "\n",
    "    # Обработка текстовых данных\n",
    "    text_and_bert = process_text_and_bert(text_and_bert)\n",
    "\n",
    "    # Объединение данных\n",
    "    train_data = merge_data(train, resnet, text_and_bert)\n",
    "\n",
    "    # Подготовка данных для обучения модели\n",
    "    X_train, X_val, y_train, y_val = prepare_data(train_data)\n",
    "\n",
    "    # Обучение модели\n",
    "    model = train_model(X_train, y_train)\n",
    "    \n",
    "    # Оценка модели\n",
    "    evaluate_model(model, X_val, y_val)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534adc60-5223-401f-b2c0-2867eb17ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRAUC: 0.5936540981777991\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Путь к данным  \n",
    "attributes_path = 'F:/competition/attributes.parquet'  \n",
    "resnet_path = 'F:/competition/resnet.parquet'  \n",
    "text_and_bert_path = 'F:/competition/text_and_bert.parquet'  \n",
    "train_path = 'F:/competition/train.parquet'  \n",
    "test_path = 'F:/competition/test.parquet'  \n",
    "\n",
    "# Функция для загрузки данных  \n",
    "def load_data(sample_size=0.1):  \n",
    "    # Загрузка данных из файлов Parquet  \n",
    "    attributes = pd.read_parquet(attributes_path)  \n",
    "    resnet = pd.read_parquet(resnet_path)  \n",
    "    text_and_bert = pd.read_parquet(text_and_bert_path)  \n",
    "    train = pd.read_parquet(train_path)  \n",
    "    test = pd.read_parquet(test_path)  \n",
    "\n",
    "    # Если sample_size меньше 1, то выборка данных  \n",
    "    if sample_size < 1:  \n",
    "        attributes = attributes.sample(frac=sample_size, random_state=42)  \n",
    "        resnet = resnet.sample(frac=sample_size, random_state=42)  \n",
    "        text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)  \n",
    "        train = train.sample(frac=sample_size, random_state=42)  \n",
    "        test = test.sample(frac=sample_size, random_state=42)  \n",
    "\n",
    "    # Возвращаем 5 DataFrame'ов\n",
    "    return attributes, resnet, text_and_bert, train, test  \n",
    "\n",
    "# Обработка текстовых данных\n",
    "def process_text_and_bert(df):\n",
    "    # Объединение столбцов name и description в единый текст\n",
    "    df['combined_text'] = df['name'] + ' ' + df['description']\n",
    "    return df\n",
    "\n",
    "# Объединение данных\n",
    "def merge_data(train, resnet, text_and_bert):\n",
    "    # Объединение с эмбеддингами ResNet\n",
    "    train_data = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_1'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    train_data = train_data.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_2'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Объединение с текстовыми данными и эмбеддингами BERT\n",
    "    train_data = train_data.merge(text_and_bert[['variantid', 'combined_text', 'name_bert_64']], left_on='variantid1', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'combined_text': 'text_1', 'name_bert_64': 'text_embedding_1'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    train_data = train_data.merge(text_and_bert[['variantid', 'combined_text', 'name_bert_64']], left_on='variantid2', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'combined_text': 'text_2', 'name_bert_64': 'text_embedding_2'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Удаление строк, в которых есть пропущенные значения\n",
    "    train_data = train_data.dropna()\n",
    "\n",
    "    return train_data\n",
    "\n",
    "# Объединение эмбеддингов\n",
    "def combine_embeddings(row):\n",
    "    # Объединение эмбеддингов изображений и текста\n",
    "    pic_embeddings = np.concatenate([row['pic_embeddings_1'][0], row['pic_embeddings_2'][0]])\n",
    "    text_embeddings = np.concatenate([row['text_embedding_1'], row['text_embedding_2']])\n",
    "    return np.concatenate([pic_embeddings, text_embeddings])\n",
    "\n",
    "# Подготовка данных для обучения модели\n",
    "def prepare_data(train_data):\n",
    "    # Объединение эмбеддингов изображений и текстов\n",
    "    train_data['combined_embeddings'] = train_data.apply(combine_embeddings, axis=1)\n",
    "\n",
    "    # Преобразование объединенных эмбеддингов в массив numpy\n",
    "    X = np.vstack(train_data['combined_embeddings'].values)\n",
    "    y = train_data['target']\n",
    "\n",
    "    # Разделение данных на обучающую и валидационную выборки\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "# Обучение модели\n",
    "def train_model(X_train, y_train):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Сохранение обученной модели\n",
    "    joblib.dump(model, 'random_forest.pkl')\n",
    "    return model\n",
    "\n",
    "# Оценка модели\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "    prauc = auc(recall, precision)\n",
    "    print(f'PRAUC: {prauc}')\n",
    "\n",
    "    # Сохранение векторизатора, если таковой используется\n",
    "    # Пример:\n",
    "    # joblib.dump(tfidf_vectorizer, 'vectorizer1.pk3')\n",
    "\n",
    "# Главная функция для запуска всего процесса\n",
    "def main():\n",
    "    attributes, resnet, text_and_bert, train, test = load_data()\n",
    "\n",
    "    # Обработка текстовых данных\n",
    "    text_and_bert = process_text_and_bert(text_and_bert)\n",
    "\n",
    "    # Объединение данных\n",
    "    train_data = merge_data(train, resnet, text_and_bert)\n",
    "\n",
    "    # Подготовка данных для обучения модели\n",
    "    X_train, X_val, y_train, y_val = prepare_data(train_data)\n",
    "\n",
    "    # Обучение модели\n",
    "    model = train_model(X_train, y_train)\n",
    "    \n",
    "    # Сохранение модели\n",
    "    joblib.dump(model, 'random_forest1.pk3')\n",
    "\n",
    "    # Оценка модели\n",
    "    evaluate_model(model, X_val, y_val)\n",
    "\n",
    "    # Закомментированный код для CatBoost\n",
    "    # from catboost import CatBoostClassifier\n",
    "    # model_catboost = CatBoostClassifier(iterations=1000, depth=10, learning_rate=0.1, loss_function='Logloss', eval_metric='AUC')\n",
    "    # model_catboost.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=True)\n",
    "    # joblib.dump(model_catboost, 'catboost_model.pkl')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6963f85-dccb-472e-8004-4f08e75f7362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 2s\tremaining: 33m 23s\n",
      "1:\ttotal: 3.86s\tremaining: 32m 4s\n",
      "2:\ttotal: 5.62s\tremaining: 31m 7s\n",
      "3:\ttotal: 7.41s\tremaining: 30m 45s\n",
      "4:\ttotal: 9.19s\tremaining: 30m 28s\n",
      "5:\ttotal: 11s\tremaining: 30m 20s\n",
      "6:\ttotal: 12.9s\tremaining: 30m 27s\n",
      "7:\ttotal: 14.7s\tremaining: 30m 24s\n",
      "8:\ttotal: 16.6s\tremaining: 30m 26s\n",
      "9:\ttotal: 18.4s\tremaining: 30m 21s\n",
      "10:\ttotal: 20.1s\tremaining: 30m 11s\n",
      "11:\ttotal: 22s\tremaining: 30m 13s\n",
      "12:\ttotal: 23.9s\tremaining: 30m 18s\n",
      "13:\ttotal: 25.8s\tremaining: 30m 14s\n",
      "14:\ttotal: 27.5s\tremaining: 30m 8s\n",
      "15:\ttotal: 29.3s\tremaining: 30m\n",
      "16:\ttotal: 31.1s\tremaining: 29m 58s\n",
      "17:\ttotal: 33s\tremaining: 29m 58s\n",
      "18:\ttotal: 34.7s\tremaining: 29m 51s\n",
      "19:\ttotal: 36.5s\tremaining: 29m 47s\n",
      "20:\ttotal: 38.3s\tremaining: 29m 44s\n",
      "21:\ttotal: 40.1s\tremaining: 29m 41s\n",
      "22:\ttotal: 41.8s\tremaining: 29m 36s\n",
      "23:\ttotal: 43.6s\tremaining: 29m 33s\n",
      "24:\ttotal: 45.4s\tremaining: 29m 31s\n",
      "25:\ttotal: 47.3s\tremaining: 29m 31s\n",
      "26:\ttotal: 49s\tremaining: 29m 27s\n",
      "27:\ttotal: 50.8s\tremaining: 29m 22s\n",
      "28:\ttotal: 52.6s\tremaining: 29m 20s\n",
      "29:\ttotal: 54.4s\tremaining: 29m 19s\n",
      "30:\ttotal: 56.3s\tremaining: 29m 18s\n",
      "31:\ttotal: 58.1s\tremaining: 29m 16s\n",
      "32:\ttotal: 59.9s\tremaining: 29m 15s\n",
      "33:\ttotal: 1m 1s\tremaining: 29m 14s\n",
      "34:\ttotal: 1m 3s\tremaining: 29m 12s\n",
      "35:\ttotal: 1m 5s\tremaining: 29m 13s\n",
      "36:\ttotal: 1m 7s\tremaining: 29m 12s\n",
      "37:\ttotal: 1m 9s\tremaining: 29m 13s\n",
      "38:\ttotal: 1m 11s\tremaining: 29m 12s\n",
      "39:\ttotal: 1m 12s\tremaining: 29m 10s\n",
      "40:\ttotal: 1m 14s\tremaining: 29m 6s\n",
      "41:\ttotal: 1m 16s\tremaining: 29m 5s\n",
      "42:\ttotal: 1m 18s\tremaining: 29m 4s\n",
      "43:\ttotal: 1m 20s\tremaining: 29m 3s\n",
      "44:\ttotal: 1m 22s\tremaining: 29m 1s\n",
      "45:\ttotal: 1m 23s\tremaining: 29m\n",
      "46:\ttotal: 1m 25s\tremaining: 28m 59s\n",
      "47:\ttotal: 1m 27s\tremaining: 28m 57s\n",
      "48:\ttotal: 1m 29s\tremaining: 28m 55s\n",
      "49:\ttotal: 1m 31s\tremaining: 28m 54s\n",
      "50:\ttotal: 1m 33s\tremaining: 28m 51s\n",
      "51:\ttotal: 1m 34s\tremaining: 28m 49s\n",
      "52:\ttotal: 1m 36s\tremaining: 28m 47s\n",
      "53:\ttotal: 1m 38s\tremaining: 28m 44s\n",
      "54:\ttotal: 1m 40s\tremaining: 28m 43s\n",
      "55:\ttotal: 1m 42s\tremaining: 28m 41s\n",
      "56:\ttotal: 1m 43s\tremaining: 28m 40s\n",
      "57:\ttotal: 1m 45s\tremaining: 28m 39s\n",
      "58:\ttotal: 1m 47s\tremaining: 28m 37s\n",
      "59:\ttotal: 1m 49s\tremaining: 28m 36s\n",
      "60:\ttotal: 1m 51s\tremaining: 28m 35s\n",
      "61:\ttotal: 1m 53s\tremaining: 28m 35s\n",
      "62:\ttotal: 1m 55s\tremaining: 28m 34s\n",
      "63:\ttotal: 1m 57s\tremaining: 28m 33s\n",
      "64:\ttotal: 1m 59s\tremaining: 28m 33s\n",
      "65:\ttotal: 2m\tremaining: 28m 31s\n",
      "66:\ttotal: 2m 2s\tremaining: 28m 29s\n",
      "67:\ttotal: 2m 4s\tremaining: 28m 28s\n",
      "68:\ttotal: 2m 6s\tremaining: 28m 27s\n",
      "69:\ttotal: 2m 8s\tremaining: 28m 24s\n",
      "70:\ttotal: 2m 10s\tremaining: 28m 23s\n",
      "71:\ttotal: 2m 12s\tremaining: 28m 23s\n",
      "72:\ttotal: 2m 14s\tremaining: 28m 22s\n",
      "73:\ttotal: 2m 15s\tremaining: 28m 20s\n",
      "74:\ttotal: 2m 17s\tremaining: 28m 19s\n",
      "75:\ttotal: 2m 19s\tremaining: 28m 17s\n",
      "76:\ttotal: 2m 21s\tremaining: 28m 16s\n",
      "77:\ttotal: 2m 23s\tremaining: 28m 13s\n",
      "78:\ttotal: 2m 25s\tremaining: 28m 11s\n",
      "79:\ttotal: 2m 27s\tremaining: 28m 10s\n",
      "80:\ttotal: 2m 28s\tremaining: 28m 9s\n",
      "81:\ttotal: 2m 30s\tremaining: 28m 7s\n",
      "82:\ttotal: 2m 32s\tremaining: 28m 5s\n",
      "83:\ttotal: 2m 34s\tremaining: 28m 5s\n",
      "84:\ttotal: 2m 36s\tremaining: 28m 3s\n",
      "85:\ttotal: 2m 38s\tremaining: 28m 1s\n",
      "86:\ttotal: 2m 40s\tremaining: 28m\n",
      "87:\ttotal: 2m 42s\tremaining: 27m 59s\n",
      "88:\ttotal: 2m 43s\tremaining: 27m 57s\n",
      "89:\ttotal: 2m 45s\tremaining: 27m 56s\n",
      "90:\ttotal: 2m 47s\tremaining: 27m 54s\n",
      "91:\ttotal: 2m 49s\tremaining: 27m 53s\n",
      "92:\ttotal: 2m 51s\tremaining: 27m 51s\n",
      "93:\ttotal: 2m 53s\tremaining: 27m 50s\n",
      "94:\ttotal: 2m 55s\tremaining: 27m 48s\n",
      "95:\ttotal: 2m 57s\tremaining: 27m 47s\n",
      "96:\ttotal: 2m 58s\tremaining: 27m 45s\n",
      "97:\ttotal: 3m\tremaining: 27m 44s\n",
      "98:\ttotal: 3m 2s\tremaining: 27m 43s\n",
      "99:\ttotal: 3m 4s\tremaining: 27m 41s\n",
      "100:\ttotal: 3m 6s\tremaining: 27m 39s\n",
      "101:\ttotal: 3m 8s\tremaining: 27m 38s\n",
      "102:\ttotal: 3m 10s\tremaining: 27m 36s\n",
      "103:\ttotal: 3m 12s\tremaining: 27m 35s\n",
      "104:\ttotal: 3m 13s\tremaining: 27m 33s\n",
      "105:\ttotal: 3m 15s\tremaining: 27m 31s\n",
      "106:\ttotal: 3m 17s\tremaining: 27m 30s\n",
      "107:\ttotal: 3m 19s\tremaining: 27m 29s\n",
      "108:\ttotal: 3m 21s\tremaining: 27m 27s\n",
      "109:\ttotal: 3m 23s\tremaining: 27m 24s\n",
      "110:\ttotal: 3m 25s\tremaining: 27m 23s\n",
      "111:\ttotal: 3m 27s\tremaining: 27m 21s\n",
      "112:\ttotal: 3m 28s\tremaining: 27m 19s\n",
      "113:\ttotal: 3m 30s\tremaining: 27m 18s\n",
      "114:\ttotal: 3m 32s\tremaining: 27m 17s\n",
      "115:\ttotal: 3m 34s\tremaining: 27m 16s\n",
      "116:\ttotal: 3m 36s\tremaining: 27m 14s\n",
      "117:\ttotal: 3m 38s\tremaining: 27m 12s\n",
      "118:\ttotal: 3m 40s\tremaining: 27m 11s\n",
      "119:\ttotal: 3m 42s\tremaining: 27m 9s\n",
      "120:\ttotal: 3m 44s\tremaining: 27m 7s\n",
      "121:\ttotal: 3m 45s\tremaining: 27m 5s\n",
      "122:\ttotal: 3m 47s\tremaining: 27m 4s\n",
      "123:\ttotal: 3m 49s\tremaining: 27m 3s\n",
      "124:\ttotal: 3m 51s\tremaining: 27m 1s\n",
      "125:\ttotal: 3m 53s\tremaining: 27m\n",
      "126:\ttotal: 3m 55s\tremaining: 26m 59s\n",
      "127:\ttotal: 3m 57s\tremaining: 26m 58s\n",
      "128:\ttotal: 3m 59s\tremaining: 26m 56s\n",
      "129:\ttotal: 4m 1s\tremaining: 26m 55s\n",
      "130:\ttotal: 4m 3s\tremaining: 26m 53s\n",
      "131:\ttotal: 4m 5s\tremaining: 26m 51s\n",
      "132:\ttotal: 4m 6s\tremaining: 26m 49s\n",
      "133:\ttotal: 4m 8s\tremaining: 26m 47s\n",
      "134:\ttotal: 4m 10s\tremaining: 26m 45s\n",
      "135:\ttotal: 4m 12s\tremaining: 26m 43s\n",
      "136:\ttotal: 4m 14s\tremaining: 26m 41s\n",
      "137:\ttotal: 4m 16s\tremaining: 26m 39s\n",
      "138:\ttotal: 4m 17s\tremaining: 26m 38s\n",
      "139:\ttotal: 4m 19s\tremaining: 26m 36s\n",
      "140:\ttotal: 4m 21s\tremaining: 26m 34s\n",
      "141:\ttotal: 4m 23s\tremaining: 26m 33s\n",
      "142:\ttotal: 4m 25s\tremaining: 26m 31s\n",
      "143:\ttotal: 4m 27s\tremaining: 26m 30s\n",
      "144:\ttotal: 4m 29s\tremaining: 26m 28s\n",
      "145:\ttotal: 4m 31s\tremaining: 26m 27s\n",
      "146:\ttotal: 4m 33s\tremaining: 26m 25s\n",
      "147:\ttotal: 4m 35s\tremaining: 26m 24s\n",
      "148:\ttotal: 4m 37s\tremaining: 26m 22s\n",
      "149:\ttotal: 4m 39s\tremaining: 26m 21s\n",
      "150:\ttotal: 4m 40s\tremaining: 26m 19s\n",
      "151:\ttotal: 4m 42s\tremaining: 26m 18s\n",
      "152:\ttotal: 4m 44s\tremaining: 26m 15s\n",
      "153:\ttotal: 4m 46s\tremaining: 26m 13s\n",
      "154:\ttotal: 4m 48s\tremaining: 26m 11s\n",
      "155:\ttotal: 4m 50s\tremaining: 26m 9s\n",
      "156:\ttotal: 4m 52s\tremaining: 26m 8s\n",
      "157:\ttotal: 4m 53s\tremaining: 26m 6s\n",
      "158:\ttotal: 4m 55s\tremaining: 26m 4s\n",
      "159:\ttotal: 4m 57s\tremaining: 26m 2s\n",
      "160:\ttotal: 4m 59s\tremaining: 25m 59s\n",
      "161:\ttotal: 5m 1s\tremaining: 25m 57s\n",
      "162:\ttotal: 5m 2s\tremaining: 25m 55s\n",
      "163:\ttotal: 5m 4s\tremaining: 25m 53s\n",
      "164:\ttotal: 5m 6s\tremaining: 25m 51s\n",
      "165:\ttotal: 5m 8s\tremaining: 25m 49s\n",
      "166:\ttotal: 5m 10s\tremaining: 25m 47s\n",
      "167:\ttotal: 5m 11s\tremaining: 25m 45s\n",
      "168:\ttotal: 5m 13s\tremaining: 25m 43s\n",
      "169:\ttotal: 5m 15s\tremaining: 25m 41s\n",
      "170:\ttotal: 5m 17s\tremaining: 25m 39s\n",
      "171:\ttotal: 5m 19s\tremaining: 25m 36s\n",
      "172:\ttotal: 5m 21s\tremaining: 25m 34s\n",
      "173:\ttotal: 5m 22s\tremaining: 25m 32s\n",
      "174:\ttotal: 5m 24s\tremaining: 25m 30s\n",
      "175:\ttotal: 5m 26s\tremaining: 25m 28s\n",
      "176:\ttotal: 5m 28s\tremaining: 25m 26s\n",
      "177:\ttotal: 5m 30s\tremaining: 25m 24s\n",
      "178:\ttotal: 5m 31s\tremaining: 25m 22s\n",
      "179:\ttotal: 5m 33s\tremaining: 25m 19s\n",
      "180:\ttotal: 5m 35s\tremaining: 25m 17s\n",
      "181:\ttotal: 5m 37s\tremaining: 25m 15s\n",
      "182:\ttotal: 5m 38s\tremaining: 25m 13s\n",
      "183:\ttotal: 5m 40s\tremaining: 25m 10s\n",
      "184:\ttotal: 5m 42s\tremaining: 25m 8s\n",
      "185:\ttotal: 5m 44s\tremaining: 25m 6s\n",
      "186:\ttotal: 5m 45s\tremaining: 25m 3s\n",
      "187:\ttotal: 5m 47s\tremaining: 25m 1s\n",
      "188:\ttotal: 5m 49s\tremaining: 24m 58s\n",
      "189:\ttotal: 5m 50s\tremaining: 24m 56s\n",
      "190:\ttotal: 5m 52s\tremaining: 24m 53s\n",
      "191:\ttotal: 5m 54s\tremaining: 24m 51s\n",
      "192:\ttotal: 5m 56s\tremaining: 24m 49s\n",
      "193:\ttotal: 5m 58s\tremaining: 24m 47s\n",
      "194:\ttotal: 5m 59s\tremaining: 24m 45s\n",
      "195:\ttotal: 6m 1s\tremaining: 24m 42s\n",
      "196:\ttotal: 6m 3s\tremaining: 24m 40s\n",
      "197:\ttotal: 6m 4s\tremaining: 24m 38s\n",
      "198:\ttotal: 6m 6s\tremaining: 24m 36s\n",
      "199:\ttotal: 6m 8s\tremaining: 24m 34s\n",
      "200:\ttotal: 6m 10s\tremaining: 24m 31s\n",
      "201:\ttotal: 6m 11s\tremaining: 24m 29s\n",
      "202:\ttotal: 6m 13s\tremaining: 24m 26s\n",
      "203:\ttotal: 6m 15s\tremaining: 24m 24s\n",
      "204:\ttotal: 6m 17s\tremaining: 24m 22s\n",
      "205:\ttotal: 6m 18s\tremaining: 24m 20s\n",
      "206:\ttotal: 6m 20s\tremaining: 24m 18s\n",
      "207:\ttotal: 6m 22s\tremaining: 24m 16s\n",
      "208:\ttotal: 6m 24s\tremaining: 24m 14s\n",
      "209:\ttotal: 6m 26s\tremaining: 24m 12s\n",
      "210:\ttotal: 6m 27s\tremaining: 24m 10s\n",
      "211:\ttotal: 6m 29s\tremaining: 24m 8s\n",
      "212:\ttotal: 6m 31s\tremaining: 24m 7s\n",
      "213:\ttotal: 6m 33s\tremaining: 24m 5s\n",
      "214:\ttotal: 6m 35s\tremaining: 24m 3s\n",
      "215:\ttotal: 6m 37s\tremaining: 24m 1s\n",
      "216:\ttotal: 6m 38s\tremaining: 23m 59s\n",
      "217:\ttotal: 6m 40s\tremaining: 23m 56s\n",
      "218:\ttotal: 6m 42s\tremaining: 23m 54s\n",
      "219:\ttotal: 6m 44s\tremaining: 23m 52s\n",
      "220:\ttotal: 6m 45s\tremaining: 23m 50s\n",
      "221:\ttotal: 6m 47s\tremaining: 23m 48s\n",
      "222:\ttotal: 6m 49s\tremaining: 23m 46s\n",
      "223:\ttotal: 6m 51s\tremaining: 23m 44s\n",
      "224:\ttotal: 6m 53s\tremaining: 23m 42s\n",
      "225:\ttotal: 6m 54s\tremaining: 23m 41s\n",
      "226:\ttotal: 6m 56s\tremaining: 23m 39s\n",
      "227:\ttotal: 6m 58s\tremaining: 23m 37s\n",
      "228:\ttotal: 7m\tremaining: 23m 35s\n",
      "229:\ttotal: 7m 2s\tremaining: 23m 33s\n",
      "230:\ttotal: 7m 4s\tremaining: 23m 31s\n",
      "231:\ttotal: 7m 5s\tremaining: 23m 29s\n",
      "232:\ttotal: 7m 7s\tremaining: 23m 27s\n",
      "233:\ttotal: 7m 9s\tremaining: 23m 25s\n",
      "234:\ttotal: 7m 11s\tremaining: 23m 23s\n",
      "235:\ttotal: 7m 13s\tremaining: 23m 22s\n",
      "236:\ttotal: 7m 14s\tremaining: 23m 20s\n",
      "237:\ttotal: 7m 16s\tremaining: 23m 18s\n",
      "238:\ttotal: 7m 18s\tremaining: 23m 16s\n",
      "239:\ttotal: 7m 20s\tremaining: 23m 14s\n",
      "240:\ttotal: 7m 22s\tremaining: 23m 12s\n",
      "241:\ttotal: 7m 23s\tremaining: 23m 10s\n",
      "242:\ttotal: 7m 25s\tremaining: 23m 8s\n",
      "243:\ttotal: 7m 27s\tremaining: 23m 6s\n",
      "244:\ttotal: 7m 29s\tremaining: 23m 4s\n",
      "245:\ttotal: 7m 31s\tremaining: 23m 2s\n",
      "246:\ttotal: 7m 32s\tremaining: 23m\n",
      "247:\ttotal: 7m 34s\tremaining: 22m 58s\n",
      "248:\ttotal: 7m 36s\tremaining: 22m 56s\n",
      "249:\ttotal: 7m 38s\tremaining: 22m 54s\n",
      "250:\ttotal: 7m 40s\tremaining: 22m 52s\n",
      "251:\ttotal: 7m 41s\tremaining: 22m 51s\n",
      "252:\ttotal: 7m 43s\tremaining: 22m 49s\n",
      "253:\ttotal: 7m 45s\tremaining: 22m 47s\n",
      "254:\ttotal: 7m 47s\tremaining: 22m 45s\n",
      "255:\ttotal: 7m 49s\tremaining: 22m 43s\n",
      "256:\ttotal: 7m 50s\tremaining: 22m 41s\n",
      "257:\ttotal: 7m 52s\tremaining: 22m 39s\n",
      "258:\ttotal: 7m 54s\tremaining: 22m 37s\n",
      "259:\ttotal: 7m 56s\tremaining: 22m 36s\n",
      "260:\ttotal: 7m 58s\tremaining: 22m 34s\n",
      "261:\ttotal: 8m\tremaining: 22m 32s\n",
      "262:\ttotal: 8m 1s\tremaining: 22m 30s\n",
      "263:\ttotal: 8m 3s\tremaining: 22m 28s\n",
      "264:\ttotal: 8m 5s\tremaining: 22m 26s\n",
      "265:\ttotal: 8m 7s\tremaining: 22m 24s\n",
      "266:\ttotal: 8m 9s\tremaining: 22m 22s\n",
      "267:\ttotal: 8m 10s\tremaining: 22m 20s\n",
      "268:\ttotal: 8m 12s\tremaining: 22m 18s\n",
      "269:\ttotal: 8m 14s\tremaining: 22m 16s\n",
      "270:\ttotal: 8m 16s\tremaining: 22m 15s\n",
      "271:\ttotal: 8m 18s\tremaining: 22m 13s\n",
      "272:\ttotal: 8m 19s\tremaining: 22m 11s\n",
      "273:\ttotal: 8m 21s\tremaining: 22m 9s\n",
      "274:\ttotal: 8m 23s\tremaining: 22m 7s\n",
      "275:\ttotal: 8m 25s\tremaining: 22m 5s\n",
      "276:\ttotal: 8m 26s\tremaining: 22m 3s\n",
      "277:\ttotal: 8m 28s\tremaining: 22m 1s\n",
      "278:\ttotal: 8m 30s\tremaining: 21m 59s\n",
      "279:\ttotal: 8m 32s\tremaining: 21m 57s\n",
      "280:\ttotal: 8m 34s\tremaining: 21m 55s\n",
      "281:\ttotal: 8m 35s\tremaining: 21m 53s\n",
      "282:\ttotal: 8m 37s\tremaining: 21m 51s\n",
      "283:\ttotal: 8m 39s\tremaining: 21m 49s\n",
      "284:\ttotal: 8m 41s\tremaining: 21m 47s\n",
      "285:\ttotal: 8m 43s\tremaining: 21m 46s\n",
      "286:\ttotal: 8m 45s\tremaining: 21m 44s\n",
      "287:\ttotal: 8m 46s\tremaining: 21m 42s\n",
      "288:\ttotal: 8m 48s\tremaining: 21m 40s\n",
      "289:\ttotal: 8m 50s\tremaining: 21m 38s\n",
      "290:\ttotal: 8m 52s\tremaining: 21m 36s\n",
      "291:\ttotal: 8m 54s\tremaining: 21m 35s\n",
      "292:\ttotal: 8m 56s\tremaining: 21m 33s\n",
      "293:\ttotal: 8m 57s\tremaining: 21m 31s\n",
      "294:\ttotal: 8m 59s\tremaining: 21m 29s\n",
      "295:\ttotal: 9m 1s\tremaining: 21m 27s\n",
      "296:\ttotal: 9m 3s\tremaining: 21m 26s\n",
      "297:\ttotal: 9m 5s\tremaining: 21m 24s\n",
      "298:\ttotal: 9m 7s\tremaining: 21m 22s\n",
      "299:\ttotal: 9m 8s\tremaining: 21m 20s\n",
      "300:\ttotal: 9m 10s\tremaining: 21m 19s\n",
      "301:\ttotal: 9m 12s\tremaining: 21m 17s\n",
      "302:\ttotal: 9m 14s\tremaining: 21m 15s\n",
      "303:\ttotal: 9m 16s\tremaining: 21m 13s\n",
      "304:\ttotal: 9m 18s\tremaining: 21m 11s\n",
      "305:\ttotal: 9m 19s\tremaining: 21m 9s\n",
      "306:\ttotal: 9m 21s\tremaining: 21m 7s\n",
      "307:\ttotal: 9m 23s\tremaining: 21m 6s\n",
      "308:\ttotal: 9m 25s\tremaining: 21m 4s\n",
      "309:\ttotal: 9m 27s\tremaining: 21m 2s\n",
      "310:\ttotal: 9m 29s\tremaining: 21m\n",
      "311:\ttotal: 9m 30s\tremaining: 20m 59s\n",
      "312:\ttotal: 9m 32s\tremaining: 20m 57s\n",
      "313:\ttotal: 9m 34s\tremaining: 20m 55s\n",
      "314:\ttotal: 9m 36s\tremaining: 20m 53s\n",
      "315:\ttotal: 9m 38s\tremaining: 20m 52s\n",
      "316:\ttotal: 9m 40s\tremaining: 20m 50s\n",
      "317:\ttotal: 9m 42s\tremaining: 20m 48s\n",
      "318:\ttotal: 9m 44s\tremaining: 20m 47s\n",
      "319:\ttotal: 9m 46s\tremaining: 20m 45s\n",
      "320:\ttotal: 9m 47s\tremaining: 20m 43s\n",
      "321:\ttotal: 9m 49s\tremaining: 20m 41s\n",
      "322:\ttotal: 9m 51s\tremaining: 20m 39s\n",
      "323:\ttotal: 9m 53s\tremaining: 20m 37s\n",
      "324:\ttotal: 9m 55s\tremaining: 20m 36s\n",
      "325:\ttotal: 9m 56s\tremaining: 20m 34s\n",
      "326:\ttotal: 9m 58s\tremaining: 20m 32s\n",
      "327:\ttotal: 10m\tremaining: 20m 30s\n",
      "328:\ttotal: 10m 2s\tremaining: 20m 29s\n",
      "329:\ttotal: 10m 4s\tremaining: 20m 27s\n",
      "330:\ttotal: 10m 6s\tremaining: 20m 25s\n",
      "331:\ttotal: 10m 8s\tremaining: 20m 23s\n",
      "332:\ttotal: 10m 10s\tremaining: 20m 22s\n",
      "333:\ttotal: 10m 12s\tremaining: 20m 20s\n",
      "334:\ttotal: 10m 13s\tremaining: 20m 18s\n",
      "335:\ttotal: 10m 15s\tremaining: 20m 17s\n",
      "336:\ttotal: 10m 17s\tremaining: 20m 15s\n",
      "337:\ttotal: 10m 19s\tremaining: 20m 13s\n",
      "338:\ttotal: 10m 21s\tremaining: 20m 11s\n",
      "339:\ttotal: 10m 23s\tremaining: 20m 9s\n",
      "340:\ttotal: 10m 25s\tremaining: 20m 8s\n",
      "341:\ttotal: 10m 27s\tremaining: 20m 6s\n",
      "342:\ttotal: 10m 28s\tremaining: 20m 4s\n",
      "343:\ttotal: 10m 30s\tremaining: 20m 2s\n",
      "344:\ttotal: 10m 32s\tremaining: 20m 1s\n",
      "345:\ttotal: 10m 34s\tremaining: 19m 59s\n",
      "346:\ttotal: 10m 36s\tremaining: 19m 57s\n",
      "347:\ttotal: 10m 38s\tremaining: 19m 55s\n",
      "348:\ttotal: 10m 39s\tremaining: 19m 53s\n",
      "349:\ttotal: 10m 41s\tremaining: 19m 51s\n",
      "350:\ttotal: 10m 43s\tremaining: 19m 49s\n",
      "351:\ttotal: 10m 45s\tremaining: 19m 48s\n",
      "352:\ttotal: 10m 47s\tremaining: 19m 46s\n",
      "353:\ttotal: 10m 49s\tremaining: 19m 44s\n",
      "354:\ttotal: 10m 50s\tremaining: 19m 42s\n",
      "355:\ttotal: 10m 52s\tremaining: 19m 41s\n",
      "356:\ttotal: 10m 54s\tremaining: 19m 39s\n",
      "357:\ttotal: 10m 56s\tremaining: 19m 37s\n",
      "358:\ttotal: 10m 58s\tremaining: 19m 35s\n",
      "359:\ttotal: 11m\tremaining: 19m 33s\n",
      "360:\ttotal: 11m 2s\tremaining: 19m 32s\n",
      "361:\ttotal: 11m 3s\tremaining: 19m 30s\n",
      "362:\ttotal: 11m 5s\tremaining: 19m 28s\n",
      "363:\ttotal: 11m 7s\tremaining: 19m 26s\n",
      "364:\ttotal: 11m 9s\tremaining: 19m 24s\n",
      "365:\ttotal: 11m 11s\tremaining: 19m 22s\n",
      "366:\ttotal: 11m 13s\tremaining: 19m 20s\n",
      "367:\ttotal: 11m 14s\tremaining: 19m 19s\n",
      "368:\ttotal: 11m 16s\tremaining: 19m 17s\n",
      "369:\ttotal: 11m 18s\tremaining: 19m 15s\n",
      "370:\ttotal: 11m 20s\tremaining: 19m 13s\n",
      "371:\ttotal: 11m 22s\tremaining: 19m 11s\n",
      "372:\ttotal: 11m 24s\tremaining: 19m 10s\n",
      "373:\ttotal: 11m 26s\tremaining: 19m 8s\n",
      "374:\ttotal: 11m 28s\tremaining: 19m 6s\n",
      "375:\ttotal: 11m 29s\tremaining: 19m 4s\n",
      "376:\ttotal: 11m 31s\tremaining: 19m 3s\n",
      "377:\ttotal: 11m 33s\tremaining: 19m 1s\n",
      "378:\ttotal: 11m 35s\tremaining: 18m 59s\n",
      "379:\ttotal: 11m 37s\tremaining: 18m 57s\n",
      "380:\ttotal: 11m 39s\tremaining: 18m 55s\n",
      "381:\ttotal: 11m 41s\tremaining: 18m 54s\n",
      "382:\ttotal: 11m 42s\tremaining: 18m 52s\n",
      "383:\ttotal: 11m 44s\tremaining: 18m 50s\n",
      "384:\ttotal: 11m 46s\tremaining: 18m 48s\n",
      "385:\ttotal: 11m 48s\tremaining: 18m 47s\n",
      "386:\ttotal: 11m 50s\tremaining: 18m 45s\n",
      "387:\ttotal: 11m 52s\tremaining: 18m 43s\n",
      "388:\ttotal: 11m 54s\tremaining: 18m 41s\n",
      "389:\ttotal: 11m 55s\tremaining: 18m 39s\n",
      "390:\ttotal: 11m 57s\tremaining: 18m 38s\n",
      "391:\ttotal: 11m 59s\tremaining: 18m 36s\n",
      "392:\ttotal: 12m 1s\tremaining: 18m 34s\n",
      "393:\ttotal: 12m 3s\tremaining: 18m 32s\n",
      "394:\ttotal: 12m 5s\tremaining: 18m 30s\n",
      "395:\ttotal: 12m 7s\tremaining: 18m 29s\n",
      "396:\ttotal: 12m 8s\tremaining: 18m 27s\n",
      "397:\ttotal: 12m 10s\tremaining: 18m 25s\n",
      "398:\ttotal: 12m 12s\tremaining: 18m 23s\n",
      "399:\ttotal: 12m 14s\tremaining: 18m 21s\n",
      "400:\ttotal: 12m 16s\tremaining: 18m 20s\n",
      "401:\ttotal: 12m 18s\tremaining: 18m 18s\n",
      "402:\ttotal: 12m 20s\tremaining: 18m 16s\n",
      "403:\ttotal: 12m 22s\tremaining: 18m 14s\n",
      "404:\ttotal: 12m 24s\tremaining: 18m 13s\n",
      "405:\ttotal: 12m 25s\tremaining: 18m 11s\n",
      "406:\ttotal: 12m 27s\tremaining: 18m 9s\n",
      "407:\ttotal: 12m 29s\tremaining: 18m 7s\n",
      "408:\ttotal: 12m 31s\tremaining: 18m 6s\n",
      "409:\ttotal: 12m 33s\tremaining: 18m 4s\n",
      "410:\ttotal: 12m 35s\tremaining: 18m 2s\n",
      "411:\ttotal: 12m 37s\tremaining: 18m\n",
      "412:\ttotal: 12m 39s\tremaining: 17m 59s\n",
      "413:\ttotal: 12m 41s\tremaining: 17m 57s\n",
      "414:\ttotal: 12m 43s\tremaining: 17m 55s\n",
      "415:\ttotal: 12m 44s\tremaining: 17m 53s\n",
      "416:\ttotal: 12m 46s\tremaining: 17m 51s\n",
      "417:\ttotal: 12m 48s\tremaining: 17m 50s\n",
      "418:\ttotal: 12m 50s\tremaining: 17m 48s\n",
      "419:\ttotal: 12m 52s\tremaining: 17m 46s\n",
      "420:\ttotal: 12m 54s\tremaining: 17m 44s\n",
      "421:\ttotal: 12m 55s\tremaining: 17m 42s\n",
      "422:\ttotal: 12m 57s\tremaining: 17m 40s\n",
      "423:\ttotal: 12m 59s\tremaining: 17m 39s\n",
      "424:\ttotal: 13m 1s\tremaining: 17m 37s\n",
      "425:\ttotal: 13m 3s\tremaining: 17m 35s\n",
      "426:\ttotal: 13m 5s\tremaining: 17m 33s\n",
      "427:\ttotal: 13m 7s\tremaining: 17m 31s\n",
      "428:\ttotal: 13m 8s\tremaining: 17m 30s\n",
      "429:\ttotal: 13m 10s\tremaining: 17m 28s\n",
      "430:\ttotal: 13m 12s\tremaining: 17m 26s\n",
      "431:\ttotal: 13m 14s\tremaining: 17m 24s\n",
      "432:\ttotal: 13m 16s\tremaining: 17m 22s\n",
      "433:\ttotal: 13m 18s\tremaining: 17m 21s\n",
      "434:\ttotal: 13m 20s\tremaining: 17m 19s\n",
      "435:\ttotal: 13m 22s\tremaining: 17m 17s\n",
      "436:\ttotal: 13m 23s\tremaining: 17m 15s\n",
      "437:\ttotal: 13m 25s\tremaining: 17m 13s\n",
      "438:\ttotal: 13m 27s\tremaining: 17m 11s\n",
      "439:\ttotal: 13m 29s\tremaining: 17m 10s\n",
      "440:\ttotal: 13m 31s\tremaining: 17m 8s\n",
      "441:\ttotal: 13m 32s\tremaining: 17m 6s\n",
      "442:\ttotal: 13m 34s\tremaining: 17m 4s\n",
      "443:\ttotal: 13m 36s\tremaining: 17m 2s\n",
      "444:\ttotal: 13m 38s\tremaining: 17m\n",
      "445:\ttotal: 13m 40s\tremaining: 16m 59s\n",
      "446:\ttotal: 13m 42s\tremaining: 16m 57s\n",
      "447:\ttotal: 13m 44s\tremaining: 16m 55s\n",
      "448:\ttotal: 13m 45s\tremaining: 16m 53s\n",
      "449:\ttotal: 13m 47s\tremaining: 16m 51s\n",
      "450:\ttotal: 13m 49s\tremaining: 16m 49s\n",
      "451:\ttotal: 13m 51s\tremaining: 16m 47s\n",
      "452:\ttotal: 13m 53s\tremaining: 16m 46s\n",
      "453:\ttotal: 13m 54s\tremaining: 16m 44s\n",
      "454:\ttotal: 13m 56s\tremaining: 16m 42s\n",
      "455:\ttotal: 13m 58s\tremaining: 16m 40s\n",
      "456:\ttotal: 14m\tremaining: 16m 38s\n",
      "457:\ttotal: 14m 2s\tremaining: 16m 36s\n",
      "458:\ttotal: 14m 4s\tremaining: 16m 34s\n",
      "459:\ttotal: 14m 5s\tremaining: 16m 33s\n",
      "460:\ttotal: 14m 7s\tremaining: 16m 31s\n",
      "461:\ttotal: 14m 9s\tremaining: 16m 29s\n",
      "462:\ttotal: 14m 11s\tremaining: 16m 27s\n",
      "463:\ttotal: 14m 13s\tremaining: 16m 25s\n",
      "464:\ttotal: 14m 15s\tremaining: 16m 24s\n",
      "465:\ttotal: 14m 17s\tremaining: 16m 22s\n",
      "466:\ttotal: 14m 19s\tremaining: 16m 20s\n",
      "467:\ttotal: 14m 21s\tremaining: 16m 18s\n",
      "468:\ttotal: 14m 22s\tremaining: 16m 16s\n",
      "469:\ttotal: 14m 24s\tremaining: 16m 15s\n",
      "470:\ttotal: 14m 26s\tremaining: 16m 13s\n",
      "471:\ttotal: 14m 28s\tremaining: 16m 11s\n",
      "472:\ttotal: 14m 30s\tremaining: 16m 9s\n",
      "473:\ttotal: 14m 32s\tremaining: 16m 8s\n",
      "474:\ttotal: 14m 34s\tremaining: 16m 6s\n",
      "475:\ttotal: 14m 36s\tremaining: 16m 4s\n",
      "476:\ttotal: 14m 37s\tremaining: 16m 2s\n",
      "477:\ttotal: 14m 39s\tremaining: 16m\n",
      "478:\ttotal: 14m 41s\tremaining: 15m 59s\n",
      "479:\ttotal: 14m 43s\tremaining: 15m 57s\n",
      "480:\ttotal: 14m 45s\tremaining: 15m 55s\n",
      "481:\ttotal: 14m 47s\tremaining: 15m 53s\n",
      "482:\ttotal: 14m 49s\tremaining: 15m 51s\n",
      "483:\ttotal: 14m 51s\tremaining: 15m 49s\n",
      "484:\ttotal: 14m 52s\tremaining: 15m 48s\n",
      "485:\ttotal: 14m 54s\tremaining: 15m 46s\n",
      "486:\ttotal: 14m 56s\tremaining: 15m 44s\n",
      "487:\ttotal: 14m 58s\tremaining: 15m 42s\n",
      "488:\ttotal: 15m\tremaining: 15m 40s\n",
      "489:\ttotal: 15m 2s\tremaining: 15m 38s\n",
      "490:\ttotal: 15m 4s\tremaining: 15m 37s\n",
      "491:\ttotal: 15m 5s\tremaining: 15m 35s\n",
      "492:\ttotal: 15m 7s\tremaining: 15m 33s\n",
      "493:\ttotal: 15m 9s\tremaining: 15m 31s\n",
      "494:\ttotal: 15m 11s\tremaining: 15m 29s\n",
      "495:\ttotal: 15m 13s\tremaining: 15m 27s\n",
      "496:\ttotal: 15m 15s\tremaining: 15m 26s\n",
      "497:\ttotal: 15m 16s\tremaining: 15m 24s\n",
      "498:\ttotal: 15m 18s\tremaining: 15m 22s\n",
      "499:\ttotal: 15m 20s\tremaining: 15m 20s\n",
      "500:\ttotal: 15m 22s\tremaining: 15m 18s\n",
      "501:\ttotal: 15m 24s\tremaining: 15m 17s\n",
      "502:\ttotal: 15m 26s\tremaining: 15m 15s\n",
      "503:\ttotal: 15m 28s\tremaining: 15m 13s\n",
      "504:\ttotal: 15m 30s\tremaining: 15m 11s\n",
      "505:\ttotal: 15m 31s\tremaining: 15m 9s\n",
      "506:\ttotal: 15m 33s\tremaining: 15m 8s\n",
      "507:\ttotal: 15m 35s\tremaining: 15m 6s\n",
      "508:\ttotal: 15m 37s\tremaining: 15m 4s\n",
      "509:\ttotal: 15m 39s\tremaining: 15m 2s\n",
      "510:\ttotal: 15m 41s\tremaining: 15m\n",
      "511:\ttotal: 15m 42s\tremaining: 14m 58s\n",
      "512:\ttotal: 15m 44s\tremaining: 14m 56s\n",
      "513:\ttotal: 15m 46s\tremaining: 14m 55s\n",
      "514:\ttotal: 15m 48s\tremaining: 14m 53s\n",
      "515:\ttotal: 15m 50s\tremaining: 14m 51s\n",
      "516:\ttotal: 15m 52s\tremaining: 14m 49s\n",
      "517:\ttotal: 15m 54s\tremaining: 14m 48s\n",
      "518:\ttotal: 15m 56s\tremaining: 14m 46s\n",
      "519:\ttotal: 15m 58s\tremaining: 14m 44s\n",
      "520:\ttotal: 16m\tremaining: 14m 42s\n",
      "521:\ttotal: 16m 1s\tremaining: 14m 40s\n",
      "522:\ttotal: 16m 3s\tremaining: 14m 39s\n",
      "523:\ttotal: 16m 5s\tremaining: 14m 37s\n",
      "524:\ttotal: 16m 7s\tremaining: 14m 35s\n",
      "525:\ttotal: 16m 9s\tremaining: 14m 33s\n",
      "526:\ttotal: 16m 11s\tremaining: 14m 31s\n",
      "527:\ttotal: 16m 13s\tremaining: 14m 29s\n",
      "528:\ttotal: 16m 15s\tremaining: 14m 28s\n",
      "529:\ttotal: 16m 16s\tremaining: 14m 26s\n",
      "530:\ttotal: 16m 18s\tremaining: 14m 24s\n",
      "531:\ttotal: 16m 20s\tremaining: 14m 22s\n",
      "532:\ttotal: 16m 22s\tremaining: 14m 20s\n",
      "533:\ttotal: 16m 24s\tremaining: 14m 18s\n",
      "534:\ttotal: 16m 26s\tremaining: 14m 17s\n",
      "535:\ttotal: 16m 27s\tremaining: 14m 15s\n",
      "536:\ttotal: 16m 29s\tremaining: 14m 13s\n",
      "537:\ttotal: 16m 31s\tremaining: 14m 11s\n",
      "538:\ttotal: 16m 33s\tremaining: 14m 9s\n",
      "539:\ttotal: 16m 35s\tremaining: 14m 7s\n",
      "540:\ttotal: 16m 37s\tremaining: 14m 5s\n",
      "541:\ttotal: 16m 39s\tremaining: 14m 4s\n",
      "542:\ttotal: 16m 41s\tremaining: 14m 2s\n",
      "543:\ttotal: 16m 42s\tremaining: 14m\n",
      "544:\ttotal: 16m 44s\tremaining: 13m 58s\n",
      "545:\ttotal: 16m 46s\tremaining: 13m 56s\n",
      "546:\ttotal: 16m 48s\tremaining: 13m 55s\n",
      "547:\ttotal: 16m 50s\tremaining: 13m 53s\n",
      "548:\ttotal: 16m 52s\tremaining: 13m 51s\n",
      "549:\ttotal: 16m 54s\tremaining: 13m 49s\n",
      "550:\ttotal: 16m 55s\tremaining: 13m 47s\n",
      "551:\ttotal: 16m 57s\tremaining: 13m 45s\n",
      "552:\ttotal: 16m 59s\tremaining: 13m 44s\n",
      "553:\ttotal: 17m 1s\tremaining: 13m 42s\n",
      "554:\ttotal: 17m 3s\tremaining: 13m 40s\n",
      "555:\ttotal: 17m 5s\tremaining: 13m 38s\n",
      "556:\ttotal: 17m 7s\tremaining: 13m 36s\n",
      "557:\ttotal: 17m 9s\tremaining: 13m 35s\n",
      "558:\ttotal: 17m 10s\tremaining: 13m 33s\n",
      "559:\ttotal: 17m 12s\tremaining: 13m 31s\n",
      "560:\ttotal: 17m 14s\tremaining: 13m 29s\n",
      "561:\ttotal: 17m 16s\tremaining: 13m 27s\n",
      "562:\ttotal: 17m 18s\tremaining: 13m 26s\n",
      "563:\ttotal: 17m 20s\tremaining: 13m 24s\n",
      "564:\ttotal: 17m 22s\tremaining: 13m 22s\n",
      "565:\ttotal: 17m 24s\tremaining: 13m 20s\n",
      "566:\ttotal: 17m 25s\tremaining: 13m 18s\n",
      "567:\ttotal: 17m 27s\tremaining: 13m 16s\n",
      "568:\ttotal: 17m 29s\tremaining: 13m 14s\n",
      "569:\ttotal: 17m 31s\tremaining: 13m 13s\n",
      "570:\ttotal: 17m 33s\tremaining: 13m 11s\n",
      "571:\ttotal: 17m 34s\tremaining: 13m 9s\n",
      "572:\ttotal: 17m 36s\tremaining: 13m 7s\n",
      "573:\ttotal: 17m 38s\tremaining: 13m 5s\n",
      "574:\ttotal: 17m 40s\tremaining: 13m 3s\n",
      "575:\ttotal: 17m 42s\tremaining: 13m 2s\n",
      "576:\ttotal: 17m 44s\tremaining: 13m\n",
      "577:\ttotal: 17m 46s\tremaining: 12m 58s\n",
      "578:\ttotal: 17m 48s\tremaining: 12m 56s\n",
      "579:\ttotal: 17m 49s\tremaining: 12m 54s\n",
      "580:\ttotal: 17m 51s\tremaining: 12m 52s\n",
      "581:\ttotal: 17m 53s\tremaining: 12m 51s\n",
      "582:\ttotal: 17m 55s\tremaining: 12m 49s\n",
      "583:\ttotal: 17m 57s\tremaining: 12m 47s\n",
      "584:\ttotal: 17m 59s\tremaining: 12m 45s\n",
      "585:\ttotal: 18m 1s\tremaining: 12m 43s\n",
      "586:\ttotal: 18m 3s\tremaining: 12m 42s\n",
      "587:\ttotal: 18m 4s\tremaining: 12m 40s\n",
      "588:\ttotal: 18m 6s\tremaining: 12m 38s\n",
      "589:\ttotal: 18m 8s\tremaining: 12m 36s\n",
      "590:\ttotal: 18m 10s\tremaining: 12m 34s\n",
      "591:\ttotal: 18m 12s\tremaining: 12m 32s\n",
      "592:\ttotal: 18m 14s\tremaining: 12m 31s\n",
      "593:\ttotal: 18m 16s\tremaining: 12m 29s\n",
      "594:\ttotal: 18m 18s\tremaining: 12m 27s\n",
      "595:\ttotal: 18m 20s\tremaining: 12m 25s\n",
      "596:\ttotal: 18m 21s\tremaining: 12m 23s\n",
      "597:\ttotal: 18m 23s\tremaining: 12m 21s\n",
      "598:\ttotal: 18m 25s\tremaining: 12m 20s\n",
      "599:\ttotal: 18m 27s\tremaining: 12m 18s\n",
      "600:\ttotal: 18m 29s\tremaining: 12m 16s\n",
      "601:\ttotal: 18m 31s\tremaining: 12m 14s\n",
      "602:\ttotal: 18m 32s\tremaining: 12m 12s\n",
      "603:\ttotal: 18m 34s\tremaining: 12m 10s\n",
      "604:\ttotal: 18m 36s\tremaining: 12m 8s\n",
      "605:\ttotal: 18m 38s\tremaining: 12m 7s\n",
      "606:\ttotal: 18m 40s\tremaining: 12m 5s\n",
      "607:\ttotal: 18m 42s\tremaining: 12m 3s\n",
      "608:\ttotal: 18m 44s\tremaining: 12m 1s\n",
      "609:\ttotal: 18m 45s\tremaining: 11m 59s\n",
      "610:\ttotal: 18m 47s\tremaining: 11m 57s\n",
      "611:\ttotal: 18m 49s\tremaining: 11m 56s\n",
      "612:\ttotal: 18m 51s\tremaining: 11m 54s\n",
      "613:\ttotal: 18m 53s\tremaining: 11m 52s\n",
      "614:\ttotal: 18m 55s\tremaining: 11m 50s\n",
      "615:\ttotal: 18m 57s\tremaining: 11m 48s\n",
      "616:\ttotal: 18m 58s\tremaining: 11m 46s\n",
      "617:\ttotal: 19m\tremaining: 11m 45s\n",
      "618:\ttotal: 19m 2s\tremaining: 11m 43s\n",
      "619:\ttotal: 19m 4s\tremaining: 11m 41s\n",
      "620:\ttotal: 19m 6s\tremaining: 11m 39s\n",
      "621:\ttotal: 19m 8s\tremaining: 11m 37s\n",
      "622:\ttotal: 19m 10s\tremaining: 11m 35s\n",
      "623:\ttotal: 19m 11s\tremaining: 11m 34s\n",
      "624:\ttotal: 19m 13s\tremaining: 11m 32s\n",
      "625:\ttotal: 19m 15s\tremaining: 11m 30s\n",
      "626:\ttotal: 19m 17s\tremaining: 11m 28s\n",
      "627:\ttotal: 19m 19s\tremaining: 11m 26s\n",
      "628:\ttotal: 19m 21s\tremaining: 11m 24s\n",
      "629:\ttotal: 19m 22s\tremaining: 11m 22s\n",
      "630:\ttotal: 19m 24s\tremaining: 11m 21s\n",
      "631:\ttotal: 19m 26s\tremaining: 11m 19s\n",
      "632:\ttotal: 19m 28s\tremaining: 11m 17s\n",
      "633:\ttotal: 19m 30s\tremaining: 11m 15s\n",
      "634:\ttotal: 19m 31s\tremaining: 11m 13s\n",
      "635:\ttotal: 19m 33s\tremaining: 11m 11s\n",
      "636:\ttotal: 19m 35s\tremaining: 11m 9s\n",
      "637:\ttotal: 19m 37s\tremaining: 11m 8s\n",
      "638:\ttotal: 19m 39s\tremaining: 11m 6s\n",
      "639:\ttotal: 19m 41s\tremaining: 11m 4s\n",
      "640:\ttotal: 19m 43s\tremaining: 11m 2s\n",
      "641:\ttotal: 19m 44s\tremaining: 11m\n",
      "642:\ttotal: 19m 46s\tremaining: 10m 58s\n",
      "643:\ttotal: 19m 48s\tremaining: 10m 57s\n",
      "644:\ttotal: 19m 50s\tremaining: 10m 55s\n",
      "645:\ttotal: 19m 52s\tremaining: 10m 53s\n",
      "646:\ttotal: 19m 54s\tremaining: 10m 51s\n",
      "647:\ttotal: 19m 55s\tremaining: 10m 49s\n",
      "648:\ttotal: 19m 57s\tremaining: 10m 47s\n",
      "649:\ttotal: 19m 59s\tremaining: 10m 45s\n",
      "650:\ttotal: 20m 1s\tremaining: 10m 44s\n",
      "651:\ttotal: 20m 3s\tremaining: 10m 42s\n",
      "652:\ttotal: 20m 5s\tremaining: 10m 40s\n",
      "653:\ttotal: 20m 7s\tremaining: 10m 38s\n",
      "654:\ttotal: 20m 9s\tremaining: 10m 36s\n",
      "655:\ttotal: 20m 10s\tremaining: 10m 34s\n",
      "656:\ttotal: 20m 12s\tremaining: 10m 33s\n",
      "657:\ttotal: 20m 14s\tremaining: 10m 31s\n",
      "658:\ttotal: 20m 16s\tremaining: 10m 29s\n",
      "659:\ttotal: 20m 18s\tremaining: 10m 27s\n",
      "660:\ttotal: 20m 20s\tremaining: 10m 25s\n",
      "661:\ttotal: 20m 22s\tremaining: 10m 23s\n",
      "662:\ttotal: 20m 23s\tremaining: 10m 22s\n",
      "663:\ttotal: 20m 25s\tremaining: 10m 20s\n",
      "664:\ttotal: 20m 27s\tremaining: 10m 18s\n",
      "665:\ttotal: 20m 29s\tremaining: 10m 16s\n",
      "666:\ttotal: 20m 31s\tremaining: 10m 14s\n",
      "667:\ttotal: 20m 33s\tremaining: 10m 12s\n",
      "668:\ttotal: 20m 35s\tremaining: 10m 11s\n",
      "669:\ttotal: 20m 37s\tremaining: 10m 9s\n",
      "670:\ttotal: 20m 38s\tremaining: 10m 7s\n",
      "671:\ttotal: 20m 40s\tremaining: 10m 5s\n",
      "672:\ttotal: 20m 42s\tremaining: 10m 3s\n",
      "673:\ttotal: 20m 44s\tremaining: 10m 2s\n",
      "674:\ttotal: 20m 46s\tremaining: 10m\n",
      "675:\ttotal: 20m 48s\tremaining: 9m 58s\n",
      "676:\ttotal: 20m 50s\tremaining: 9m 56s\n",
      "677:\ttotal: 20m 52s\tremaining: 9m 54s\n",
      "678:\ttotal: 20m 53s\tremaining: 9m 52s\n",
      "679:\ttotal: 20m 55s\tremaining: 9m 50s\n",
      "680:\ttotal: 20m 57s\tremaining: 9m 49s\n",
      "681:\ttotal: 20m 59s\tremaining: 9m 47s\n",
      "682:\ttotal: 21m 1s\tremaining: 9m 45s\n",
      "683:\ttotal: 21m 3s\tremaining: 9m 43s\n",
      "684:\ttotal: 21m 4s\tremaining: 9m 41s\n",
      "685:\ttotal: 21m 6s\tremaining: 9m 39s\n",
      "686:\ttotal: 21m 8s\tremaining: 9m 37s\n",
      "687:\ttotal: 21m 10s\tremaining: 9m 36s\n",
      "688:\ttotal: 21m 12s\tremaining: 9m 34s\n",
      "689:\ttotal: 21m 14s\tremaining: 9m 32s\n",
      "690:\ttotal: 21m 16s\tremaining: 9m 30s\n",
      "691:\ttotal: 21m 17s\tremaining: 9m 28s\n",
      "692:\ttotal: 21m 19s\tremaining: 9m 26s\n",
      "693:\ttotal: 21m 21s\tremaining: 9m 25s\n",
      "694:\ttotal: 21m 23s\tremaining: 9m 23s\n",
      "695:\ttotal: 21m 25s\tremaining: 9m 21s\n",
      "696:\ttotal: 21m 27s\tremaining: 9m 19s\n",
      "697:\ttotal: 21m 29s\tremaining: 9m 17s\n",
      "698:\ttotal: 21m 30s\tremaining: 9m 15s\n",
      "699:\ttotal: 21m 32s\tremaining: 9m 14s\n",
      "700:\ttotal: 21m 34s\tremaining: 9m 12s\n",
      "701:\ttotal: 21m 36s\tremaining: 9m 10s\n",
      "702:\ttotal: 21m 38s\tremaining: 9m 8s\n",
      "703:\ttotal: 21m 40s\tremaining: 9m 6s\n",
      "704:\ttotal: 21m 42s\tremaining: 9m 4s\n",
      "705:\ttotal: 21m 43s\tremaining: 9m 3s\n",
      "706:\ttotal: 21m 45s\tremaining: 9m 1s\n",
      "707:\ttotal: 21m 47s\tremaining: 8m 59s\n",
      "708:\ttotal: 21m 49s\tremaining: 8m 57s\n",
      "709:\ttotal: 21m 51s\tremaining: 8m 55s\n",
      "710:\ttotal: 21m 53s\tremaining: 8m 53s\n",
      "711:\ttotal: 21m 55s\tremaining: 8m 51s\n",
      "712:\ttotal: 21m 56s\tremaining: 8m 50s\n",
      "713:\ttotal: 21m 58s\tremaining: 8m 48s\n",
      "714:\ttotal: 22m\tremaining: 8m 46s\n",
      "715:\ttotal: 22m 2s\tremaining: 8m 44s\n",
      "716:\ttotal: 22m 4s\tremaining: 8m 42s\n",
      "717:\ttotal: 22m 6s\tremaining: 8m 40s\n",
      "718:\ttotal: 22m 8s\tremaining: 8m 39s\n",
      "719:\ttotal: 22m 10s\tremaining: 8m 37s\n",
      "720:\ttotal: 22m 11s\tremaining: 8m 35s\n",
      "721:\ttotal: 22m 13s\tremaining: 8m 33s\n",
      "722:\ttotal: 22m 15s\tremaining: 8m 31s\n",
      "723:\ttotal: 22m 17s\tremaining: 8m 29s\n",
      "724:\ttotal: 22m 19s\tremaining: 8m 28s\n",
      "725:\ttotal: 22m 21s\tremaining: 8m 26s\n",
      "726:\ttotal: 22m 23s\tremaining: 8m 24s\n",
      "727:\ttotal: 22m 25s\tremaining: 8m 22s\n",
      "728:\ttotal: 22m 27s\tremaining: 8m 20s\n",
      "729:\ttotal: 22m 28s\tremaining: 8m 18s\n",
      "730:\ttotal: 22m 30s\tremaining: 8m 17s\n",
      "731:\ttotal: 22m 32s\tremaining: 8m 15s\n",
      "732:\ttotal: 22m 34s\tremaining: 8m 13s\n",
      "733:\ttotal: 22m 36s\tremaining: 8m 11s\n",
      "734:\ttotal: 22m 38s\tremaining: 8m 9s\n",
      "735:\ttotal: 22m 40s\tremaining: 8m 7s\n",
      "736:\ttotal: 22m 41s\tremaining: 8m 6s\n",
      "737:\ttotal: 22m 43s\tremaining: 8m 4s\n",
      "738:\ttotal: 22m 45s\tremaining: 8m 2s\n",
      "739:\ttotal: 22m 47s\tremaining: 8m\n",
      "740:\ttotal: 22m 49s\tremaining: 7m 58s\n",
      "741:\ttotal: 22m 51s\tremaining: 7m 56s\n",
      "742:\ttotal: 22m 52s\tremaining: 7m 54s\n",
      "743:\ttotal: 22m 54s\tremaining: 7m 53s\n",
      "744:\ttotal: 22m 56s\tremaining: 7m 51s\n",
      "745:\ttotal: 22m 58s\tremaining: 7m 49s\n",
      "746:\ttotal: 23m\tremaining: 7m 47s\n",
      "747:\ttotal: 23m 2s\tremaining: 7m 45s\n",
      "748:\ttotal: 23m 4s\tremaining: 7m 43s\n",
      "749:\ttotal: 23m 6s\tremaining: 7m 42s\n",
      "750:\ttotal: 23m 8s\tremaining: 7m 40s\n",
      "751:\ttotal: 23m 9s\tremaining: 7m 38s\n",
      "752:\ttotal: 23m 11s\tremaining: 7m 36s\n",
      "753:\ttotal: 23m 13s\tremaining: 7m 34s\n",
      "754:\ttotal: 23m 15s\tremaining: 7m 32s\n",
      "755:\ttotal: 23m 17s\tremaining: 7m 30s\n",
      "756:\ttotal: 23m 19s\tremaining: 7m 29s\n",
      "757:\ttotal: 23m 21s\tremaining: 7m 27s\n",
      "758:\ttotal: 23m 22s\tremaining: 7m 25s\n",
      "759:\ttotal: 23m 24s\tremaining: 7m 23s\n",
      "760:\ttotal: 23m 26s\tremaining: 7m 21s\n",
      "761:\ttotal: 23m 28s\tremaining: 7m 19s\n",
      "762:\ttotal: 23m 30s\tremaining: 7m 18s\n",
      "763:\ttotal: 23m 32s\tremaining: 7m 16s\n",
      "764:\ttotal: 23m 34s\tremaining: 7m 14s\n",
      "765:\ttotal: 23m 35s\tremaining: 7m 12s\n",
      "766:\ttotal: 23m 37s\tremaining: 7m 10s\n",
      "767:\ttotal: 23m 39s\tremaining: 7m 8s\n",
      "768:\ttotal: 23m 41s\tremaining: 7m 6s\n",
      "769:\ttotal: 23m 43s\tremaining: 7m 5s\n",
      "770:\ttotal: 23m 45s\tremaining: 7m 3s\n",
      "771:\ttotal: 23m 46s\tremaining: 7m 1s\n",
      "772:\ttotal: 23m 48s\tremaining: 6m 59s\n",
      "773:\ttotal: 23m 50s\tremaining: 6m 57s\n",
      "774:\ttotal: 23m 52s\tremaining: 6m 55s\n",
      "775:\ttotal: 23m 54s\tremaining: 6m 54s\n",
      "776:\ttotal: 23m 56s\tremaining: 6m 52s\n",
      "777:\ttotal: 23m 58s\tremaining: 6m 50s\n",
      "778:\ttotal: 24m\tremaining: 6m 48s\n",
      "779:\ttotal: 24m 1s\tremaining: 6m 46s\n",
      "780:\ttotal: 24m 3s\tremaining: 6m 44s\n",
      "781:\ttotal: 24m 5s\tremaining: 6m 43s\n",
      "782:\ttotal: 24m 7s\tremaining: 6m 41s\n",
      "783:\ttotal: 24m 9s\tremaining: 6m 39s\n",
      "784:\ttotal: 24m 11s\tremaining: 6m 37s\n",
      "785:\ttotal: 24m 13s\tremaining: 6m 35s\n",
      "786:\ttotal: 24m 15s\tremaining: 6m 33s\n",
      "787:\ttotal: 24m 16s\tremaining: 6m 31s\n",
      "788:\ttotal: 24m 18s\tremaining: 6m 30s\n",
      "789:\ttotal: 24m 20s\tremaining: 6m 28s\n",
      "790:\ttotal: 24m 22s\tremaining: 6m 26s\n",
      "791:\ttotal: 24m 24s\tremaining: 6m 24s\n",
      "792:\ttotal: 24m 26s\tremaining: 6m 22s\n",
      "793:\ttotal: 24m 28s\tremaining: 6m 20s\n",
      "794:\ttotal: 24m 30s\tremaining: 6m 19s\n",
      "795:\ttotal: 24m 32s\tremaining: 6m 17s\n",
      "796:\ttotal: 24m 34s\tremaining: 6m 15s\n",
      "797:\ttotal: 24m 35s\tremaining: 6m 13s\n",
      "798:\ttotal: 24m 37s\tremaining: 6m 11s\n",
      "799:\ttotal: 24m 39s\tremaining: 6m 9s\n",
      "800:\ttotal: 24m 41s\tremaining: 6m 8s\n",
      "801:\ttotal: 24m 43s\tremaining: 6m 6s\n",
      "802:\ttotal: 24m 45s\tremaining: 6m 4s\n",
      "803:\ttotal: 24m 47s\tremaining: 6m 2s\n",
      "804:\ttotal: 24m 49s\tremaining: 6m\n",
      "805:\ttotal: 24m 50s\tremaining: 5m 58s\n",
      "806:\ttotal: 24m 52s\tremaining: 5m 56s\n",
      "807:\ttotal: 24m 54s\tremaining: 5m 55s\n",
      "808:\ttotal: 24m 56s\tremaining: 5m 53s\n",
      "809:\ttotal: 24m 58s\tremaining: 5m 51s\n",
      "810:\ttotal: 25m\tremaining: 5m 49s\n",
      "811:\ttotal: 25m 2s\tremaining: 5m 47s\n",
      "812:\ttotal: 25m 3s\tremaining: 5m 45s\n",
      "813:\ttotal: 25m 5s\tremaining: 5m 44s\n",
      "814:\ttotal: 25m 7s\tremaining: 5m 42s\n",
      "815:\ttotal: 25m 9s\tremaining: 5m 40s\n",
      "816:\ttotal: 25m 11s\tremaining: 5m 38s\n",
      "817:\ttotal: 25m 13s\tremaining: 5m 36s\n",
      "818:\ttotal: 25m 15s\tremaining: 5m 34s\n",
      "819:\ttotal: 25m 17s\tremaining: 5m 33s\n",
      "820:\ttotal: 25m 18s\tremaining: 5m 31s\n",
      "821:\ttotal: 25m 20s\tremaining: 5m 29s\n",
      "822:\ttotal: 25m 22s\tremaining: 5m 27s\n",
      "823:\ttotal: 25m 24s\tremaining: 5m 25s\n",
      "824:\ttotal: 25m 26s\tremaining: 5m 23s\n",
      "825:\ttotal: 25m 28s\tremaining: 5m 21s\n",
      "826:\ttotal: 25m 30s\tremaining: 5m 20s\n",
      "827:\ttotal: 25m 32s\tremaining: 5m 18s\n",
      "828:\ttotal: 25m 33s\tremaining: 5m 16s\n",
      "829:\ttotal: 25m 35s\tremaining: 5m 14s\n",
      "830:\ttotal: 25m 37s\tremaining: 5m 12s\n",
      "831:\ttotal: 25m 39s\tremaining: 5m 10s\n",
      "832:\ttotal: 25m 41s\tremaining: 5m 9s\n",
      "833:\ttotal: 25m 43s\tremaining: 5m 7s\n",
      "834:\ttotal: 25m 45s\tremaining: 5m 5s\n",
      "835:\ttotal: 25m 47s\tremaining: 5m 3s\n",
      "836:\ttotal: 25m 48s\tremaining: 5m 1s\n",
      "837:\ttotal: 25m 50s\tremaining: 4m 59s\n",
      "838:\ttotal: 25m 52s\tremaining: 4m 57s\n",
      "839:\ttotal: 25m 54s\tremaining: 4m 56s\n",
      "840:\ttotal: 25m 56s\tremaining: 4m 54s\n",
      "841:\ttotal: 25m 58s\tremaining: 4m 52s\n",
      "842:\ttotal: 26m\tremaining: 4m 50s\n",
      "843:\ttotal: 26m 2s\tremaining: 4m 48s\n",
      "844:\ttotal: 26m 3s\tremaining: 4m 46s\n",
      "845:\ttotal: 26m 5s\tremaining: 4m 45s\n",
      "846:\ttotal: 26m 7s\tremaining: 4m 43s\n",
      "847:\ttotal: 26m 9s\tremaining: 4m 41s\n",
      "848:\ttotal: 26m 11s\tremaining: 4m 39s\n",
      "849:\ttotal: 26m 13s\tremaining: 4m 37s\n",
      "850:\ttotal: 26m 15s\tremaining: 4m 35s\n",
      "851:\ttotal: 26m 17s\tremaining: 4m 33s\n",
      "852:\ttotal: 26m 18s\tremaining: 4m 32s\n",
      "853:\ttotal: 26m 20s\tremaining: 4m 30s\n",
      "854:\ttotal: 26m 22s\tremaining: 4m 28s\n",
      "855:\ttotal: 26m 24s\tremaining: 4m 26s\n",
      "856:\ttotal: 26m 26s\tremaining: 4m 24s\n",
      "857:\ttotal: 26m 28s\tremaining: 4m 22s\n",
      "858:\ttotal: 26m 30s\tremaining: 4m 20s\n",
      "859:\ttotal: 26m 31s\tremaining: 4m 19s\n",
      "860:\ttotal: 26m 33s\tremaining: 4m 17s\n",
      "861:\ttotal: 26m 35s\tremaining: 4m 15s\n",
      "862:\ttotal: 26m 37s\tremaining: 4m 13s\n",
      "863:\ttotal: 26m 39s\tremaining: 4m 11s\n",
      "864:\ttotal: 26m 40s\tremaining: 4m 9s\n",
      "865:\ttotal: 26m 42s\tremaining: 4m 7s\n",
      "866:\ttotal: 26m 44s\tremaining: 4m 6s\n",
      "867:\ttotal: 26m 46s\tremaining: 4m 4s\n",
      "868:\ttotal: 26m 48s\tremaining: 4m 2s\n",
      "869:\ttotal: 26m 49s\tremaining: 4m\n",
      "870:\ttotal: 26m 51s\tremaining: 3m 58s\n",
      "871:\ttotal: 26m 53s\tremaining: 3m 56s\n",
      "872:\ttotal: 26m 55s\tremaining: 3m 55s\n",
      "873:\ttotal: 26m 57s\tremaining: 3m 53s\n",
      "874:\ttotal: 26m 59s\tremaining: 3m 51s\n",
      "875:\ttotal: 27m 1s\tremaining: 3m 49s\n",
      "876:\ttotal: 27m 2s\tremaining: 3m 47s\n",
      "877:\ttotal: 27m 4s\tremaining: 3m 45s\n",
      "878:\ttotal: 27m 6s\tremaining: 3m 43s\n",
      "879:\ttotal: 27m 8s\tremaining: 3m 42s\n",
      "880:\ttotal: 27m 10s\tremaining: 3m 40s\n",
      "881:\ttotal: 27m 12s\tremaining: 3m 38s\n",
      "882:\ttotal: 27m 14s\tremaining: 3m 36s\n",
      "883:\ttotal: 27m 15s\tremaining: 3m 34s\n",
      "884:\ttotal: 27m 17s\tremaining: 3m 32s\n",
      "885:\ttotal: 27m 19s\tremaining: 3m 30s\n",
      "886:\ttotal: 27m 21s\tremaining: 3m 29s\n",
      "887:\ttotal: 27m 23s\tremaining: 3m 27s\n",
      "888:\ttotal: 27m 25s\tremaining: 3m 25s\n",
      "889:\ttotal: 27m 27s\tremaining: 3m 23s\n",
      "890:\ttotal: 27m 29s\tremaining: 3m 21s\n",
      "891:\ttotal: 27m 31s\tremaining: 3m 19s\n",
      "892:\ttotal: 27m 32s\tremaining: 3m 18s\n",
      "893:\ttotal: 27m 34s\tremaining: 3m 16s\n",
      "894:\ttotal: 27m 36s\tremaining: 3m 14s\n",
      "895:\ttotal: 27m 38s\tremaining: 3m 12s\n",
      "896:\ttotal: 27m 40s\tremaining: 3m 10s\n",
      "897:\ttotal: 27m 42s\tremaining: 3m 8s\n",
      "898:\ttotal: 27m 43s\tremaining: 3m 6s\n",
      "899:\ttotal: 27m 45s\tremaining: 3m 5s\n",
      "900:\ttotal: 27m 47s\tremaining: 3m 3s\n",
      "901:\ttotal: 27m 49s\tremaining: 3m 1s\n",
      "902:\ttotal: 27m 51s\tremaining: 2m 59s\n",
      "903:\ttotal: 27m 53s\tremaining: 2m 57s\n",
      "904:\ttotal: 27m 55s\tremaining: 2m 55s\n",
      "905:\ttotal: 27m 57s\tremaining: 2m 54s\n",
      "906:\ttotal: 27m 59s\tremaining: 2m 52s\n",
      "907:\ttotal: 28m\tremaining: 2m 50s\n",
      "908:\ttotal: 28m 2s\tremaining: 2m 48s\n",
      "909:\ttotal: 28m 4s\tremaining: 2m 46s\n",
      "910:\ttotal: 28m 6s\tremaining: 2m 44s\n",
      "911:\ttotal: 28m 8s\tremaining: 2m 42s\n",
      "912:\ttotal: 28m 10s\tremaining: 2m 41s\n",
      "913:\ttotal: 28m 12s\tremaining: 2m 39s\n",
      "914:\ttotal: 28m 14s\tremaining: 2m 37s\n",
      "915:\ttotal: 28m 15s\tremaining: 2m 35s\n",
      "916:\ttotal: 28m 17s\tremaining: 2m 33s\n",
      "917:\ttotal: 28m 19s\tremaining: 2m 31s\n",
      "918:\ttotal: 28m 21s\tremaining: 2m 29s\n",
      "919:\ttotal: 28m 23s\tremaining: 2m 28s\n",
      "920:\ttotal: 28m 25s\tremaining: 2m 26s\n",
      "921:\ttotal: 28m 27s\tremaining: 2m 24s\n",
      "922:\ttotal: 28m 29s\tremaining: 2m 22s\n",
      "923:\ttotal: 28m 31s\tremaining: 2m 20s\n",
      "924:\ttotal: 28m 32s\tremaining: 2m 18s\n",
      "925:\ttotal: 28m 34s\tremaining: 2m 17s\n",
      "926:\ttotal: 28m 36s\tremaining: 2m 15s\n",
      "927:\ttotal: 28m 38s\tremaining: 2m 13s\n",
      "928:\ttotal: 28m 40s\tremaining: 2m 11s\n",
      "929:\ttotal: 28m 42s\tremaining: 2m 9s\n",
      "930:\ttotal: 28m 44s\tremaining: 2m 7s\n",
      "931:\ttotal: 28m 46s\tremaining: 2m 5s\n",
      "932:\ttotal: 28m 48s\tremaining: 2m 4s\n",
      "933:\ttotal: 28m 50s\tremaining: 2m 2s\n",
      "934:\ttotal: 28m 52s\tremaining: 2m\n",
      "935:\ttotal: 28m 53s\tremaining: 1m 58s\n",
      "936:\ttotal: 28m 55s\tremaining: 1m 56s\n",
      "937:\ttotal: 28m 57s\tremaining: 1m 54s\n",
      "938:\ttotal: 28m 59s\tremaining: 1m 52s\n",
      "939:\ttotal: 29m 1s\tremaining: 1m 51s\n",
      "940:\ttotal: 29m 3s\tremaining: 1m 49s\n",
      "941:\ttotal: 29m 5s\tremaining: 1m 47s\n",
      "942:\ttotal: 29m 7s\tremaining: 1m 45s\n",
      "943:\ttotal: 29m 9s\tremaining: 1m 43s\n",
      "944:\ttotal: 29m 10s\tremaining: 1m 41s\n",
      "945:\ttotal: 29m 12s\tremaining: 1m 40s\n",
      "946:\ttotal: 29m 14s\tremaining: 1m 38s\n",
      "947:\ttotal: 29m 16s\tremaining: 1m 36s\n",
      "948:\ttotal: 29m 18s\tremaining: 1m 34s\n",
      "949:\ttotal: 29m 20s\tremaining: 1m 32s\n",
      "950:\ttotal: 29m 22s\tremaining: 1m 30s\n",
      "951:\ttotal: 29m 24s\tremaining: 1m 28s\n",
      "952:\ttotal: 29m 26s\tremaining: 1m 27s\n",
      "953:\ttotal: 29m 27s\tremaining: 1m 25s\n",
      "954:\ttotal: 29m 29s\tremaining: 1m 23s\n",
      "955:\ttotal: 29m 31s\tremaining: 1m 21s\n",
      "956:\ttotal: 29m 33s\tremaining: 1m 19s\n",
      "957:\ttotal: 29m 35s\tremaining: 1m 17s\n",
      "958:\ttotal: 29m 37s\tremaining: 1m 15s\n",
      "959:\ttotal: 29m 39s\tremaining: 1m 14s\n",
      "960:\ttotal: 29m 41s\tremaining: 1m 12s\n",
      "961:\ttotal: 29m 43s\tremaining: 1m 10s\n",
      "962:\ttotal: 29m 45s\tremaining: 1m 8s\n",
      "963:\ttotal: 29m 47s\tremaining: 1m 6s\n",
      "964:\ttotal: 29m 48s\tremaining: 1m 4s\n",
      "965:\ttotal: 29m 50s\tremaining: 1m 3s\n",
      "966:\ttotal: 29m 52s\tremaining: 1m 1s\n",
      "967:\ttotal: 29m 54s\tremaining: 59.3s\n",
      "968:\ttotal: 29m 56s\tremaining: 57.5s\n",
      "969:\ttotal: 29m 58s\tremaining: 55.6s\n",
      "970:\ttotal: 30m\tremaining: 53.8s\n",
      "971:\ttotal: 30m 2s\tremaining: 51.9s\n",
      "972:\ttotal: 30m 4s\tremaining: 50.1s\n",
      "973:\ttotal: 30m 6s\tremaining: 48.2s\n",
      "974:\ttotal: 30m 7s\tremaining: 46.4s\n",
      "975:\ttotal: 30m 9s\tremaining: 44.5s\n",
      "976:\ttotal: 30m 11s\tremaining: 42.7s\n",
      "977:\ttotal: 30m 13s\tremaining: 40.8s\n",
      "978:\ttotal: 30m 15s\tremaining: 38.9s\n",
      "979:\ttotal: 30m 17s\tremaining: 37.1s\n",
      "980:\ttotal: 30m 19s\tremaining: 35.2s\n",
      "981:\ttotal: 30m 21s\tremaining: 33.4s\n",
      "982:\ttotal: 30m 23s\tremaining: 31.5s\n",
      "983:\ttotal: 30m 25s\tremaining: 29.7s\n",
      "984:\ttotal: 30m 27s\tremaining: 27.8s\n",
      "985:\ttotal: 30m 29s\tremaining: 26s\n",
      "986:\ttotal: 30m 30s\tremaining: 24.1s\n",
      "987:\ttotal: 30m 32s\tremaining: 22.3s\n",
      "988:\ttotal: 30m 34s\tremaining: 20.4s\n",
      "989:\ttotal: 30m 36s\tremaining: 18.6s\n",
      "990:\ttotal: 30m 38s\tremaining: 16.7s\n",
      "991:\ttotal: 30m 40s\tremaining: 14.8s\n",
      "992:\ttotal: 30m 42s\tremaining: 13s\n",
      "993:\ttotal: 30m 44s\tremaining: 11.1s\n",
      "994:\ttotal: 30m 46s\tremaining: 9.28s\n",
      "995:\ttotal: 30m 48s\tremaining: 7.42s\n",
      "996:\ttotal: 30m 49s\tremaining: 5.57s\n",
      "997:\ttotal: 30m 51s\tremaining: 3.71s\n",
      "998:\ttotal: 30m 53s\tremaining: 1.85s\n",
      "999:\ttotal: 30m 55s\tremaining: 0us\n",
      "PRAUC: 0.628791379885663\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Путь к данным  \n",
    "attributes_path = 'F:/competition/attributes.parquet'  \n",
    "resnet_path = 'F:/competition/resnet.parquet'  \n",
    "text_and_bert_path = 'F:/competition/text_and_bert.parquet'  \n",
    "train_path = 'F:/competition/train.parquet'  \n",
    "test_path = 'F:/competition/test.parquet'  \n",
    "\n",
    "# Функция для загрузки данных  \n",
    "def load_data(sample_size=0.1):  \n",
    "    # Загрузка данных из файлов Parquet  \n",
    "    attributes = pd.read_parquet(attributes_path)  \n",
    "    resnet = pd.read_parquet(resnet_path)  \n",
    "    text_and_bert = pd.read_parquet(text_and_bert_path)  \n",
    "    train = pd.read_parquet(train_path)  \n",
    "    test = pd.read_parquet(test_path)  \n",
    "\n",
    "    # Если sample_size меньше 1, то выборка данных  \n",
    "    if sample_size < 1:  \n",
    "        attributes = attributes.sample(frac=sample_size, random_state=42)  \n",
    "        resnet = resnet.sample(frac=sample_size, random_state=42)  \n",
    "        text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)  \n",
    "        train = train.sample(frac=sample_size, random_state=42)  \n",
    "        test = test.sample(frac=sample_size, random_state=42)  \n",
    "\n",
    "    # Возвращаем 5 DataFrame'ов\n",
    "    return attributes, resnet, text_and_bert, train, test  \n",
    "\n",
    "# Обработка текстовых данных\n",
    "def process_text_and_bert(df):\n",
    "    # Объединение столбцов name и description в единый текст\n",
    "    df['combined_text'] = df['name'] + ' ' + df['description']\n",
    "    return df\n",
    "\n",
    "# Объединение данных\n",
    "def merge_data(train, resnet, text_and_bert):\n",
    "    # Объединение с эмбеддингами ResNet\n",
    "    train_data = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_1'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    train_data = train_data.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_2'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Объединение с текстовыми данными и эмбеддингами BERT\n",
    "    train_data = train_data.merge(text_and_bert[['variantid', 'combined_text', 'name_bert_64']], left_on='variantid1', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'combined_text': 'text_1', 'name_bert_64': 'text_embedding_1'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    train_data = train_data.merge(text_and_bert[['variantid', 'combined_text', 'name_bert_64']], left_on='variantid2', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'combined_text': 'text_2', 'name_bert_64': 'text_embedding_2'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Удаление строк, в которых есть пропущенные значения\n",
    "    train_data = train_data.dropna()\n",
    "\n",
    "    return train_data\n",
    "\n",
    "# Объединение эмбеддингов\n",
    "def combine_embeddings(row):\n",
    "    # Объединение эмбеддингов изображений и текста\n",
    "    pic_embeddings = np.concatenate([row['pic_embeddings_1'][0], row['pic_embeddings_2'][0]])\n",
    "    text_embeddings = np.concatenate([row['text_embedding_1'], row['text_embedding_2']])\n",
    "    return np.concatenate([pic_embeddings, text_embeddings])\n",
    "\n",
    "# Подготовка данных для обучения модели\n",
    "def prepare_data(train_data):\n",
    "    # Объединение эмбеддингов изображений и текстов\n",
    "    train_data['combined_embeddings'] = train_data.apply(combine_embeddings, axis=1)\n",
    "\n",
    "    # Преобразование объединенных эмбеддингов в массив numpy\n",
    "    X = np.vstack(train_data['combined_embeddings'].values)\n",
    "    y = train_data['target']\n",
    "\n",
    "    # Разделение данных на обучающую и валидационную выборки\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "# Обучение модели\n",
    "def train_model(X_train, y_train):\n",
    "    model = CatBoostClassifier(iterations=1000, depth=10, learning_rate=0.1, loss_function='Logloss', eval_metric='AUC', verbose=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Сохранение обученной модели\n",
    "    joblib.dump(model, 'catboost_model1.pkl')\n",
    "    return model\n",
    "\n",
    "# Оценка модели\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "    prauc = auc(recall, precision)\n",
    "    print(f'PRAUC: {prauc}')\n",
    "\n",
    "    # Сохранение модели\n",
    "    joblib.dump(model, 'catboost_model1.pkl')\n",
    "\n",
    "    # Сохранение векторизатора (если используется), в данном случае он не используется\n",
    "    # joblib.dump(tfidf_vectorizer, 'vectorizer1.pk3')\n",
    "\n",
    "# Главная функция для запуска всего процесса\n",
    "def main():\n",
    "    attributes, resnet, text_and_bert, train, test = load_data()\n",
    "\n",
    "    # Обработка текстовых данных\n",
    "    text_and_bert = process_text_and_bert(text_and_bert)\n",
    "\n",
    "    # Объединение данных\n",
    "    train_data = merge_data(train, resnet, text_and_bert)\n",
    "\n",
    "    # Подготовка данных для обучения модели\n",
    "    X_train, X_val, y_train, y_val = prepare_data(train_data)\n",
    "\n",
    "    # Обучение модели\n",
    "    model = train_model(X_train, y_train)\n",
    "    \n",
    "    # Оценка модели\n",
    "    evaluate_model(model, X_val, y_val)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ced27b-0294-463c-8902-f182ce31f9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891b20a-3a82-4744-a080-08957680cbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af57ab6-cfd2-4420-b9ea-23e0f71836b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ecc8a-fe02-4b79-8b2b-f7b5734f3960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a459af-99d3-4f7a-8841-c8347e14a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка при загрузке F:/competition/resnet.parquet: malloc of size 17179869184 failed\n",
      "Ошибка при загрузке F:/competition/text_and_bert.parquet: Отсутствуют столбцы в файле F:/competition/text_and_bert.parquet: combined_text\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.metrics import precision_recall_curve, auc  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "import joblib  \n",
    "import json  \n",
    "\n",
    "# # Путь к данным  \n",
    "# attributes_path = 'F:/competition/attributes.parquet'  \n",
    "# resnet_path = 'F:/competition/resnet.parquet'  \n",
    "# text_and_bert_path = 'F:/competition/text_and_bert.parquet'  \n",
    "# train_path = 'F:/competition/train.parquet'  \n",
    "# test_path = 'F:/competition/test.parquet'  \n",
    "\n",
    "# # Функция для загрузки данных  \n",
    "# def load_data(sample_size=0.1):  \n",
    "#     # Загрузка данных из файлов Parquet  \n",
    "#     attributes = pd.read_parquet(attributes_path)  \n",
    "#     resnet = pd.read_parquet(resnet_path)  \n",
    "#     text_and_bert = pd.read_parquet(text_and_bert_path)  \n",
    "#     train = pd.read_parquet(train_path)  \n",
    "#     test = pd.read_parquet(test_path)  \n",
    "\n",
    "#     # Если sample_size меньше 1, то выборка данных  \n",
    "#     if sample_size < 1:  \n",
    "#         attributes = attributes.sample(frac=sample_size, random_state=42)  \n",
    "#         resnet = resnet.sample(frac=sample_size, random_state=42)  \n",
    "#         text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)  \n",
    "#         train = train.sample(frac=sample_size, random_state=42)  \n",
    "#         test = test.sample(frac=sample_size, random_state=42)  \n",
    "\n",
    "#     return attributes, resnet, text_and_bert, train, test  \n",
    "\n",
    "# # Загрузка данных  \n",
    "# attributes, resnet, text_and_bert, train, test = load_data()  \n",
    "\n",
    "# Проверка наличия необходимых столбцов перед объединением  \n",
    "def load_parquet_file(path, expected_columns):  \n",
    "    \"\"\"Функция для загрузки данных из файла Parquet с проверкой столбцов.\"\"\"  \n",
    "    try:  \n",
    "        df = pd.read_parquet(path)  \n",
    "        missing_cols = [col for col in expected_columns if col not in df.columns]  \n",
    "        if missing_cols:  \n",
    "            raise ValueError(f\"Отсутствуют столбцы в файле {path}: {', '.join(missing_cols)}\")  \n",
    "        return df  \n",
    "    except Exception as e:  \n",
    "        print(f\"Ошибка при загрузке {path}: {e}\")  \n",
    "        return pd.DataFrame()  # Возвращаем пустой DataFrame в случае ошибки  \n",
    "\n",
    "# Загрузка данных с проверкой отсутствующих колонок  \n",
    "attributes = load_parquet_file(attributes_path, ['variantid', 'categories', 'characteristic_attributes_mapping'])  \n",
    "resnet = load_parquet_file(resnet_path, ['variantid', 'main_pic_embeddings_resnet_v1'])  \n",
    "text_and_bert = load_parquet_file(text_and_bert_path, ['variantid', 'combined_text'])  \n",
    "train = load_parquet_file(train_path, ['variantid1', 'variantid2', 'target'])  \n",
    "\n",
    "# Обработка атрибутов  \n",
    "if not attributes.empty:  \n",
    "    # Проверка наличия столбца 'categories'  \n",
    "    if 'categories' in attributes.columns:  \n",
    "        attributes['categories'] = attributes['categories'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)  \n",
    "    else:  \n",
    "        print(\"Столбец 'categories' отсутствует в данных атрибутов.\")  \n",
    "    \n",
    "    if 'characteristic_attributes_mapping' in attributes.columns:  \n",
    "        attributes['characteristic_attributes_mapping'] = attributes['characteristic_attributes_mapping'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)  \n",
    "    else:  \n",
    "        print(\"Столбец 'characteristic_attributes_mapping' отсутствует в данных атрибутов.\")  \n",
    "    \n",
    "    def extract_text_from_row(row):  \n",
    "        category_text = ' '.join(  \n",
    "            [' '.join(map(str, v)) if isinstance(v, list) else str(v) for v in list(row['categories'].values())]  \n",
    "        )  \n",
    "        attributes_text = ' '.join(  \n",
    "            [' '.join(map(str, v)) if isinstance(v, list) else str(v) for v in list(row['characteristic_attributes_mapping'].values())]  \n",
    "        )  \n",
    "        return f\"{category_text} {attributes_text}\"  \n",
    "\n",
    "\n",
    "\n",
    "        # Создание комбинированного текста  \n",
    "    attributes['combined_text'] = attributes.apply(extract_text_from_row, axis=1)  \n",
    "\n",
    "# Объединение данных для обучения  \n",
    "if not train.empty and not resnet.empty and not attributes.empty:  \n",
    "    train_data = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left')  \n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_1'})  \n",
    "    train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "    train_data = train_data.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', how='left')  \n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_2'})  \n",
    "    train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "    train_data = train_data.merge(attributes[['variantid', 'combined_text']], left_on='variantid1', right_on='variantid', how='left')  \n",
    "    train_data = train_data.rename(columns={'combined_text': 'text_1'})  \n",
    "    train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "    train_data = train_data.merge(attributes[['variantid', 'combined_text']], left_on='variantid2', right_on='variantid', how='left')  \n",
    "    train_data = train_data.rename(columns={'combined_text': 'text_2'})  \n",
    "    train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "    train_data = train_data.dropna()  # Удаление строк с пропущенными значениями  \n",
    "else:  \n",
    "    train_data = pd.DataFrame()  \n",
    "\n",
    "# Шаг 4: Подготовка данных  \n",
    "if not train_data.empty:  \n",
    "    # Создание объекта TfidfVectorizer  \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=2000)  \n",
    "\n",
    "    # Преобразование текстовых данных в эмбеддинги  \n",
    "    text_data = train_data['text_1'].fillna('') + ' ' + train_data['text_2'].fillna('')  \n",
    "    text_embeddings = tfidf_vectorizer.fit_transform(text_data).toarray()  \n",
    "\n",
    "    # Функция для объединения эмбеддингов изображений и текста  \n",
    "    def combine_embeddings(row):  \n",
    "        pic_embeddings = np.concatenate([row['pic_embeddings_1'][0], row['pic_embeddings_2'][0]])  \n",
    "        text_embeddings_row = text_embeddings[row.name]  \n",
    "        return np.concatenate([pic_embeddings, text_embeddings_row])  \n",
    "\n",
    "    # Применение функции к данным  \n",
    "    train_data['combined_embeddings'] = train_data.apply(combine_embeddings, axis=1)  \n",
    "\n",
    "    # Создание матрицы признаков и вектора целей  \n",
    "    X = np.vstack(train_data['combined_embeddings'].values)  \n",
    "    y = train_data['target']  \n",
    "\n",
    "    # Разделение данных на обучающую и валидационную выборки  \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.12, random_state=42)  \n",
    "else:  \n",
    "    X_train, X_val, y_train, y_val = (np.array([]),) * 4  \n",
    "\n",
    "# Шаг 5: Обучение модели  \n",
    "if X_train.size > 0 and y_train.size > 0:  \n",
    "    # Создание и обучение модели логистической регрессии  \n",
    "    model = LogisticRegression(max_iter=2000)  \n",
    "    model.fit(X_train, y_train)  \n",
    "\n",
    "    # Сохранение модели  \n",
    "    joblib.dump(model, 'baseline1.pk3')  \n",
    "\n",
    "# Шаг 6: Оценка модели  \n",
    "if X_val.size > 0 and y_val.size > 0:  \n",
    "    # Прогнозирование вероятностей  \n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]  \n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)  \n",
    "\n",
    "    # Вычисление метрик точности и полноты  \n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)  \n",
    "    prauc = auc(recall, precision)  \n",
    "    print(f'PRAUC: {prauc}')  \n",
    "\n",
    "    # Сохранение вектораизатора  \n",
    "    joblib.dump(tfidf_vectorizer, 'vectorizer1.pk3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925777b0-9a9b-4835-bfea-75d1e92ca9a1",
   "metadata": {},
   "source": [
    "PRAUC: 0.7724866290617343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4095499-3a01-43a2-bde4-9391376f7413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка при загрузке F:/competition/text_and_bert.parquet: Отсутствуют столбцы в файле F:/competition/text_and_bert.parquet: combined_text\n",
      "PRAUC: 0.7713306247507894\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.metrics import precision_recall_curve, auc  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "import joblib  \n",
    "import json  \n",
    "\n",
    "# Путь к данным  \n",
    "attributes_path = 'F:/competition/attributes.parquet'  \n",
    "resnet_path = 'F:/competition/resnet.parquet'  \n",
    "text_and_bert_path = 'F:/competition/text_and_bert.parquet'  \n",
    "train_path = 'F:/competition/train.parquet'  \n",
    "test_path = 'F:/competition/test.parquet'  \n",
    "\n",
    "# Функция для загрузки данных  \n",
    "def load_data(sample_size=0.1):  \n",
    "    # Загрузка данных из файлов Parquet  \n",
    "    attributes = pd.read_parquet(attributes_path)  \n",
    "    resnet = pd.read_parquet(resnet_path)  \n",
    "    text_and_bert = pd.read_parquet(text_and_bert_path)  \n",
    "    train = pd.read_parquet(train_path)  \n",
    "    test = pd.read_parquet(test_path)  \n",
    "\n",
    "    # Если sample_size меньше 1, то выборка данных  \n",
    "    if sample_size < 1:  \n",
    "        attributes = attributes.sample(frac=sample_size, random_state=42)  \n",
    "        resnet = resnet.sample(frac=sample_size, random_state=42)  \n",
    "        text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)  \n",
    "        train = train.sample(frac=sample_size, random_state=42)  \n",
    "        test = test.sample(frac=sample_size, random_state=42)  \n",
    "\n",
    "    return attributes, resnet, text_and_bert, train, test  \n",
    "\n",
    "# Загрузка данных  \n",
    "attributes, resnet, text_and_bert, train, test = load_data()  \n",
    "\n",
    "# Проверка наличия необходимых столбцов перед объединением  \n",
    "def load_parquet_file(path, expected_columns):  \n",
    "    \"\"\"Функция для загрузки данных из файла Parquet с проверкой столбцов.\"\"\"  \n",
    "    try:  \n",
    "        df = pd.read_parquet(path)  \n",
    "        missing_cols = [col for col in expected_columns if col not in df.columns]  \n",
    "        if missing_cols:  \n",
    "            raise ValueError(f\"Отсутствуют столбцы в файле {path}: {', '.join(missing_cols)}\")  \n",
    "        return df  \n",
    "    except Exception as e:  \n",
    "        print(f\"Ошибка при загрузке {path}: {e}\")  \n",
    "        return pd.DataFrame()  # Возвращаем пустой DataFrame в случае ошибки  \n",
    "\n",
    "# Загрузка данных с проверкой отсутствующих колонок  \n",
    "attributes = load_parquet_file('F:/competition/attributes.parquet', ['variantid', 'categories', 'characteristic_attributes_mapping'])  \n",
    "resnet = load_parquet_file('F:/competition/resnet.parquet', ['variantid', 'main_pic_embeddings_resnet_v1'])  \n",
    "text_and_bert = load_parquet_file('F:/competition/text_and_bert.parquet', ['variantid', 'combined_text'])  \n",
    "train = load_parquet_file('F:/competition/train.parquet', ['variantid1', 'variantid2', 'target'])  \n",
    "\n",
    "# Обработка атрибутов  \n",
    "if not attributes.empty:  \n",
    "    # Проверка наличия столбца 'categories'  \n",
    "    if 'categories' in attributes.columns:  \n",
    "        attributes['categories'] = attributes['categories'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)  \n",
    "    else:  \n",
    "        print(\"Столбец 'categories' отсутствует в данных атрибутов.\")  \n",
    "    \n",
    "    if 'characteristic_attributes_mapping' in attributes.columns:  \n",
    "        attributes['characteristic_attributes_mapping'] = attributes['characteristic_attributes_mapping'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)  \n",
    "    else:  \n",
    "        print(\"Столбец 'characteristic_attributes_mapping' отсутствует в данных атрибутов.\")  \n",
    "    \n",
    "    def extract_text_from_row(row):  \n",
    "        category_text = ' '.join(  \n",
    "            [' '.join(map(str, v)) if isinstance(v, list) else str(v) for v in list(row['categories'].values())]  \n",
    "        )  \n",
    "        attributes_text = ' '.join(  \n",
    "            [' '.join(map(str, v)) if isinstance(v, list) else str(v) for v in list(row['characteristic_attributes_mapping'].values())]  \n",
    "        )  \n",
    "        return f\"{category_text} {attributes_text}\"  \n",
    "\n",
    "    # Создание комбинированного текста  \n",
    "    attributes['combined_text'] = attributes.apply(extract_text_from_row, axis=1)  \n",
    "\n",
    "# Объединение данных для обучения  \n",
    "if not train.empty and not resnet.empty and not attributes.empty:  \n",
    "    try:\n",
    "        train_data = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left')  \n",
    "        train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_1'})  \n",
    "        train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "        train_data = train_data.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', how='left')  \n",
    "        train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_2'})  \n",
    "        train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "        train_data = train_data.merge(attributes[['variantid', 'combined_text']], left_on='variantid1', right_on='variantid', how='left')  \n",
    "        train_data = train_data.rename(columns={'combined_text': 'text_1'})  \n",
    "        train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "        train_data = train_data.merge(attributes[['variantid', 'combined_text']], left_on='variantid2', right_on='variantid', how='left')  \n",
    "        train_data = train_data.rename(columns={'combined_text': 'text_2'})  \n",
    "        train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "        train_data = train_data.dropna()  # Удаление строк с пропущенными значениями  \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при объединении данных: {e}\")\n",
    "        train_data = pd.DataFrame()  \n",
    "\n",
    "# Шаг 4: Подготовка данных  \n",
    "if not train_data.empty:  \n",
    "    # Создание объекта TfidfVectorizer  \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=2000)  \n",
    "\n",
    "    # Преобразование текстовых данных в эмбеддинги  \n",
    "    text_data = train_data['text_1'].fillna('') + ' ' + train_data['text_2'].fillna('')  \n",
    "    text_embeddings = tfidf_vectorizer.fit_transform(text_data).toarray()  \n",
    "\n",
    "    # Функция для объединения эмбеддингов изображений и текста  \n",
    "    def combine_embeddings(row):  \n",
    "        pic_embeddings = np.concatenate([row['pic_embeddings_1'][0], row['pic_embeddings_2'][0]])  \n",
    "        text_embeddings_row = text_embeddings[row.name]  \n",
    "        return np.concatenate([pic_embeddings, text_embeddings_row])  \n",
    "\n",
    "    # Применение функции к данным  \n",
    "    train_data['combined_embeddings'] = train_data.apply(combine_embeddings, axis=1)  \n",
    "\n",
    "    # Создание матрицы признаков и вектора целей  \n",
    "    X = np.vstack(train_data['combined_embeddings'].values)  \n",
    "    y = train_data['target']  \n",
    "\n",
    "    # Разделение данных на обучающую и валидационную выборки  \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.12, random_state=42)  \n",
    "else:  \n",
    "    X_train, X_val, y_train, y_val = (np.array([]),) * 4  \n",
    "\n",
    "# Шаг 5: Обучение модели  \n",
    "if X_train.size > 0 and y_train.size > 0:  \n",
    "    # Создание и обучение модели логистической регрессии  \n",
    "    model = LogisticRegression(max_iter=2000)  \n",
    "    model.fit(X_train, y_train)  \n",
    "\n",
    "    # Сохранение модели  \n",
    "    joblib.dump(model, 'baseline1.pk3')  \n",
    "\n",
    "# Шаг 6: Оценка модели  \n",
    "if X_val.size > 0 and y_val.size > 0:  \n",
    "    # Прогнозирование вероятностей  \n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]  \n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)  \n",
    "\n",
    "    # Вычисление метрик точности и полноты  \n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)  \n",
    "    prauc = auc(recall, precision)  \n",
    "    print(f'PRAUC: {prauc}')  \n",
    "\n",
    "    # Сохранение вектораизатора  \n",
    "    joblib.dump(tfidf_vectorizer, 'vectorizer1.pk3')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88423b23-f0b0-4ea6-864d-96c3525de48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a2b12-ca1c-4493-8d32-c580fc151bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190e556-6b8c-4a9c-9bea-3ef147aaf293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25179b10-259f-46a1-8c33-27541e8115a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c94b48-444f-40d7-ae0e-652bf9fd7ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96409e1-5871-48bd-a212-d5692645cf65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1284d9cf-a589-4327-93ea-fbe0df49c462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e8e4d29-da68-4357-8e0c-3419534e3bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отсутствуют столбцы в файле F:/competition/text_and_bert.parquet: combined_text\n",
      "PRAUC: 0.7724866290617343\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.metrics import precision_recall_curve, auc  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "import joblib  \n",
    "import json  \n",
    "\n",
    "# Путь к данным  \n",
    "attributes_path = 'F:/competition/attributes.parquet'  \n",
    "resnet_path = 'F:/competition/resnet.parquet'  \n",
    "text_and_bert_path = 'F:/competition/text_and_bert.parquet'  \n",
    "train_path = 'F:/competition/train.parquet'  \n",
    "test_path = 'F:/competition/test.parquet'  \n",
    "\n",
    "# Функция для загрузки данных  \n",
    "def load_data(sample_size=0.1):  \n",
    "    # Загрузка данных из файлов Parquet  \n",
    "    attributes = pd.read_parquet(attributes_path)  \n",
    "    resnet = pd.read_parquet(resnet_path)  \n",
    "    text_and_bert = pd.read_parquet(text_and_bert_path)  \n",
    "    train = pd.read_parquet(train_path)  \n",
    "    test = pd.read_parquet(test_path)  \n",
    "\n",
    "    # Если sample_size меньше 1, то выборка данных  \n",
    "    if sample_size < 1:  \n",
    "        attributes = attributes.sample(frac=sample_size, random_state=42)  \n",
    "        resnet = resnet.sample(frac=sample_size, random_state=42)  \n",
    "        text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)  \n",
    "        train = train.sample(frac=sample_size, random_state=42)  \n",
    "        test = test.sample(frac=sample_size, random_state=42)  \n",
    "\n",
    "    return attributes, resnet, text_and_bert, train, test  \n",
    "\n",
    "# Загрузка данных  \n",
    "attributes, resnet, text_and_bert, train, test = load_data()  \n",
    "\n",
    "\n",
    "# Загрузка данных с проверкой отсутствующих колонок  \n",
    "def load_parquet_file(path, expected_columns):  \n",
    "    \"\"\"Функция для загрузки данных из файла Parquet с проверкой столбцов.\"\"\"  \n",
    "    try:  \n",
    "        df = pd.read_parquet(path)  \n",
    "        missing_cols = [col for col in expected_columns if col not in df.columns]  \n",
    "        if missing_cols:  \n",
    "            print(f\"Отсутствуют столбцы в файле {path}: {', '.join(missing_cols)}\")  \n",
    "            # Создаем отсутствующие столбцы с пустыми значениями  \n",
    "            for col in missing_cols:  \n",
    "                df[col] = np.nan  \n",
    "        return df  \n",
    "    except Exception as e:  \n",
    "        print(f\"Ошибка при загрузке {path}: {e}\")  \n",
    "        return pd.DataFrame()  # Возвращаем пустой DataFrame в случае ошибки  \n",
    "\n",
    "# Загрузка данных с проверкой отсутствующих колонок  \n",
    "attributes = load_parquet_file(attributes_path, ['variantid', 'categories', 'characteristic_attributes_mapping'])  \n",
    "resnet = load_parquet_file(resnet_path, ['variantid', 'main_pic_embeddings_resnet_v1'])  \n",
    "text_and_bert = load_parquet_file(text_and_bert_path, ['variantid', 'combined_text'])  \n",
    "train = load_parquet_file(train_path, ['variantid1', 'variantid2', 'target'])  \n",
    "\n",
    "# Обработка атрибутов  \n",
    "if not attributes.empty:  \n",
    "    # Проверка наличия столбца 'categories'  \n",
    "    if 'categories' in attributes.columns:  \n",
    "        attributes['categories'] = attributes['categories'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)  \n",
    "    else:  \n",
    "        print(\"Столбец 'categories' отсутствует в данных атрибутов.\")  \n",
    "    \n",
    "    if 'characteristic_attributes_mapping' in attributes.columns:  \n",
    "        attributes['characteristic_attributes_mapping'] = attributes['characteristic_attributes_mapping'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)  \n",
    "    else:  \n",
    "        print(\"Столбец 'characteristic_attributes_mapping' отсутствует в данных атрибутов.\")  \n",
    "    \n",
    "    def extract_text_from_row(row):  \n",
    "        category_text = ' '.join(  \n",
    "            [' '.join(map(str, v)) if isinstance(v, list) else str(v) for v in list(row['categories'].values())]  \n",
    "        )  \n",
    "        attributes_text = ' '.join(  \n",
    "            [' '.join(map(str, v)) if isinstance(v, list) else str(v) for v in list(row['characteristic_attributes_mapping'].values())]  \n",
    "        )  \n",
    "        return f\"{category_text} {attributes_text}\"  \n",
    "\n",
    "    # Создание комбинированного текста  \n",
    "    attributes['combined_text'] = attributes.apply(extract_text_from_row, axis=1)  \n",
    "\n",
    "# Объединение данных для обучения  \n",
    "if not train.empty and not resnet.empty and not attributes.empty:  \n",
    "    train_data = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left')  \n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_1'})  \n",
    "    train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "    train_data = train_data.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', how='left')  \n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_2'})  \n",
    "    train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "    train_data = train_data.merge(attributes[['variantid', 'combined_text']], left_on='variantid1', right_on='variantid', how='left')  \n",
    "    train_data = train_data.rename(columns={'combined_text': 'text_1'})  \n",
    "    train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "    train_data = train_data.merge(attributes[['variantid', 'combined_text']], left_on='variantid2', right_on='variantid', how='left')  \n",
    "    train_data = train_data.rename(columns={'combined_text': 'text_2'})  \n",
    "    train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "    train_data = train_data.dropna()  # Удаление строк с пропущенными значениями  \n",
    "else:  \n",
    "    train_data = pd.DataFrame() \n",
    "  \n",
    "\n",
    "# Шаг 4: Подготовка данных  \n",
    "if not train_data.empty:  \n",
    "    # Создание объекта TfidfVectorizer  \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=2000)  \n",
    "\n",
    "    # Преобразование текстовых данных в эмбеддинги  \n",
    "    text_data = train_data['text_1'].fillna('') + ' ' + train_data['text_2'].fillna('')  \n",
    "    text_embeddings = tfidf_vectorizer.fit_transform(text_data).toarray()  \n",
    "\n",
    "    # Функция для объединения эмбеддингов изображений и текста  \n",
    "    def combine_embeddings(row):  \n",
    "        pic_embeddings = np.concatenate([row['pic_embeddings_1'][0], row['pic_embeddings_2'][0]])  \n",
    "        text_embeddings_row = text_embeddings[row.name]  \n",
    "        return np.concatenate([pic_embeddings, text_embeddings_row])  \n",
    "\n",
    "    # Применение функции к данным  \n",
    "    train_data['combined_embeddings'] = train_data.apply(combine_embeddings, axis=1)  \n",
    "\n",
    "    # Создание матрицы признаков и вектора целей  \n",
    "    X = np.vstack(train_data['combined_embeddings'].values)  \n",
    "    y = train_data['target']  \n",
    "\n",
    "    # Разделение данных на обучающую и валидационную выборки  \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=23)  \n",
    "else:  \n",
    "    X_train, X_val, y_train, y_val = (np.array([]),) * 4  \n",
    "\n",
    "                                  \n",
    "\n",
    "# Шаг 5: Обучение модели  \n",
    "if X_train.size > 0 and y_train.size > 0:  \n",
    "    # Создание и обучение модели логистической регрессии  \n",
    "    model = LogisticRegression(max_iter=2000)  \n",
    "    model.fit(X_train, y_train)  \n",
    "\n",
    "    # Сохранение модели  \n",
    "    joblib.dump(model, 'baseline0.pkl')  \n",
    "\n",
    "# Шаг 6: Оценка модели  \n",
    "if X_val.size > 0 and y_val.size > 0:  \n",
    "    # Прогнозирование вероятностей  \n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]  \n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)  \n",
    "\n",
    "    # Вычисление метрик точности и полноты  \n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)  \n",
    "    prauc = auc(recall, precision)  \n",
    "    print(f'PRAUC: {prauc}')  \n",
    "\n",
    "    # Сохранение вектораизатора  \n",
    "    joblib.dump(tfidf_vectorizer, 'vectorizer0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ae451-5ef7-462e-b9a8-9cf3c90e9186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22208051-8f5a-4040-9368-32f4ced611eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d96790-7dc7-49da-91ef-369c02eaa12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2509101-18ae-459e-95f8-c7c1d6954cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка при загрузке F:/competition/text_and_bert.parquet: Отсутствуют столбцы в файле F:/competition/text_and_bert.parquet: combined_text\n",
      "PRAUC: 0.7724866290617343\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.metrics import precision_recall_curve, auc  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "import joblib  \n",
    "import json  \n",
    "\n",
    "# Путь к данным  \n",
    "attributes_path = 'F:/competition/attributes.parquet'  \n",
    "resnet_path = 'F:/competition/resnet.parquet'  \n",
    "text_and_bert_path = 'F:/competition/text_and_bert.parquet'  \n",
    "train_path = 'F:/competition/train.parquet'  \n",
    "test_path = 'F:/competition/test.parquet'  \n",
    "\n",
    "# Функция для загрузки данных  \n",
    "def load_data(sample_size=0.1):  \n",
    "    # Загрузка данных из файлов Parquet  \n",
    "    attributes = pd.read_parquet(attributes_path)  \n",
    "    resnet = pd.read_parquet(resnet_path)  \n",
    "    text_and_bert = pd.read_parquet(text_and_bert_path)  \n",
    "    train = pd.read_parquet(train_path)  \n",
    "    test = pd.read_parquet(test_path)  \n",
    "\n",
    "    # Если sample_size меньше 1, то выборка данных  \n",
    "    if sample_size < 1:  \n",
    "        attributes = attributes.sample(frac=sample_size, random_state=42)  \n",
    "        resnet = resnet.sample(frac=sample_size, random_state=42)  \n",
    "        text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)  \n",
    "        train = train.sample(frac=sample_size, random_state=42)  \n",
    "        test = test.sample(frac=sample_size, random_state=42)  \n",
    "\n",
    "    return attributes, resnet, text_and_bert, train, test  \n",
    "\n",
    "# Загрузка данных  \n",
    "attributes, resnet, text_and_bert, train, test = load_data()  \n",
    "\n",
    "# Проверка наличия необходимых столбцов перед объединением  \n",
    "def load_parquet_file(path, expected_columns):  \n",
    "    \"\"\"Функция для загрузки данных из файла Parquet с проверкой столбцов.\"\"\"  \n",
    "    try:  \n",
    "        df = pd.read_parquet(path)  \n",
    "        missing_cols = [col for col in expected_columns if col not in df.columns]  \n",
    "        if missing_cols:  \n",
    "            raise ValueError(f\"Отсутствуют столбцы в файле {path}: {', '.join(missing_cols)}\")  \n",
    "        return df  \n",
    "    except Exception as e:  \n",
    "        print(f\"Ошибка при загрузке {path}: {e}\")  \n",
    "        return pd.DataFrame()  # Возвращаем пустой DataFrame в случае ошибки  \n",
    "\n",
    "# Загрузка данных с проверкой отсутствующих колонок  \n",
    "attributes = load_parquet_file(attributes_path, ['variantid', 'categories', 'characteristic_attributes_mapping'])  \n",
    "resnet = load_parquet_file(resnet_path, ['variantid', 'main_pic_embeddings_resnet_v1'])  \n",
    "text_and_bert = load_parquet_file(text_and_bert_path, ['variantid', 'combined_text'])  \n",
    "train = load_parquet_file(train_path, ['variantid1', 'variantid2', 'target'])  \n",
    "\n",
    "# Обработка атрибутов  \n",
    "if not attributes.empty:  \n",
    "    attributes['categories'] = attributes['categories'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)  \n",
    "    attributes['characteristic_attributes_mapping'] = attributes['characteristic_attributes_mapping'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)  \n",
    "\n",
    "    def extract_text_from_row(row):  \n",
    "        category_text = ' '.join(  \n",
    "            [' '.join(map(str, v)) if isinstance(v, list) else str(v) for v in list(row['categories'].values())]  \n",
    "        )  \n",
    "        attributes_text = ' '.join(  \n",
    "            [' '.join(map(str, v)) if isinstance(v, list) else str(v) for v in list(row['characteristic_attributes_mapping'].values())]  \n",
    "        )  \n",
    "        return f\"{category_text} {attributes_text}\"  \n",
    "\n",
    "    # Создание комбинированного текста  \n",
    "    attributes['combined_text'] = attributes.apply(extract_text_from_row, axis=1)  \n",
    "\n",
    "# Объединение данных для обучения  \n",
    "if not train.empty and not resnet.empty and not attributes.empty:  \n",
    "    train_data = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left')  \n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_1'})  \n",
    "    train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "    train_data = train_data.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', how='left')  \n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_2'})  \n",
    "    train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "    train_data = train_data.merge(attributes[['variantid', 'combined_text']], left_on='variantid1', right_on='variantid', how='left')  \n",
    "    train_data = train_data.rename(columns={'combined_text': 'text_1'})  \n",
    "    train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "    train_data = train_data.merge(attributes[['variantid', 'combined_text']], left_on='variantid2', right_on='variantid', how='left')  \n",
    "    train_data = train_data.rename(columns={'combined_text': 'text_2'})  \n",
    "    train_data = train_data.drop(columns=['variantid'])  \n",
    "\n",
    "    train_data = train_data.dropna()  # Удаление строк с пропущенными значениями  \n",
    "else:  \n",
    "    train_data = pd.DataFrame()  \n",
    "\n",
    "# Шаг 4: Подготовка данных  \n",
    "if not train_data.empty:  \n",
    "    # Создание объекта TfidfVectorizer  \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=2000)  \n",
    "\n",
    "    # Преобразование текстовых данных в эмбеддинги  \n",
    "    text_data = train_data['text_1'].fillna('') + ' ' + train_data['text_2'].fillna('')  \n",
    "    text_embeddings = tfidf_vectorizer.fit_transform(text_data).toarray()  \n",
    "\n",
    "    # Функция для объединения эмбеддингов изображений и текста  \n",
    "    def combine_embeddings(row):  \n",
    "        pic_embeddings = np.concatenate([row['pic_embeddings_1'][0], row['pic_embeddings_2'][0]])  \n",
    "        text_embeddings_row = text_embeddings[row.name]  \n",
    "        return np.concatenate([pic_embeddings, text_embeddings_row])  \n",
    "\n",
    "    # Применение функции к данным  \n",
    "    train_data['combined_embeddings'] = train_data.apply(combine_embeddings, axis=1)  \n",
    "\n",
    "    # Создание матрицы признаков и вектора целей  \n",
    "    X = np.vstack(train_data['combined_embeddings'].values)  \n",
    "    y = train_data['target']  \n",
    "\n",
    "    # Разделение данных на обучающую и валидационную выборки  \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=23)  \n",
    "else:  \n",
    "    X_train, X_val, y_train, y_val = (np.array([]),) * 4  \n",
    "\n",
    "# Шаг 5: Обучение модели  \n",
    "if X_train.size > 0 and y_train.size > 0:  \n",
    "    # Создание и обучение модели логистической регрессии  \n",
    "    model = LogisticRegression(max_iter=2000)  \n",
    "    model.fit(X_train, y_train)  \n",
    "\n",
    "    # Сохранение модели  \n",
    "    joblib.dump(model, 'baseline1.pkl')  \n",
    "\n",
    "# Шаг 6: Оценка модели  \n",
    "if X_val.size > 0 and y_val.size > 0:  \n",
    "    # Прогнозирование вероятностей  \n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]  \n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)  \n",
    "\n",
    "    # Вычисление метрик точности и полноты  \n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)  \n",
    "    prauc = auc(recall, precision)  \n",
    "    print(f'PRAUC: {prauc}')  \n",
    "\n",
    "    # Сохранение вектораизатора  \n",
    "    joblib.dump(tfidf_vectorizer, 'vectorizer1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d488e99-1a8f-4000-a344-41a6c3371906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55caa0-ef99-43d0-97cb-09482a493cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975a18b-3caf-4ef7-8b82-ab4d6f9a5b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459a7f41-edfc-465f-83f8-c139886e0cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a76bdd82-28c0-4c99-9782-8005dbbb877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text and Bert DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>name_bert_64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47920382</td>\n",
       "      <td>Игрушка для ванной , сувенир Уточка Дьяволица</td>\n",
       "      <td>Серия  уточек бренда FUNNY DUCKS представлена ...</td>\n",
       "      <td>[-0.24964939057826996, 0.6433379650115967, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49801845</td>\n",
       "      <td>Стразы(бусины) клеевые на листе 9*16 см</td>\n",
       "      <td>Стразы(бусины) клеевые на листе 9*16 см</td>\n",
       "      <td>[-0.6397916078567505, 0.3660058379173279, 0.67...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49853444</td>\n",
       "      <td>Набор для вышивания Vervaco \"Подушка. Геометри...</td>\n",
       "      <td>Состав набора: канва-страмин Zweigart с нанесе...</td>\n",
       "      <td>[-0.27170804142951965, 0.3709857165813446, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49893028</td>\n",
       "      <td>Кружево коклюшечное \"Prym\", цвет: серый, 8 мм,...</td>\n",
       "      <td>Кружево коклюшечное \"Prym\" предназначено для о...</td>\n",
       "      <td>[-0.5911799669265747, 0.45404374599456787, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49987483</td>\n",
       "      <td>Оригами Настольная игра Фиксики Кодовый замок</td>\n",
       "      <td>В настольной игре Оригами \"Фиксики. Кодовый за...</td>\n",
       "      <td>[-0.29846125841140747, 0.41105785965919495, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252564</th>\n",
       "      <td>1799186009</td>\n",
       "      <td>ЭлектросамокатSKU-000226hb, черный</td>\n",
       "      <td>Введение в продукт&lt;br/&gt;&lt;br/&gt;Название продукта:...</td>\n",
       "      <td>[-0.5560247898101807, 0.4461979568004608, 0.47...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252565</th>\n",
       "      <td>1801305376</td>\n",
       "      <td>Золотой человек | Дик Филип Киндред</td>\n",
       "      <td>«…Выворачивать наизнанку базовые категории реа...</td>\n",
       "      <td>[-0.4282682240009308, 0.49429962038993835, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252566</th>\n",
       "      <td>1802944592</td>\n",
       "      <td>Daiwa Монофильная леска для рыбалки, размотка:...</td>\n",
       "      <td>None</td>\n",
       "      <td>[-0.5640578866004944, 0.13208043575286865, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252567</th>\n",
       "      <td>1803038155</td>\n",
       "      <td>Шарлатанка в Академии драконов | Миш Виктория</td>\n",
       "      <td>Тяжело поступать адекватно, когда тебя «любящи...</td>\n",
       "      <td>[-0.35129043459892273, 0.48004797101020813, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252568</th>\n",
       "      <td>1809203525</td>\n",
       "      <td>Фигурка садовая</td>\n",
       "      <td>Садовая фигурка изготовлена из холоднокатанног...</td>\n",
       "      <td>[-0.4391442835330963, 0.5276063680648804, 0.56...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2252569 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          variantid                                               name  \\\n",
       "0          47920382      Игрушка для ванной , сувенир Уточка Дьяволица   \n",
       "1          49801845            Стразы(бусины) клеевые на листе 9*16 см   \n",
       "2          49853444  Набор для вышивания Vervaco \"Подушка. Геометри...   \n",
       "3          49893028  Кружево коклюшечное \"Prym\", цвет: серый, 8 мм,...   \n",
       "4          49987483      Оригами Настольная игра Фиксики Кодовый замок   \n",
       "...             ...                                                ...   \n",
       "2252564  1799186009                 ЭлектросамокатSKU-000226hb, черный   \n",
       "2252565  1801305376                Золотой человек | Дик Филип Киндред   \n",
       "2252566  1802944592  Daiwa Монофильная леска для рыбалки, размотка:...   \n",
       "2252567  1803038155      Шарлатанка в Академии драконов | Миш Виктория   \n",
       "2252568  1809203525                                    Фигурка садовая   \n",
       "\n",
       "                                               description  \\\n",
       "0        Серия  уточек бренда FUNNY DUCKS представлена ...   \n",
       "1                  Стразы(бусины) клеевые на листе 9*16 см   \n",
       "2        Состав набора: канва-страмин Zweigart с нанесе...   \n",
       "3        Кружево коклюшечное \"Prym\" предназначено для о...   \n",
       "4        В настольной игре Оригами \"Фиксики. Кодовый за...   \n",
       "...                                                    ...   \n",
       "2252564  Введение в продукт<br/><br/>Название продукта:...   \n",
       "2252565  «…Выворачивать наизнанку базовые категории реа...   \n",
       "2252566                                               None   \n",
       "2252567  Тяжело поступать адекватно, когда тебя «любящи...   \n",
       "2252568  Садовая фигурка изготовлена из холоднокатанног...   \n",
       "\n",
       "                                              name_bert_64  \n",
       "0        [-0.24964939057826996, 0.6433379650115967, 0.4...  \n",
       "1        [-0.6397916078567505, 0.3660058379173279, 0.67...  \n",
       "2        [-0.27170804142951965, 0.3709857165813446, 0.4...  \n",
       "3        [-0.5911799669265747, 0.45404374599456787, 0.4...  \n",
       "4        [-0.29846125841140747, 0.41105785965919495, 0....  \n",
       "...                                                    ...  \n",
       "2252564  [-0.5560247898101807, 0.4461979568004608, 0.47...  \n",
       "2252565  [-0.4282682240009308, 0.49429962038993835, 0.5...  \n",
       "2252566  [-0.5640578866004944, 0.13208043575286865, 0.6...  \n",
       "2252567  [-0.35129043459892273, 0.48004797101020813, 0....  \n",
       "2252568  [-0.4391442835330963, 0.5276063680648804, 0.56...  \n",
       "\n",
       "[2252569 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05bcb6e3-8b18-4a00-a069-00dfea74d753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid1</th>\n",
       "      <th>variantid2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1447875869</td>\n",
       "      <td>1447872068</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1176231201</td>\n",
       "      <td>284733670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>658617865</td>\n",
       "      <td>549848659</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>719320625</td>\n",
       "      <td>719370486</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1067645658</td>\n",
       "      <td>949954740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168511</th>\n",
       "      <td>970087885</td>\n",
       "      <td>944020046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168512</th>\n",
       "      <td>834554806</td>\n",
       "      <td>118393780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168513</th>\n",
       "      <td>1473905394</td>\n",
       "      <td>1473908406</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168514</th>\n",
       "      <td>104892989</td>\n",
       "      <td>1549506903</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168515</th>\n",
       "      <td>1370539398</td>\n",
       "      <td>1306446495</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1168516 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         variantid1  variantid2  target\n",
       "0        1447875869  1447872068       1\n",
       "1        1176231201   284733670       1\n",
       "2         658617865   549848659       0\n",
       "3         719320625   719370486       1\n",
       "4        1067645658   949954740       0\n",
       "...             ...         ...     ...\n",
       "1168511   970087885   944020046       0\n",
       "1168512   834554806   118393780       1\n",
       "1168513  1473905394  1473908406       1\n",
       "1168514   104892989  1549506903       1\n",
       "1168515  1370539398  1306446495       1\n",
       "\n",
       "[1168516 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "dc68aba4-fd6c-4bdb-8682-9f4f32b94dae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992ec9a-52ea-4b62-b8cb-0fe0a434e596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3378ee-2fc2-44fa-80dd-4f3bb1312bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRAUC: 0.7724866290617343\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import precision_recall_curve, auc\n",
    "# import joblib\n",
    "# import json\n",
    "\n",
    "# # Функция для загрузки данных\n",
    "# def load_data(sample_size=0.1):\n",
    "#     # Загрузка данных из файлов Parquet\n",
    "#     attributes = pd.read_parquet('F:/competition/attributes.parquet')\n",
    "#     resnet = pd.read_parquet('F:/competition/resnet.parquet')\n",
    "#     text_and_bert = pd.read_parquet('F:/competition/text_and_bert.parquet')\n",
    "#     train = pd.read_parquet('F:/competition/train.parquet')\n",
    "#     test = pd.read_parquet('F:/competition/test.parquet')\n",
    "\n",
    "#     # Если sample_size меньше 1, то выборка данных\n",
    "#     if sample_size < 1:\n",
    "#         attributes = attributes.sample(frac=sample_size, random_state=42)\n",
    "#         resnet = resnet.sample(frac=sample_size, random_state=42)\n",
    "#         text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)\n",
    "#         train = train.sample(frac=sample_size, random_state=42)\n",
    "#         test = test.sample(frac=sample_size, random_state=42)\n",
    "\n",
    "#     return attributes, resnet, text_and_bert, train, test\n",
    "# # Шаг 1: Загрузка данных и проверка наличия столбцов\n",
    "# def load_parquet_file(path, expected_columns):\n",
    "#     \"\"\"Функция для загрузки данных из файла Parquet с проверкой столбцов.\"\"\"\n",
    "#     try:\n",
    "#         df = pd.read_parquet(path)\n",
    "#         missing_cols = [col for col in expected_columns if col not in df.columns]\n",
    "#         if missing_cols:\n",
    "#             raise ValueError(f\"Отсутствуют столбцы в файле {path}: {', '.join(missing_cols)}\")\n",
    "#         return df\n",
    "#     except Exception as e:\n",
    "#         print(f\"Ошибка при загрузке {path}: {e}\")\n",
    "#         return pd.DataFrame()  # Возвращаем пустой DataFrame в случае ошибки\n",
    "\n",
    "# # Загрузка данных из файлов с проверкой столбцов\n",
    "# attributes = load_parquet_file(attributes_path, ['variantid', 'categories', 'characteristic_attributes_mapping'])\n",
    "# resnet = load_parquet_file(resnet_path, ['variantid', 'main_pic_embeddings_resnet_v1'])\n",
    "# text_and_bert = load_parquet_file(text_and_bert_path, ['variantid', 'combined_text'])\n",
    "# train = load_parquet_file(train_path, ['variantid1', 'variantid2', 'target'])\n",
    "\n",
    "# # Шаг 2: Обработка атрибутов и текстов\n",
    "# if not attributes.empty:\n",
    "#     # Преобразование JSON строк в объекты\n",
    "#     attributes['categories'] = attributes['categories'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "#     attributes['characteristic_attributes_mapping'] = attributes['characteristic_attributes_mapping'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n",
    "\n",
    "#     # Функция для извлечения текста из строки\n",
    "#     def extract_text_from_row(row):\n",
    "#         category_text = ' '.join(\n",
    "#             [' '.join(map(str, v)) if isinstance(v, list) else str(v) for v in list(row['categories'].values())]\n",
    "#         )\n",
    "#         attributes_text = ' '.join(\n",
    "#             [' '.join(map(str, v)) if isinstance(v, list) else str(v) for v in list(row['characteristic_attributes_mapping'].values())]\n",
    "#         )\n",
    "#         return f\"{category_text} {attributes_text}\"\n",
    "\n",
    "#     # Применение функции к каждому ряду данных\n",
    "#     attributes['combined_text'] = attributes.apply(extract_text_from_row, axis=1)\n",
    "\n",
    "# # Проверка наличия необходимых столбцов перед объединением\n",
    "# if not text_and_bert.empty and 'combined_text' in text_and_bert.columns:\n",
    "#     combined_df = attributes.merge(text_and_bert[['variantid', 'combined_text']], on='variantid', how='left')\n",
    "# else:\n",
    "#     combined_df = attributes\n",
    "\n",
    "# # Шаг 3: Объединение данных\n",
    "# if not train.empty and not resnet.empty and not combined_df.empty:\n",
    "#     train_data = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left')\n",
    "#     train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_1'})\n",
    "#     train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "#     train_data = train_data.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', how='left')\n",
    "#     train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_2'})\n",
    "#     train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "#     train_data = train_data.merge(combined_df[['variantid', 'combined_text']], left_on='variantid1', right_on='variantid', how='left')\n",
    "#     train_data = train_data.rename(columns={'combined_text': 'text_1'})\n",
    "#     train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "#     train_data = train_data.merge(combined_df[['variantid', 'combined_text']], left_on='variantid2', right_on='variantid', how='left')\n",
    "#     train_data = train_data.rename(columns={'combined_text': 'text_2'})\n",
    "#     train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "#     train_data = train_data.dropna()  # Удаление строк с пропущенными значениями\n",
    "# else:\n",
    "#     train_data = pd.DataFrame()\n",
    "\n",
    "# # Шаг 4: Подготовка данных\n",
    "if not train_data.empty:\n",
    "    # Создание объекта TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=2000)\n",
    "\n",
    "    # Преобразование текстовых данных в эмбеддинги\n",
    "    text_data = train_data['text_1'].fillna('') + ' ' + train_data['text_2'].fillna('')\n",
    "    text_embeddings = tfidf_vectorizer.fit_transform(text_data).toarray()\n",
    "\n",
    "    # Функция для объединения эмбеддингов изображений и текста\n",
    "    def combine_embeddings(row):\n",
    "        pic_embeddings = np.concatenate([row['pic_embeddings_1'][0], row['pic_embeddings_2'][0]])\n",
    "        text_embeddings_row = text_embeddings[row.name]\n",
    "        return np.concatenate([pic_embeddings, text_embeddings_row])\n",
    "\n",
    "    # Применение функции к данным\n",
    "    train_data['combined_embeddings'] = train_data.apply(combine_embeddings, axis=1)\n",
    "\n",
    "    # Создание матрицы признаков и вектора целей\n",
    "    X = np.vstack(train_data['combined_embeddings'].values)\n",
    "    y = train_data['target']\n",
    "\n",
    "    # Разделение данных на обучающую и валидационную выборки\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "else:\n",
    "    X_train, X_val, y_train, y_val = (np.array([]),) * 4\n",
    "\n",
    "# Шаг 5: Обучение модели\n",
    "if X_train.size > 0 and y_train.size > 0:\n",
    "    # Создание и обучение модели логистической регрессии\n",
    "    model = LogisticRegression(max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Сохранение модели\n",
    "    joblib.dump(model, 'baseline3.pkl')\n",
    "\n",
    "# Шаг 6: Оценка модели\n",
    "if X_val.size > 0 and y_val.size > 0:\n",
    "    # Прогнозирование вероятностей\n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "    # Вычисление метрик точности и полноты\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "    prauc = auc(recall, precision)\n",
    "    print(f'PRAUC: {prauc}')\n",
    "\n",
    "    # Сохранение вектораизатора\n",
    "    joblib.dump(tfidf_vectorizer, 'vectorizer3.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7085847-8749-4022-94a2-e9c2f5b80da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f2df5-5ac0-4a10-af6a-fbf47d7ea7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa0d50-2488-4434-a984-7632464bc89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e83393-60b6-421d-9c3d-1151347071a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3966572-841e-4db1-8e49-b0f600b1b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Функция для загрузки данных\n",
    "def load_data(sample_size=0.1):\n",
    "    # Загрузка данных из файлов Parquet\n",
    "    attributes = pd.read_parquet('F:/competition/attributes.parquet')\n",
    "    resnet = pd.read_parquet('F:/competition/resnet.parquet')\n",
    "    text_and_bert = pd.read_parquet('F:/competition/text_and_bert.parquet')\n",
    "    train = pd.read_parquet('F:/competition/train.parquet')\n",
    "    test = pd.read_parquet('F:/competition/test.parquet')\n",
    "\n",
    "    # Если sample_size меньше 1, то выборка данных\n",
    "    if sample_size < 1:\n",
    "        attributes = attributes.sample(frac=sample_size, random_state=42)\n",
    "        resnet = resnet.sample(frac=sample_size, random_state=42)\n",
    "        text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)\n",
    "        train = train.sample(frac=sample_size, random_state=42)\n",
    "        test = test.sample(frac=sample_size, random_state=42)\n",
    "\n",
    "    return attributes, resnet, text_and_bert, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01607e88-80b3-472a-a4cb-c52f25f30ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes, resnet, text_and_bert, train, test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8804f7cc-7bb2-4ae9-8002-1cf41a20e283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid</th>\n",
       "      <th>categories</th>\n",
       "      <th>characteristic_attributes_mapping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>809556</th>\n",
       "      <td>1068398160</td>\n",
       "      <td>{\"1\": \"Одежда и обувь\", \"2\": \"Одежда\", \"3\": \"О...</td>\n",
       "      <td>{\"Модель\": [\"вязаная\", \"утепленная модель\"], \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253664</th>\n",
       "      <td>80256086</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Строительство и ремонт\", \"3...</td>\n",
       "      <td>{\"Комплектация\": [\"светильник, инструкция\"], \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214869</th>\n",
       "      <td>1402305170</td>\n",
       "      <td>{\"1\": \"EPG\", \"2\": \"Строительство и ремонт\", \"3...</td>\n",
       "      <td>{\"Комплектация\": [\"Встраиваемый светильник Art...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variantid                                         categories  \\\n",
       "809556  1068398160  {\"1\": \"Одежда и обувь\", \"2\": \"Одежда\", \"3\": \"О...   \n",
       "253664    80256086  {\"1\": \"EPG\", \"2\": \"Строительство и ремонт\", \"3...   \n",
       "214869  1402305170  {\"1\": \"EPG\", \"2\": \"Строительство и ремонт\", \"3...   \n",
       "\n",
       "                        characteristic_attributes_mapping  \n",
       "809556  {\"Модель\": [\"вязаная\", \"утепленная модель\"], \"...  \n",
       "253664  {\"Комплектация\": [\"светильник, инструкция\"], \"...  \n",
       "214869  {\"Комплектация\": [\"Встраиваемый светильник Art...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Просмотр первых нескольких строк каждого DataFrame\n",
    "print(\"Attributes DataFrame:\")\n",
    "attributes.head(3)  # Вывод первых 5 строк DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de5b5883-a958-4f31-a5db-1ac36c4fb308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resnet DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid</th>\n",
       "      <th>main_pic_embeddings_resnet_v1</th>\n",
       "      <th>pic_embeddings_resnet_v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>809556</th>\n",
       "      <td>1068398160</td>\n",
       "      <td>[[0.4887434244155884, 0.2201225757598877, -0.2...</td>\n",
       "      <td>[[0.49651038646698, 0.3525044620037079, -0.650...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253664</th>\n",
       "      <td>80256086</td>\n",
       "      <td>[[0.15673470497131348, 0.19669684767723083, 1....</td>\n",
       "      <td>[[0.22057659924030304, 0.23254850506782532, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214869</th>\n",
       "      <td>1402305170</td>\n",
       "      <td>[[-1.1406502723693848, 0.29915758967399597, -0...</td>\n",
       "      <td>[[-0.22224298119544983, 0.037943556904792786, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variantid                      main_pic_embeddings_resnet_v1  \\\n",
       "809556  1068398160  [[0.4887434244155884, 0.2201225757598877, -0.2...   \n",
       "253664    80256086  [[0.15673470497131348, 0.19669684767723083, 1....   \n",
       "214869  1402305170  [[-1.1406502723693848, 0.29915758967399597, -0...   \n",
       "\n",
       "                                 pic_embeddings_resnet_v1  \n",
       "809556  [[0.49651038646698, 0.3525044620037079, -0.650...  \n",
       "253664  [[0.22057659924030304, 0.23254850506782532, 0....  \n",
       "214869  [[-0.22224298119544983, 0.037943556904792786, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Просмотр первых нескольких строк каждого DataFrame\n",
    "print(\"\\nResnet DataFrame:\")\n",
    "resnet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de2a0b80-35fe-4e16-bc3c-6d625ffd0f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text and Bert DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>name_bert_64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>809556</th>\n",
       "      <td>1068398160</td>\n",
       "      <td>Пинетки RUSKNIT Малышам</td>\n",
       "      <td>Долгожданная новинка! Утепленные пинетки вязан...</td>\n",
       "      <td>[-0.4336099624633789, 0.33932164311408997, 0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253664</th>\n",
       "      <td>80256086</td>\n",
       "      <td>Подвесной светильник Maytoni Uva MOD059PL-03G</td>\n",
       "      <td>Светильник потолочный подвесной Maytoni MOD059...</td>\n",
       "      <td>[-0.5351067781448364, 0.6248574256896973, 0.52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214869</th>\n",
       "      <td>1402305170</td>\n",
       "      <td>Arte Lamp Встраиваемый светильник</td>\n",
       "      <td>Встраиваемый светильник Arte Lamp Keid A2162PL...</td>\n",
       "      <td>[-0.34187906980514526, 0.4898632764816284, 0.7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variantid                                           name  \\\n",
       "809556  1068398160                        Пинетки RUSKNIT Малышам   \n",
       "253664    80256086  Подвесной светильник Maytoni Uva MOD059PL-03G   \n",
       "214869  1402305170              Arte Lamp Встраиваемый светильник   \n",
       "\n",
       "                                              description  \\\n",
       "809556  Долгожданная новинка! Утепленные пинетки вязан...   \n",
       "253664  Светильник потолочный подвесной Maytoni MOD059...   \n",
       "214869  Встраиваемый светильник Arte Lamp Keid A2162PL...   \n",
       "\n",
       "                                             name_bert_64  \n",
       "809556  [-0.4336099624633789, 0.33932164311408997, 0.7...  \n",
       "253664  [-0.5351067781448364, 0.6248574256896973, 0.52...  \n",
       "214869  [-0.34187906980514526, 0.4898632764816284, 0.7...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nText and Bert DataFrame:\")\n",
    "text_and_bert.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d3b93a-28d6-450c-a871-a754bf0721f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid1</th>\n",
       "      <th>variantid2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181389</th>\n",
       "      <td>938776155</td>\n",
       "      <td>473815138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269066</th>\n",
       "      <td>1349745192</td>\n",
       "      <td>1349703019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044282</th>\n",
       "      <td>1450853415</td>\n",
       "      <td>1450853943</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variantid1  variantid2  target\n",
       "181389    938776155   473815138       1\n",
       "269066   1349745192  1349703019       0\n",
       "1044282  1450853415  1450853943       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTrain DataFrame:\")\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cdceae2-83aa-4a12-ab4f-28f207c4bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid1</th>\n",
       "      <th>variantid2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12928</th>\n",
       "      <td>641921785</td>\n",
       "      <td>646752712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17577</th>\n",
       "      <td>1735144642</td>\n",
       "      <td>1727990945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25225</th>\n",
       "      <td>1785914688</td>\n",
       "      <td>1769974301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       variantid1  variantid2\n",
       "12928   641921785   646752712\n",
       "17577  1735144642  1727990945\n",
       "25225  1785914688  1769974301"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTest DataFrame:\")\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d7125e-7c3b-4bc3-91f8-6dde99fdae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attributes DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 225257 entries, 809556 to 2179262\n",
      "Data columns (total 3 columns):\n",
      " #   Column                             Non-Null Count   Dtype \n",
      "---  ------                             --------------   ----- \n",
      " 0   variantid                          225257 non-null  int64 \n",
      " 1   categories                         225257 non-null  object\n",
      " 2   characteristic_attributes_mapping  225257 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 6.9+ MB\n",
      "\n",
      "Resnet DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 225257 entries, 809556 to 2179262\n",
      "Data columns (total 3 columns):\n",
      " #   Column                         Non-Null Count   Dtype \n",
      "---  ------                         --------------   ----- \n",
      " 0   variantid                      225257 non-null  int64 \n",
      " 1   main_pic_embeddings_resnet_v1  225257 non-null  object\n",
      " 2   pic_embeddings_resnet_v1       166276 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 6.9+ MB\n",
      "\n",
      "Text and Bert DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 225257 entries, 809556 to 2179262\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   variantid     225257 non-null  int64 \n",
      " 1   name          225257 non-null  object\n",
      " 2   description   198981 non-null  object\n",
      " 3   name_bert_64  225257 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 8.6+ MB\n",
      "\n",
      "Train DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 116852 entries, 181389 to 25571\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype\n",
      "---  ------      --------------   -----\n",
      " 0   variantid1  116852 non-null  int64\n",
      " 1   variantid2  116852 non-null  int64\n",
      " 2   target      116852 non-null  int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 3.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Для более подробного анализа можно также использовать метод .info() для отображения информации о столбцах и типах данных\n",
    "print(\"\\nAttributes DataFrame Info:\")\n",
    "attributes.info()\n",
    "\n",
    "print(\"\\nResnet DataFrame Info:\")\n",
    "resnet.info()\n",
    "\n",
    "print(\"\\nText and Bert DataFrame Info:\")\n",
    "text_and_bert.info()\n",
    "\n",
    "print(\"\\nTrain DataFrame Info:\")\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ef681-7cdb-4c09-9c29-eda4d5503304",
   "metadata": {},
   "source": [
    "# Предобработка таблицы Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64b6ad60-1e12-4a35-80ba-55614b942779",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Загрузка данных\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#attributes = pd.read_parquet('F:/competition/attributes.parquet')\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Преобразование JSON-строк в словари Python\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m attributes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mattributes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategories\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m attributes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacteristic_attributes_mapping\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m attributes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacteristic_attributes_mapping\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(json\u001b[38;5;241m.\u001b[39mloads)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Пример проверки преобразованных данных\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\series.py:4917\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\json\\__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[1;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    340\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not dict"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Загрузка данных\n",
    "#attributes = pd.read_parquet('F:/competition/attributes.parquet')\n",
    "\n",
    "# Преобразование JSON-строк в словари Python\n",
    "attributes['categories'] = attributes['categories'].apply(json.loads)\n",
    "attributes['characteristic_attributes_mapping'] = attributes['characteristic_attributes_mapping'].apply(json.loads)\n",
    "\n",
    "# Пример проверки преобразованных данных\n",
    "attributes.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71393f23-04a9-47c0-811c-0aeac8f3bdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 225257 entries, 809556 to 2179262\n",
      "Data columns (total 3 columns):\n",
      " #   Column                             Non-Null Count   Dtype \n",
      "---  ------                             --------------   ----- \n",
      " 0   variantid                          225257 non-null  int64 \n",
      " 1   categories                         225257 non-null  object\n",
      " 2   characteristic_attributes_mapping  225257 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "attributes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70b299d-84ed-4e54-b46f-412fb294ef7e",
   "metadata": {},
   "source": [
    "# Предобработка таблицы Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "614a9a62-1a0f-408b-8d3c-0d5d256a1a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variantid                         int64\n",
      "main_pic_embeddings_resnet_v1    object\n",
      "pic_embeddings_resnet_v1         object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid</th>\n",
       "      <th>main_pic_embeddings_resnet_v1</th>\n",
       "      <th>pic_embeddings_resnet_v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>809556</th>\n",
       "      <td>1068398160</td>\n",
       "      <td>[[0.4887434244155884, 0.2201225757598877, -0.2...</td>\n",
       "      <td>[[0.49651038646698, 0.3525044620037079, -0.650...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253664</th>\n",
       "      <td>80256086</td>\n",
       "      <td>[[0.15673470497131348, 0.19669684767723083, 1....</td>\n",
       "      <td>[[0.22057659924030304, 0.23254850506782532, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214869</th>\n",
       "      <td>1402305170</td>\n",
       "      <td>[[-1.1406502723693848, 0.29915758967399597, -0...</td>\n",
       "      <td>[[-0.22224298119544983, 0.037943556904792786, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variantid                      main_pic_embeddings_resnet_v1  \\\n",
       "809556  1068398160  [[0.4887434244155884, 0.2201225757598877, -0.2...   \n",
       "253664    80256086  [[0.15673470497131348, 0.19669684767723083, 1....   \n",
       "214869  1402305170  [[-1.1406502723693848, 0.29915758967399597, -0...   \n",
       "\n",
       "                                 pic_embeddings_resnet_v1  \n",
       "809556  [[0.49651038646698, 0.3525044620037079, -0.650...  \n",
       "253664  [[0.22057659924030304, 0.23254850506782532, 0....  \n",
       "214869  [[-0.22224298119544983, 0.037943556904792786, ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Загрузка данных\n",
    "#resnet = pd.read_parquet('F:/competition/resnet.parquet')\n",
    "\n",
    "# Проверка формата данных в столбцах\n",
    "print(resnet.dtypes)\n",
    "print(resnet.head())\n",
    "\n",
    "# Преобразование данных в массивы numpy (если они не уже в таком формате)\n",
    "def ensure_array(data):\n",
    "    if isinstance(data, str):  # Если данные строка, предположительно в формате JSON\n",
    "        try:\n",
    "            return np.array(json.loads(data))\n",
    "        except json.JSONDecodeError:\n",
    "            return np.zeros(0)\n",
    "    elif isinstance(data, (list, np.ndarray)):  # Если данные уже список или массив\n",
    "        return np.array(data)\n",
    "    else:\n",
    "        return np.zeros(0)\n",
    "\n",
    "# Преобразование строк JSON в numpy массивы (если необходимо)\n",
    "resnet['main_pic_embeddings_resnet_v1'] = resnet['main_pic_embeddings_resnet_v1'].apply(ensure_array)\n",
    "resnet['pic_embeddings_resnet_v1'] = resnet['pic_embeddings_resnet_v1'].apply(ensure_array)\n",
    "\n",
    "# Пример проверки преобразованных данных\n",
    "resnet.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11cd2aa1-6798-4091-84ec-b9c4a6ee48b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variantid                         int64\n",
       "main_pic_embeddings_resnet_v1    object\n",
       "pic_embeddings_resnet_v1         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47401e-5eb5-4d31-9e8d-78cb5cebc022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1ac0ca6-0933-4892-889d-73f71e18a878",
   "metadata": {},
   "source": [
    "# Предобработка таблицы Text and Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0abb0e12-0813-45e9-91b2-d8d4984ba21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>name_bert_64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>809556</th>\n",
       "      <td>1068398160</td>\n",
       "      <td>Пинетки RUSKNIT Малышам</td>\n",
       "      <td>Долгожданная новинка! Утепленные пинетки вязан...</td>\n",
       "      <td>[-0.4336099624633789, 0.33932164311408997, 0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253664</th>\n",
       "      <td>80256086</td>\n",
       "      <td>Подвесной светильник Maytoni Uva MOD059PL-03G</td>\n",
       "      <td>Светильник потолочный подвесной Maytoni MOD059...</td>\n",
       "      <td>[-0.5351067781448364, 0.6248574256896973, 0.52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214869</th>\n",
       "      <td>1402305170</td>\n",
       "      <td>Arte Lamp Встраиваемый светильник</td>\n",
       "      <td>Встраиваемый светильник Arte Lamp Keid A2162PL...</td>\n",
       "      <td>[-0.34187906980514526, 0.4898632764816284, 0.7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variantid                                           name  \\\n",
       "809556  1068398160                        Пинетки RUSKNIT Малышам   \n",
       "253664    80256086  Подвесной светильник Maytoni Uva MOD059PL-03G   \n",
       "214869  1402305170              Arte Lamp Встраиваемый светильник   \n",
       "\n",
       "                                              description  \\\n",
       "809556  Долгожданная новинка! Утепленные пинетки вязан...   \n",
       "253664  Светильник потолочный подвесной Maytoni MOD059...   \n",
       "214869  Встраиваемый светильник Arte Lamp Keid A2162PL...   \n",
       "\n",
       "                                             name_bert_64  \n",
       "809556  [-0.4336099624633789, 0.33932164311408997, 0.7...  \n",
       "253664  [-0.5351067781448364, 0.6248574256896973, 0.52...  \n",
       "214869  [-0.34187906980514526, 0.4898632764816284, 0.7...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Загрузка данных\n",
    "#text_and_bert = pd.read_parquet('F:/competition/text_and_bert.parquet')\n",
    "\n",
    "# Преобразование строк JSON в numpy массивы\n",
    "def ensure_array(data):\n",
    "    if isinstance(data, str):  # Если данные строка, предположительно в формате JSON\n",
    "        try:\n",
    "            return np.array(json.loads(data))\n",
    "        except json.JSONDecodeError:\n",
    "            return np.zeros(0)\n",
    "    elif isinstance(data, (list, np.ndarray)):  # Если данные уже список или массив\n",
    "        return np.array(data)\n",
    "    else:\n",
    "        return np.zeros(0)\n",
    "\n",
    "text_and_bert['name_bert_64'] = text_and_bert['name_bert_64'].apply(ensure_array)\n",
    "\n",
    "# Пример проверки преобразованных данных\n",
    "text_and_bert.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172e9ae-bb8c-49b0-8440-610bad1cca45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541360b-254f-4088-854a-d106496ba625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb4647-7c82-4187-b1f2-7ec223086ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce2176-adf1-473d-8170-c8537ec4a3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b33bdc-2530-422d-90f9-9d9769ea935c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16407c-6dcf-49f2-b0db-0dbc96914f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5100866-1e35-4079-8da8-4b9390baaa01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f135ac90-8af3-438f-9d3d-ed7d12712835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Функция для загрузки данных\n",
    "def load_data(sample_size=1):\n",
    "    # Загрузка данных из файлов Parquet\n",
    "    attributes = pd.read_parquet('F:/competition/attributes.parquet')\n",
    "    resnet = pd.read_parquet('F:/competition/resnet.parquet')\n",
    "    text_and_bert = pd.read_parquet('F:/competition/text_and_bert.parquet')\n",
    "    train = pd.read_parquet('F:/competition/train.parquet')\n",
    "    test = pd.read_parquet('F:/competition/test.parquet')\n",
    "\n",
    "    # Если sample_size меньше 1, то выборка данных\n",
    "    if sample_size < 1:\n",
    "        attributes = attributes.sample(frac=sample_size, random_state=42)\n",
    "        resnet = resnet.sample(frac=sample_size, random_state=42)\n",
    "        text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)\n",
    "        train = train.sample(frac=sample_size, random_state=42)\n",
    "        test = test.sample(frac=sample_size, random_state=42)\n",
    "\n",
    "    return attributes, resnet, text_and_bert, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89ec643-ded5-4382-b8d9-f1e1bedf6f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes, resnet, text_and_bert, train, test = load_data(sample_size=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "582d27b9-84ce-4d41-a102-b41d4c19f7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variantid</th>\n",
       "      <th>categories</th>\n",
       "      <th>characteristic_attributes_mapping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>809556</th>\n",
       "      <td>1068398160</td>\n",
       "      <td>{'1': 'Одежда и обувь', '2': 'Одежда', '3': 'О...</td>\n",
       "      <td>{'Модель': ['вязаная', 'утепленная модель'], '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253664</th>\n",
       "      <td>80256086</td>\n",
       "      <td>{'1': 'EPG', '2': 'Строительство и ремонт', '3...</td>\n",
       "      <td>{'Комплектация': ['светильник, инструкция'], '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214869</th>\n",
       "      <td>1402305170</td>\n",
       "      <td>{'1': 'EPG', '2': 'Строительство и ремонт', '3...</td>\n",
       "      <td>{'Комплектация': ['Встраиваемый светильник Art...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         variantid                                         categories  \\\n",
       "809556  1068398160  {'1': 'Одежда и обувь', '2': 'Одежда', '3': 'О...   \n",
       "253664    80256086  {'1': 'EPG', '2': 'Строительство и ремонт', '3...   \n",
       "214869  1402305170  {'1': 'EPG', '2': 'Строительство и ремонт', '3...   \n",
       "\n",
       "                        characteristic_attributes_mapping  \n",
       "809556  {'Модель': ['вязаная', 'утепленная модель'], '...  \n",
       "253664  {'Комплектация': ['светильник, инструкция'], '...  \n",
       "214869  {'Комплектация': ['Встраиваемый светильник Art...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451db2d5-d865-44f0-82d6-dc7c842ac687",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Преобразование строк JSON в объекты Python\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m attributes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mattributes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategories\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m attributes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacteristic_attributes_mapping\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m attributes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharacteristic_attributes_mapping\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(json\u001b[38;5;241m.\u001b[39mloads)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\series.py:4917\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\json\\__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[1;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    340\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not dict"
     ]
    }
   ],
   "source": [
    "# Преобразование строк JSON в объекты Python\n",
    "attributes['categories'] = attributes['categories'].apply(json.loads)\n",
    "attributes['characteristic_attributes_mapping'] = attributes['characteristic_attributes_mapping'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394bf43-e5cf-4853-ab8e-b8ffdb2946be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de77ee-57ec-492b-90ed-41ae6619a61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5866f2-0dd9-4b33-b5ac-09d221182350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c5cfe9-a17b-430b-ae71-4824cc9e62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Заполнение пропущенных значений в 'description' пустыми строками\n",
    "# text_and_bert['description'] = text_and_bert['description'].fillna('')\n",
    "\n",
    "# # Объединение таблиц\n",
    "# # Объединение с Resnet для pic_embeddings_1\n",
    "# train_data = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left')\n",
    "# train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_1'})\n",
    "# train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "# # Объединение с Resnet для pic_embeddings_2\n",
    "# train_data = train_data.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', how='left')\n",
    "# train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_2'})\n",
    "# train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "# # Объединение с Text and Bert для text_embedding_1\n",
    "# train_data = train_data.merge(text_and_bert[['variantid', 'name_bert_64']], left_on='variantid1', right_on='variantid', how='left')\n",
    "# train_data = train_data.rename(columns={'name_bert_64': 'text_embedding_1'})\n",
    "# train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "# # Объединение с Text and Bert для text_embedding_2\n",
    "# train_data = train_data.merge(text_and_bert[['variantid', 'name_bert_64']], left_on='variantid2', right_on='variantid', how='left')\n",
    "# train_data = train_data.rename(columns={'name_bert_64': 'text_embedding_2'})\n",
    "# train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "# # Удаление строк с пропущенными значениями\n",
    "# train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1247d-b61a-4331-9212-ffff15a4ea3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d7212-be98-4840-a25f-950af0884f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d11039-13f4-48fc-9dbf-7ab7d99613e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4697f36a-96c9-4254-947c-588976ce16e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732de1fa-b041-4287-b278-624795aefd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c68b98c-fe69-40a9-9a83-fed5fa584d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9105bc4-5f93-4011-968b-bd55b5c319ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5b0f9-5738-49f9-bf1e-c296c1c5e25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea971320-b3f9-4cd9-98f0-596380bf708f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381086c-3321-45f4-91c1-d92492c89678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7a35e-ee33-461e-a084-18a73921a988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163dcef-8df1-4c06-ab84-fefa55822a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3aac5-7906-4955-88d8-e3d8312e2086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8767834f-5975-4e43-9e84-3fe57b7b7ce8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 158\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# Подготовка данных для тестирования\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# test_data = process_test_data(test, resnet, text_and_bert)  # Нужно реализовать эту функцию\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# test_predictions = model.predict_proba(test_data)[:, 1]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Запуск основной функции, если скрипт выполняется напрямую\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 144\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    141\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m prepare_data(train_data)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Оценка модели\u001b[39;00m\n\u001b[0;32m    147\u001b[0m evaluate_model(model, X_val, y_val)\n",
      "Cell \u001b[1;32mIn[4], line 116\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(X_train, y_train):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# Создание и обучение модели логистической регрессии\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     model \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m--> 116\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# Сохранение модели на диск\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdump(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1223\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1221\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1223\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\utils\\_array_api.py:751\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    749\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# # Функция для загрузки данных\n",
    "# def load_data(sample_size=0.1):\n",
    "#     # Загрузка данных из файлов Parquet\n",
    "#     attributes = pd.read_parquet('F:/competition/attributes.parquet')\n",
    "#     resnet = pd.read_parquet('F:/competition/resnet.parquet')\n",
    "#     text_and_bert = pd.read_parquet('F:/competition/text_and_bert.parquet')\n",
    "#     train = pd.read_parquet('F:/competition/train.parquet')\n",
    "#     test = pd.read_parquet('F:/competition/test.parquet')\n",
    "\n",
    "#     # Если sample_size меньше 1, то выборка данных\n",
    "#     if sample_size < 1:\n",
    "#         attributes = attributes.sample(frac=sample_size, random_state=42)\n",
    "#         resnet = resnet.sample(frac=sample_size, random_state=42)\n",
    "#         text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)\n",
    "#         train = train.sample(frac=sample_size, random_state=42)\n",
    "#         test = test.sample(frac=sample_size, random_state=42)\n",
    "\n",
    "#     return attributes, resnet, text_and_bert, train, test\n",
    "\n",
    "# Функция для обработки данных\n",
    "def process_data(attributes, resnet, text_and_bert, train):\n",
    "    # Преобразование строк JSON в объекты Python\n",
    "    attributes['categories'] = attributes['categories'].apply(json.loads)\n",
    "    attributes['characteristic_attributes_mapping'] = attributes['characteristic_attributes_mapping'].apply(json.loads)\n",
    "    \n",
    "    # Заполнение пропущенных значений в 'description' пустыми строками\n",
    "    text_and_bert['description'] = text_and_bert['description'].fillna('')\n",
    "\n",
    "    # Объединение таблиц\n",
    "    # Объединение с Resnet для pic_embeddings_1\n",
    "    train_data = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_1'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Объединение с Resnet для pic_embeddings_2\n",
    "    train_data = train_data.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_2'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Объединение с Text and Bert для text_embedding_1\n",
    "    train_data = train_data.merge(text_and_bert[['variantid', 'name_bert_64']], left_on='variantid1', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'name_bert_64': 'text_embedding_1'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Объединение с Text and Bert для text_embedding_2\n",
    "    train_data = train_data.merge(text_and_bert[['variantid', 'name_bert_64']], left_on='variantid2', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'name_bert_64': 'text_embedding_2'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Удаление строк с пропущенными значениями\n",
    "    train_data = train_data.dropna()\n",
    "\n",
    "    return train_data\n",
    "\n",
    "# Функция для комбинирования эмбеддингов\n",
    "def combine_embeddings(row):\n",
    "    try:\n",
    "        # Преобразование эмбеддингов из строковых представлений в массивы NumPy\n",
    "        pic_embeddings_1 = np.array(row['pic_embeddings_1'])\n",
    "        pic_embeddings_2 = np.array(row['pic_embeddings_2'])\n",
    "        text_embeddings_1 = np.array(row['text_embedding_1'])\n",
    "        text_embeddings_2 = np.array(row['text_embedding_2'])\n",
    "\n",
    "        # Проверка на пустые эмбеддинги\n",
    "        if pic_embeddings_1.size == 0 or pic_embeddings_2.size == 0 or text_embeddings_1.size == 0 or text_embeddings_2.size == 0:\n",
    "            return np.zeros((0,))\n",
    "\n",
    "        # Проверка на согласованность размеров\n",
    "        if len(pic_embeddings_1) != len(pic_embeddings_2) or len(text_embeddings_1) != len(text_embeddings_2):\n",
    "            return np.zeros((0,))\n",
    "\n",
    "        # Объединение эмбеддингов\n",
    "        pic_embeddings = np.concatenate([pic_embeddings_1, pic_embeddings_2])\n",
    "        text_embeddings = np.concatenate([text_embeddings_1, text_embeddings_2])\n",
    "\n",
    "        return np.concatenate([pic_embeddings, text_embeddings])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка преобразования эмбеддингов: {e}\")\n",
    "        return np.zeros((0,))\n",
    "\n",
    "# Функция для подготовки данных для обучения модели\n",
    "def prepare_data(train_data):\n",
    "    # Создание комбинированных эмбеддингов\n",
    "    train_data['combined_embeddings'] = train_data.apply(combine_embeddings, axis=1)\n",
    "\n",
    "    # Отделение эмбеддингов от целевой переменной\n",
    "    combined_embeddings = train_data['combined_embeddings'].tolist()\n",
    "\n",
    "    # Удаление строк с пустыми эмбеддингами\n",
    "    valid_rows = [embeddings for embeddings in combined_embeddings if embeddings.size > 0]\n",
    "    if len(valid_rows) == 0:\n",
    "        raise ValueError(\"Все строки содержат пустые эмбеддинги.\")\n",
    "    \n",
    "    # Преобразование списка валидных эмбеддингов в массив NumPy\n",
    "    X = np.vstack(valid_rows)\n",
    "    y = train_data.loc[train_data['combined_embeddings'].apply(lambda x: x.size > 0), 'target'].values\n",
    "\n",
    "    # Разделение данных на обучающую и валидационную выборки\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(X_train, y_train):\n",
    "    # Создание и обучение модели логистической регрессии\n",
    "    model = LogisticRegression(max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Сохранение модели на диск\n",
    "    joblib.dump(model, 'model.pkl')\n",
    "    return model\n",
    "\n",
    "# Функция для оценки модели\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    # Получение вероятностей предсказания\n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Вычисление Precision-Recall AUC\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(f'PR AUC: {pr_auc}')\n",
    "\n",
    "# Основная функция\n",
    "def main():\n",
    "    # Загрузка данных\n",
    "    #attributes, resnet, text_and_bert, train, test = load_data(sample_size=0.1)\n",
    "    \n",
    "    # Обработка данных\n",
    "    train_data = process_data(attributes, resnet, text_and_bert, train)\n",
    "    \n",
    "    # Подготовка данных для обучения\n",
    "    X_train, X_val, y_train, y_val = prepare_data(train_data)\n",
    "\n",
    "    # Обучение модели\n",
    "    model = train_model(X_train, y_train)\n",
    "    \n",
    "    # Оценка модели\n",
    "    evaluate_model(model, X_val, y_val)\n",
    "\n",
    "    # Подготовка данных для тестирования\n",
    "    # test_data = process_test_data(test, resnet, text_and_bert)  # Нужно реализовать эту функцию\n",
    "    # test_predictions = model.predict_proba(test_data)[:, 1]\n",
    "    # Создание файла с результатами\n",
    "    # submission = pd.DataFrame({'id': test['id'], 'target': test_predictions})\n",
    "    # submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Запуск основной функции, если скрипт выполняется напрямую\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae061f0-ebeb-4522-800e-9496e989d841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6159e105-b1c0-4565-a2cb-0f6d98bf8865",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 110\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# Подготовка данных для тестирования\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# test_data = process_test_data(test, resnet, text_and_bert)  # Нужно реализовать эту функцию\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# test_predictions = model.predict_proba(test_data)[:, 1]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Запуск основной функции, если скрипт выполняется напрямую\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 96\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m23\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Оценка модели\u001b[39;00m\n\u001b[0;32m     99\u001b[0m evaluate_model(model, X_val, y_val)\n",
      "Cell \u001b[1;32mIn[30], line 65\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(X_train, y_train):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# Создание и обучение модели логистической регрессии\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     model \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Сохранение модели на диск\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdump(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1223\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1221\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1223\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\utils\\_array_api.py:751\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    749\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Функция для обработки данных\n",
    "def process_data(attributes, resnet, text_and_bert, train):\n",
    "    # Проверка типа данных в столбцах 'categories' и 'characteristic_attributes_mapping'\n",
    "    if isinstance(attributes['categories'].iloc[0], str):\n",
    "        attributes['categories'] = attributes['categories'].apply(json.loads)\n",
    "    if isinstance(attributes['characteristic_attributes_mapping'].iloc[0], str):\n",
    "        attributes['characteristic_attributes_mapping'] = attributes['characteristic_attributes_mapping'].apply(json.loads)\n",
    "    \n",
    "    # Заполнение пропущенных значений в 'description' пустыми строками\n",
    "    text_and_bert['description'] = text_and_bert['description'].fillna('')\n",
    "\n",
    "    # Объединение таблиц\n",
    "    train_data = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_1'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    train_data = train_data.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_2'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    train_data = train_data.merge(text_and_bert[['variantid', 'name_bert_64']], left_on='variantid1', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'name_bert_64': 'text_embedding_1'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    train_data = train_data.merge(text_and_bert[['variantid', 'name_bert_64']], left_on='variantid2', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'name_bert_64': 'text_embedding_2'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Удаление строк с пропущенными значениями\n",
    "    train_data = train_data.dropna()\n",
    "\n",
    "    return train_data\n",
    "\n",
    "# Функция для подготовки данных для обучения модели\n",
    "def prepare_data(train_data):\n",
    "    # Разворачиваем векторы эмбеддингов в одномерные массивы\n",
    "    train_data['pic_embeddings_1'] = train_data['pic_embeddings_1'].apply(lambda x: np.array(x).flatten())\n",
    "    train_data['pic_embeddings_2'] = train_data['pic_embeddings_2'].apply(lambda x: np.array(x).flatten())\n",
    "    train_data['text_embedding_1'] = train_data['text_embedding_1'].apply(lambda x: np.array(x).flatten())\n",
    "    train_data['text_embedding_2'] = train_data['text_embedding_2'].apply(lambda x: np.array(x).flatten())\n",
    "\n",
    "    # Объединение всех признаков в один массив\n",
    "    X = np.hstack([\n",
    "        np.stack(train_data['pic_embeddings_1']),\n",
    "        np.stack(train_data['pic_embeddings_2']),\n",
    "        np.stack(train_data['text_embedding_1']),\n",
    "        np.stack(train_data['text_embedding_2'])\n",
    "    ])\n",
    "\n",
    "    y = train_data['target'].values\n",
    "    return X, y\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(X_train, y_train):\n",
    "    # Создание и обучение модели логистической регрессии\n",
    "    model = LogisticRegression(max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Сохранение модели на диск\n",
    "    joblib.dump(model, 'model.pkl')\n",
    "    return model\n",
    "\n",
    "# Функция для оценки модели\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    # Получение вероятностей предсказания\n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Вычисление Precision-Recall AUC\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(f'PR AUC: {pr_auc}')\n",
    "\n",
    "# Основная функция\n",
    "def main():\n",
    "    # Загрузка данных\n",
    "    # attributes, resnet, text_and_bert, train, test = load_data(sample_size=0.1)\n",
    "    \n",
    "    # Обработка данных\n",
    "    train_data = process_data(attributes, resnet, text_and_bert, train)\n",
    "    \n",
    "    # Подготовка данных для обучения\n",
    "    X, y = prepare_data(train_data)\n",
    "    \n",
    "    # Разделение данных на обучающую и валидационную выборки\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "    # Обучение модели\n",
    "    model = train_model(X_train, y_train)\n",
    "    \n",
    "    # Оценка модели\n",
    "    evaluate_model(model, X_val, y_val)\n",
    "\n",
    "    # Подготовка данных для тестирования\n",
    "    # test_data = process_test_data(test, resnet, text_and_bert)  # Нужно реализовать эту функцию\n",
    "    # test_predictions = model.predict_proba(test_data)[:, 1]\n",
    "    # Создание файла с результатами\n",
    "    # submission = pd.DataFrame({'id': test['id'], 'target': test_predictions})\n",
    "    # submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Запуск основной функции, если скрипт выполняется напрямую\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd1dc2-9965-48b2-b7cd-67c2d449d06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669cdb68-a007-4797-b712-ce0a60ec2cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c6165-3640-4f5d-968a-0c74311e3e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353e12d-7017-4a89-a409-b19a45da6149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd8549-fc1f-463a-ab2c-10771d127d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd100e9-f187-4fa2-bda6-150b79349ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62076743-35d8-4e02-81d1-bdb8b4a98c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018d016-99f6-47ac-a223-b1a3ef7d7271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6378e27-1121-4240-893f-af92e51ccfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5336a77-0c91-4e5d-8038-478901bfd50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04e47d-9d5b-42d0-a8cc-b3214bea01e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e28d02f-7c64-4a18-8f99-bf74950078b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер X_train: (933, 130)\n",
      "Размер y_train: 933\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 166\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Подготовка данных для тестирования\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# test_data = process_test_data(test, resnet, text_and_bert)  # Нужно реализовать эту функцию\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# test_predictions = model.predict_proba(test_data)[:, 1]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Запуск основной функции, если скрипт выполняется напрямую\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 166\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 152\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    149\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m prepare_data(train_data)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Оценка модели\u001b[39;00m\n\u001b[0;32m    155\u001b[0m evaluate_model(model, X_val, y_val)\n",
      "Cell \u001b[1;32mIn[9], line 124\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Создание и обучение модели логистической регрессии\u001b[39;00m\n\u001b[0;32m    123\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m--> 124\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Сохранение модели на диск\u001b[39;00m\n\u001b[0;32m    127\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1223\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1221\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1223\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ilyhkino_okrushenie\\lib\\site-packages\\sklearn\\utils\\_array_api.py:751\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    749\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# # Функция для загрузки данных\n",
    "# def load_data(sample_size=0.1):\n",
    "#     # Загрузка данных из файлов Parquet\n",
    "#     attributes = pd.read_parquet('F:/competition/attributes.parquet')\n",
    "#     resnet = pd.read_parquet('F:/competition/resnet.parquet')\n",
    "#     text_and_bert = pd.read_parquet('F:/competition/text_and_bert.parquet')\n",
    "#     train = pd.read_parquet('F:/competition/train.parquet')\n",
    "#     test = pd.read_parquet('F:/competition/test.parquet')\n",
    "\n",
    "#     # Если sample_size меньше 1, то выборка данных\n",
    "#     if sample_size < 1:\n",
    "#         attributes = attributes.sample(frac=sample_size, random_state=42)\n",
    "#         resnet = resnet.sample(frac=sample_size, random_state=42)\n",
    "#         text_and_bert = text_and_bert.sample(frac=sample_size, random_state=42)\n",
    "#         train = train.sample(frac=sample_size, random_state=42)\n",
    "#         test = test.sample(frac=sample_size, random_state=42)\n",
    "\n",
    "#     return attributes, resnet, text_and_bert, train, test\n",
    "\n",
    "# Функция для обработки данных\n",
    "def process_data(attributes, resnet, text_and_bert, train):\n",
    "    # Преобразование строк JSON в объекты Python\n",
    "    attributes['categories'] = attributes['categories'].apply(json.loads)\n",
    "    attributes['characteristic_attributes_mapping'] = attributes['characteristic_attributes_mapping'].apply(json.loads)\n",
    "    \n",
    "    # Заполнение пропущенных значений в 'description' пустыми строками\n",
    "    text_and_bert['description'] = text_and_bert['description'].fillna('')\n",
    "\n",
    "    # Объединение таблиц\n",
    "    # Объединение с Resnet для pic_embeddings_1\n",
    "    train_data = train.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid1', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_1'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Объединение с Resnet для pic_embeddings_2\n",
    "    train_data = train_data.merge(resnet[['variantid', 'main_pic_embeddings_resnet_v1']], left_on='variantid2', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'main_pic_embeddings_resnet_v1': 'pic_embeddings_2'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Объединение с Text and Bert для text_embedding_1\n",
    "    train_data = train_data.merge(text_and_bert[['variantid', 'name_bert_64']], left_on='variantid1', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'name_bert_64': 'text_embedding_1'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Объединение с Text and Bert для text_embedding_2\n",
    "    train_data = train_data.merge(text_and_bert[['variantid', 'name_bert_64']], left_on='variantid2', right_on='variantid', how='left')\n",
    "    train_data = train_data.rename(columns={'name_bert_64': 'text_embedding_2'})\n",
    "    train_data = train_data.drop(columns=['variantid'])\n",
    "\n",
    "    # Удаление строк с пропущенными значениями\n",
    "    train_data = train_data.dropna()\n",
    "\n",
    "    return train_data\n",
    "\n",
    "# Функция для комбинирования эмбеддингов\n",
    "def combine_embeddings(row):\n",
    "    try:\n",
    "        # Преобразование эмбеддингов из строковых представлений в массивы NumPy\n",
    "        pic_embeddings_1 = np.array(row['pic_embeddings_1'])\n",
    "        pic_embeddings_2 = np.array(row['pic_embeddings_2'])\n",
    "        text_embeddings_1 = np.array(row['text_embedding_1'])\n",
    "        text_embeddings_2 = np.array(row['text_embedding_2'])\n",
    "\n",
    "        # Проверка на пустые эмбеддинги\n",
    "        if pic_embeddings_1.size == 0 or pic_embeddings_2.size == 0 or text_embeddings_1.size == 0 or text_embeddings_2.size == 0:\n",
    "            return np.zeros((0,))\n",
    "\n",
    "        # Проверка на согласованность размеров\n",
    "        if len(pic_embeddings_1) != len(pic_embeddings_2) or len(text_embeddings_1) != len(text_embeddings_2):\n",
    "            return np.zeros((0,))\n",
    "\n",
    "        # Объединение эмбеддингов\n",
    "        pic_embeddings = np.concatenate([pic_embeddings_1, pic_embeddings_2])\n",
    "        text_embeddings = np.concatenate([text_embeddings_1, text_embeddings_2])\n",
    "\n",
    "        return np.concatenate([pic_embeddings, text_embeddings])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка преобразования эмбеддингов: {e}\")\n",
    "        return np.zeros((0,))\n",
    "\n",
    "# Функция для подготовки данных для обучения модели\n",
    "def prepare_data(train_data):\n",
    "    # Создание комбинированных эмбеддингов\n",
    "    train_data['combined_embeddings'] = train_data.apply(combine_embeddings, axis=1)\n",
    "\n",
    "    # Отделение эмбеддингов от целевой переменной\n",
    "    combined_embeddings = train_data['combined_embeddings'].tolist()\n",
    "\n",
    "    # Удаление строк с пустыми эмбеддингами\n",
    "    valid_rows = [embeddings for embeddings in combined_embeddings if embeddings.size > 0]\n",
    "    if len(valid_rows) == 0:\n",
    "        raise ValueError(\"Все строки содержат пустые эмбеддинги.\")\n",
    "    \n",
    "    # Преобразование списка валидных эмбеддингов в массив NumPy\n",
    "    X = np.vstack(valid_rows)\n",
    "    y = train_data.loc[train_data['combined_embeddings'].apply(lambda x: x.size > 0), 'target'].values\n",
    "\n",
    "    # Убедитесь, что X и y имеют согласованную длину\n",
    "    if X.shape[0] != len(y):\n",
    "        raise ValueError(\"Размеры X и y не совпадают.\")\n",
    "\n",
    "    # Разделение данных на обучающую и валидационную выборки\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "# Функция для обучения модели\n",
    "def train_model(X_train, y_train):\n",
    "    # Проверка размерностей перед обучением\n",
    "    print(f\"Размер X_train: {X_train.shape}\")\n",
    "    print(f\"Размер y_train: {len(y_train)}\")\n",
    "\n",
    "    # Создание и обучение модели логистической регрессии\n",
    "    model = LogisticRegression(max_iter=2000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Сохранение модели на диск\n",
    "    joblib.dump(model, 'model.pkl')\n",
    "    return model\n",
    "\n",
    "# Функция для оценки модели\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    # Получение вероятностей предсказания\n",
    "    y_pred_prob = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Вычисление Precision-Recall AUC\n",
    "    precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(f'PR AUC: {pr_auc}')\n",
    "\n",
    "# Основная функция\n",
    "def main():\n",
    "    # Загрузка данных\n",
    "    attributes, resnet, text_and_bert, train, test = load_data(sample_size=0.1)\n",
    "    \n",
    "    # Обработка данных\n",
    "    train_data = process_data(attributes, resnet, text_and_bert, train)\n",
    "    \n",
    "    # Подготовка данных для обучения\n",
    "    X_train, X_val, y_train, y_val = prepare_data(train_data)\n",
    "\n",
    "    # Обучение модели\n",
    "    model = train_model(X_train, y_train)\n",
    "    \n",
    "    # Оценка модели\n",
    "    evaluate_model(model, X_val, y_val)\n",
    "\n",
    "    # Подготовка данных для тестирования\n",
    "    # test_data = process_test_data(test, resnet, text_and_bert)  # Нужно реализовать эту функцию\n",
    "    # test_predictions = model.predict_proba(test_data)[:, 1]\n",
    "    # Создание файла с результатами\n",
    "    # submission = pd.DataFrame({'id': test['id'], 'target': test_predictions})\n",
    "    # submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Запуск основной функции, если скрипт выполняется напрямую\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac6f96-de2d-4e06-850a-507465421378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb42dd1-bd8a-46bb-987b-ac0dd63e3988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd6043-48c3-401a-8d66-144e17018373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
